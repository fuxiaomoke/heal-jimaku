"""
SRTå­—å¹•å¤„ç†å™¨æ¨¡å—

è´Ÿè´£å°†ASRï¼ˆè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼‰è½¬å½•ç»“æœè½¬æ¢ä¸ºæ ‡å‡†çš„SRTå­—å¹•æ ¼å¼ã€‚
åŒ…å«æ™ºèƒ½æ–‡æœ¬åˆ†å‰²ã€æ—¶é—´æˆ³ç²¾ç¡®å¯¹é½ã€å­—å¹•æ¡ç›®ä¼˜åŒ–ç­‰æ ¸å¿ƒåŠŸèƒ½ã€‚
æ”¯æŒå¤šè¯­è¨€å¤„ç†å’Œè‡ªå®šä¹‰å‚æ•°é…ç½®ã€‚

ä½œè€…: fuxiaomoke
ç‰ˆæœ¬: 0.2.2.0
"""

import re
import difflib
import json
from typing import List, Optional, Any, Dict
from .data_models import TimestampedWord, ParsedTranscription, SubtitleEntry
import config as app_config # ä½¿ç”¨åˆ«åä»¥å‡å°‘æ½œåœ¨å†²çªå¹¶æ¸…æ™°åŒ–æ¥æº

class SrtProcessor:
    """
    SRTå­—å¹•å¤„ç†å™¨

    è´Ÿè´£å°†ASRè½¬å½•ç»“æœè½¬æ¢ä¸ºSRTå­—å¹•æ ¼å¼ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ†å‰²ã€æ—¶é—´æˆ³å¯¹é½ã€
    å­—å¹•ä¼˜åŒ–ç­‰æ ¸å¿ƒåŠŸèƒ½ã€‚
    """

    # SRTå¤„ç†é˜¶æ®µçš„æƒé‡å¸¸é‡
    WEIGHT_ALIGN = 20  # å¯¹é½é˜¶æ®µæƒé‡ï¼ˆåŸºç¡€SRTç”Ÿæˆï¼‰
    WEIGHT_MERGE = 20  # åˆå¹¶é˜¶æ®µæƒé‡
    WEIGHT_FORMAT = 20 # æ ¼å¼åŒ–é˜¶æ®µæƒé‡ï¼ˆä¸åŒ…æ‹¬AIçº é”™ï¼‰
    WEIGHT_AI_CORRECTION = 40  # AIçº é”™é˜¶æ®µæƒé‡ï¼ˆä»…Sonioxæ¨¡å¼ä½¿ç”¨ï¼‰

    def __init__(self, initial_config: Optional[Dict[str, Any]] = None) -> None:
        """åˆå§‹åŒ–SRTå¤„ç†å™¨"""
        self._signals: Optional[Any] = None
        self._current_progress_offset: int = 0
        self._current_progress_range: int = 100

    # Soniox ä¸“ç”¨å¸¸é‡é›†åˆ (Mode C)
    SONIOX_THRESHOLDS = {
        "CONF_LIMIT": app_config.DEFAULT_SONIOX_LOW_CONFIDENCE_THRESHOLD,        # ç½®ä¿¡åº¦çº¢çº¿é˜ˆå€¼
        "LARGE_GAP": 0.80,         # å¼‚å¸¸å¤§é—´è·é˜ˆå€¼ (ç§’)
        "EXT_GAP_MIN": 0.55,       # å®‰å…¨åŠ å°¾å·´çš„æœ€å°é—´è· (ç§’)
        "TAIL_LEN": 0.30,          # å°¾å·´é•¿åº¦ (ç§’)
        "START_PAD": 0.25,         # å¼€å§‹æ—¶é—´å‰æ‘‡ (ç§’)
        "RAPID_GAP": 0.15          # æ€¥é€Ÿè¿è¯»åˆ¤å®šé˜ˆå€¼ (ç§’)
    }
    def __init__(self, initial_config: Optional[Dict[str, Any]] = None):
        self._signals: Optional[Any] = None
        self._current_progress_offset: int = 0
        self._current_progress_range: int = 100

        # åˆå§‹åŒ–SRTå¤„ç†å‚æ•°çš„é»˜è®¤å€¼
        self.min_duration_target: float = app_config.DEFAULT_MIN_DURATION_TARGET
        self.max_duration: float = app_config.DEFAULT_MAX_DURATION
        self.max_chars_per_line: int = app_config.DEFAULT_MAX_CHARS_PER_LINE
        self.default_gap_ms: int = app_config.DEFAULT_DEFAULT_GAP_MS

        # åˆå§‹åŒ–LLMé…ç½®ç›¸å…³çš„æˆå‘˜å˜é‡
        self.llm_api_key: Optional[str] = app_config.DEFAULT_LLM_API_KEY
        self.llm_base_url: Optional[str] = app_config.DEFAULT_LLM_API_BASE_URL
        self.llm_model_name: Optional[str] = app_config.DEFAULT_LLM_MODEL_NAME
        self.llm_temperature: float = app_config.DEFAULT_LLM_TEMPERATURE

        if initial_config:
            self.configure_from_main_config(initial_config)

    def set_signals_forwarder(self, signals_forwarder: Any):
        self._signals = signals_forwarder

    def configure_from_main_config(self, main_config_data: Dict[str, Any]):
        """
        Update SRT processor parameters from main application configuration.

        Args:
            main_config_data: Dictionary containing configuration values using USER_..._KEY constants
        """
        # Update SRT parameters using USER_..._KEY from main configuration
        self.min_duration_target = float(main_config_data.get(app_config.USER_MIN_DURATION_TARGET_KEY, app_config.DEFAULT_MIN_DURATION_TARGET))
        self.max_duration = float(main_config_data.get(app_config.USER_MAX_DURATION_KEY, app_config.DEFAULT_MAX_DURATION))
        self.max_chars_per_line = int(main_config_data.get(app_config.USER_MAX_CHARS_PER_LINE_KEY, app_config.DEFAULT_MAX_CHARS_PER_LINE))
        self.default_gap_ms = int(main_config_data.get(app_config.USER_DEFAULT_GAP_MS_KEY, app_config.DEFAULT_DEFAULT_GAP_MS))

        # Update LLM parameters - use same approach as ConversionWorker for consistency
        # First try to get from legacy config keys (for backward compatibility)
        self.llm_api_key = main_config_data.get(app_config.USER_LLM_API_KEY_KEY, app_config.DEFAULT_LLM_API_KEY)
        self.llm_base_url = main_config_data.get(app_config.USER_LLM_API_BASE_URL_KEY, app_config.DEFAULT_LLM_API_BASE_URL)
        self.llm_model_name = main_config_data.get(app_config.USER_LLM_MODEL_NAME_KEY, app_config.DEFAULT_LLM_MODEL_NAME)
        self.llm_temperature = float(main_config_data.get(app_config.USER_LLM_TEMPERATURE_KEY, app_config.DEFAULT_LLM_TEMPERATURE))

        # If legacy keys are empty, try the new multi-profile system as fallback
        if not self.llm_api_key:
            current_llm_profile = app_config.get_current_llm_profile(main_config_data)
            self.llm_api_key = current_llm_profile.get("api_key", app_config.DEFAULT_LLM_API_KEY)
            if not self.llm_base_url or self.llm_base_url == app_config.DEFAULT_LLM_API_BASE_URL:
                self.llm_base_url = current_llm_profile.get("api_base_url", app_config.DEFAULT_LLM_API_BASE_URL)
            if not self.llm_model_name or self.llm_model_name == app_config.DEFAULT_LLM_MODEL_NAME:
                self.llm_model_name = current_llm_profile.get("model_name", app_config.DEFAULT_LLM_MODEL_NAME)

        self.log(f"é…ç½®LLMå‚æ•°: API keyå‰10å­—ç¬¦={self.llm_api_key[:10] if self.llm_api_key else 'None'}..., base_url={self.llm_base_url}, model={self.llm_model_name}")

    # --- æ–°å¢/æ¢å¤ update_srt_params æ–¹æ³• ---
    def update_srt_params(self, srt_params_dict: Dict[str, Any]):
        """
        Update SRT processing parameters from a simple dictionary.

        This method is called by MainWindow.start_conversion() with parameters
        from self.advanced_srt_settings.

        Args:
            srt_params_dict: Dictionary containing SRT processing parameters
        """
        self.min_duration_target = float(srt_params_dict.get('min_duration_target', self.min_duration_target))
        self.max_duration = float(srt_params_dict.get('max_duration', self.max_duration))
        self.max_chars_per_line = int(srt_params_dict.get('max_chars_per_line', self.max_chars_per_line))
        self.default_gap_ms = int(srt_params_dict.get('default_gap_ms', self.default_gap_ms))


    def update_llm_config(
        self,
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        model: Optional[str] = None,
        temperature: Optional[float] = None
    ):
        self.log("æ­£åœ¨å•ç‹¬æ›´æ–° SrtProcessor çš„LLM APIå‚æ•°...")
        if api_key is not None: self.llm_api_key = api_key
        if base_url is not None: self.llm_base_url = base_url
        if model is not None: self.llm_model_name = model
        if temperature is not None: self.llm_temperature = float(temperature)
        self.log(f"  LLMå‚æ•°å•ç‹¬æ›´æ–°å: BaseURL='{self.llm_base_url}', Model='{self.llm_model_name}', Temp={self.llm_temperature}, APIKeySet={bool(self.llm_api_key)}")

    def get_current_llm_config_for_api_call(self) -> Dict[str, Any]:
        return {
            "api_key": self.llm_api_key,
            "custom_api_base_url_str": self.llm_base_url,
            "custom_model_name": self.llm_model_name,
            "custom_temperature": self.llm_temperature,
        }

    def log(self, message: str):
        if self._signals and hasattr(self._signals, 'log_message') and hasattr(self._signals.log_message, 'emit'):
            self._signals.log_message.emit(f"[SRT Processor] {message}")
        else:
            print(f"[SRT Processor] {message}")

    def _is_worker_running(self) -> bool: 
        if self._signals and hasattr(self._signals, 'parent') and \
           hasattr(self._signals.parent(), 'is_running'): 
            return self._signals.parent().is_running
        return True

    def _emit_srt_progress(self, current_step: int, total_steps: int):
        if total_steps == 0:
            internal_percentage = 100
        else:
            internal_percentage = min(int((current_step / total_steps) * 100), 100)
        
        if self._signals and hasattr(self._signals, 'progress') and hasattr(self._signals.progress, 'emit'):
            global_progress = self._current_progress_offset + int(internal_percentage * (self._current_progress_range / 100.0))
            capped_progress = min(max(global_progress, self._current_progress_offset), self._current_progress_offset + self._current_progress_range)
            capped_progress = min(capped_progress, 99) 
            self._signals.progress.emit(capped_progress)

    def _is_bracketed_content(self, text: str) -> bool:
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦ä¸ºæ‹¬å·å†…å®¹ï¼ˆä»»ä½•æ‹¬å·å†…çš„å†…å®¹éƒ½åº”è¯¥ç‹¬ç«‹å¤„ç†ï¼‰"""
        if not text or not text.strip():
            return False

        text = text.strip()

        # æ£€æŸ¥æ˜¯å¦å®Œå…¨è¢«æ‹¬å·åŒ…å›´
        # æ”¯æŒå„ç§æ‹¬å·ç±»å‹ï¼š()ã€ï¼ˆï¼‰ã€ã€ã€‘ã€[]ã€{}ã€<>
        bracket_patterns = [
            r"^\(.*\)$",      # ()
            r"^ï¼ˆ.*ï¼‰$",        # ï¼ˆï¼‰
            r"^ã€.*ã€‘$",        # ã€ã€‘
            r"^\[.*\]$",        # []
            r"^\{.*\}$",        # {}
            r"^<.*>$",          # <>
        ]

        return any(re.match(pattern, text) for pattern in bracket_patterns)

    
    def _is_audio_event_words(self, words_list) -> bool:
        """æ£€æŸ¥è¯åˆ—è¡¨æ˜¯å¦ä¸ºæ‹¬å·å†…å®¹ï¼ˆä»£è¡¨éè¯­è¨€å£°éŸ³æˆ–ç‰¹æ®Šæ ‡è®°ï¼‰"""
        if not words_list:
            return False

        # ç»„åˆæ‰€æœ‰è¯çš„æ–‡æœ¬
        full_text = "".join([w.text for w in words_list]).strip()

        # æ£€æŸ¥æ˜¯å¦ä¸ºæ‹¬å·å†…å®¹
        if self._is_bracketed_content(full_text):
            return True

        # å¦‚æœASRæ ‡è®°ä¸ºaudio_eventç±»å‹ï¼Œä¹Ÿè®¤ä¸ºæ˜¯éŸ³é¢‘äº‹ä»¶
        return any(getattr(w, 'type', 'word') == 'audio_event' for w in words_list)

    def format_timecode(self, seconds_float: float) -> str:
        if not isinstance(seconds_float, (int, float)) or seconds_float < 0:
            return "00:00:00,000"
        total_seconds_int = int(seconds_float)
        milliseconds = int(round((seconds_float - total_seconds_int) * 1000))
        if milliseconds >= 1000:
            total_seconds_int += 1
            milliseconds = 0
        hours = total_seconds_int // 3600
        minutes = (total_seconds_int % 3600) // 60
        seconds = total_seconds_int % 60
        return f"{hours:02d}:{minutes:02d}:{seconds:02d},{milliseconds:03d}"

    def check_word_has_punctuation(self, word_text: str, punctuation_set: set) -> bool:
        """æ£€æŸ¥è¯æ±‡æ˜¯å¦åŒ…å«æ ‡ç‚¹ç¬¦å·ï¼ˆåŒ…æ‹¬è¯æ±‡æœ¬èº«æ˜¯æ ‡ç‚¹æˆ–ä»¥æ ‡ç‚¹ç»“å°¾ï¼‰"""
        import re
        import unicodedata

        cleaned_text = word_text.strip()
        if not cleaned_text:
            return False

        # 0. åªè¿‡æ»¤æ˜æ˜¾çš„æ— æ•ˆå†…å®¹ï¼šçº¯ç©ºæ ¼ã€å•ä¸ªæ ‡ç‚¹ç¬¦å·
        if len(cleaned_text) == 1:
            # åªè¿‡æ»¤å•ä¸ªç©ºæ ¼å’Œå•ä¸ªæ ‡ç‚¹ç¬¦å·
            if cleaned_text == ' ' or cleaned_text in punctuation_set:
                return True

        # è°ƒè¯•ï¼šè®°å½•è¯¦ç»†çš„æ£€æµ‹è¿‡ç¨‹
        step1_match = cleaned_text in punctuation_set
        step2_match = False
        step3_match = False
        step4_match = False

        # 1. æ£€æŸ¥è¯æ±‡æœ¬èº«æ˜¯å¦åœ¨é¢„å®šä¹‰æ ‡ç‚¹ç¬¦å·é›†åˆä¸­
        if step1_match:
            return True

        # 2. æ£€æŸ¥è¯æ±‡æ˜¯å¦ä»¥é¢„å®šä¹‰æ ‡ç‚¹ç¬¦å·ç»“å°¾ - ç²¾ç¡®åŒ¹é…ï¼Œä¼˜å…ˆçº§æœ€é«˜
        for punct in punctuation_set:
            if cleaned_text.endswith(punct):
                return True

        # 3. æ£€æŸ¥å¸¸è§çš„çœç•¥å·æ¨¡å¼ï¼ˆä»…å¯¹ELLIPSIS_PUNCTUATIONæ£€æµ‹ï¼‰
        # åªæœ‰åœ¨æ£€æµ‹çœç•¥å·é›†åˆæ—¶æ‰ä½¿ç”¨çœç•¥å·æ­£åˆ™è¡¨è¾¾å¼
        ellipsis_chars_in_set = any(p in punctuation_set for p in ['...', '......', 'â€¥', 'â€¦'])
        if ellipsis_chars_in_set:
            ellipsis_patterns = [r'â€¦+', r'â€¥+', r'\.{3,}']  # æŒ‰ä¼˜å…ˆçº§æ’åºï¼Œé¿å…é‡å 
            for pattern in ellipsis_patterns:
                if re.search(pattern + '$', cleaned_text):
                    return True

        # 4. ä½¿ç”¨Unicodeç±»åˆ«æ£€æµ‹æ ‡ç‚¹ç¬¦å· - ä½†æ’é™¤å¸¸è§æ ‡ç‚¹é¿å…é‡å¤åŒ¹é…
        # Unicodeæ ‡ç‚¹ç¬¦å·ç±»åˆ«ï¼šPc (è¿æ¥ç¬¦), Pd (ç ´æŠ˜å·), Pe (åå¼•å·), Pf (åå¼•å·), Pi (å‰å¼•å·), Po (å…¶ä»–æ ‡ç‚¹), Ps (å‰å¼•å·)
        last_char = cleaned_text[-1] if len(cleaned_text) > 0 else ''
        # æ’é™¤å·²ç»è¢«ç²¾ç¡®å¤„ç†çš„å¸¸è§æ ‡ç‚¹ç¬¦å·ï¼Œé¿å…äº¤å‰åŒ¹é…
        excluded_chars = {'.', 'ã€‚', '?', 'ï¼Ÿ', '!', 'ï¼', ',', 'ã€', 'ï¼Œ', 'â€¦', 'â€¥'}
        if last_char and last_char not in excluded_chars:
            if unicodedata.category(last_char) in ['Pc', 'Pd', 'Pe', 'Pf', 'Pi', 'Po', 'Ps']:
                return True

        return False

    def get_segment_words_fuzzy(self, text_segment: str, all_parsed_words: List[TimestampedWord], start_search_index: int) -> tuple[List[TimestampedWord], int, float]:
        """
        Fuzzy matching algorithm for aligning LLM segments with ASR word timestamps.

        Args:
            text_segment: LLM-generated text segment to align
            all_parsed_words: List of ASR words with timestamps
            start_search_index: Starting index in the word list for search

        Returns:
            Tuple of (matched_words, next_search_index, match_ratio)
        """
        segment_clean = text_segment.strip().replace(" ", "")
        if not segment_clean:
            return [], start_search_index, 0.0

        best_match_words_ts_objects: List[TimestampedWord] = []
        best_match_ratio = 0.0
        best_match_end_index = start_search_index

        # ä½¿ç”¨é€‚å½“çš„æœç´¢çª—å£å¤§å°
        base_len_factor = 3
        min_additional_words = 20
        max_additional_words = 60
        estimated_words_in_segment = len(text_segment.split())
        search_window_size = len(segment_clean) * base_len_factor + min(max(estimated_words_in_segment * 2, min_additional_words), max_additional_words)
        max_lookahead_outer = min(start_search_index + search_window_size, len(all_parsed_words))

        for i in range(start_search_index, max_lookahead_outer):
            if not self._is_worker_running():
                break

            current_words_text_list = []
            current_word_ts_object_list: List[TimestampedWord] = []
            max_j_lookahead = min(i + len(segment_clean) + 30, len(all_parsed_words))

            for j in range(i, max_j_lookahead):
                word_obj = all_parsed_words[j]
                current_word_ts_object_list.append(word_obj)
                current_words_text_list.append(word_obj.text.replace(" ", ""))
                built_text = "".join(current_words_text_list)

                if not built_text.strip():
                    continue

                matcher = difflib.SequenceMatcher(None, segment_clean, built_text, autojunk=False)
                ratio = matcher.ratio()

                update_best = False
                if ratio > best_match_ratio:
                    update_best = True
                elif abs(ratio - best_match_ratio) < 1e-9:
                    if best_match_words_ts_objects:
                        current_len_diff = abs(len(built_text) - len(segment_clean))
                        best_len_diff = abs(len("".join(w.text.replace(" ","") for w in best_match_words_ts_objects)) - len(segment_clean))
                        if current_len_diff < best_len_diff:
                            update_best = True
                    else:
                        update_best = True

                if update_best and ratio > 0.01:
                    best_match_ratio = ratio
                    best_match_words_ts_objects = list(current_word_ts_object_list)
                    best_match_end_index = j + 1

                if ratio > 0.95 and len(built_text) > len(segment_clean) * 1.8:
                    break

            if best_match_ratio > 0.98:
                break

        if not best_match_words_ts_objects:
            self.log(f"ä¸¥é‡è­¦å‘Š: LLMç‰‡æ®µ \"{text_segment}\" (æ¸…ç†å: \"{segment_clean}\") æ— æ³•åœ¨ASRè¯è¯­ä¸­æ‰¾åˆ°ä»»ä½•åŒ¹é…ã€‚å°†è·³è¿‡æ­¤ç‰‡æ®µã€‚æœç´¢èµ·å§‹ç´¢å¼•: {start_search_index}")
            return [], start_search_index, 0.0

        if best_match_ratio < app_config.ALIGNMENT_SIMILARITY_THRESHOLD:
            matched_text_preview = "".join([w.text for w in best_match_words_ts_objects])
            self.log(f"è­¦å‘Š: LLMç‰‡æ®µ \"{text_segment}\" (æ¸…ç†å: \"{segment_clean}\") ä¸ASRè¯è¯­çš„å¯¹é½ç›¸ä¼¼åº¦è¾ƒä½ ({best_match_ratio:.2f})ã€‚ASRåŒ¹é…æ–‡æœ¬: \"{matched_text_preview}\"")

            # å›é€€å¯¹é½ç­–ç•¥ï¼šå¦‚æœç›¸ä¼¼åº¦ä¸ä½äºé˜ˆå€¼çš„70%ï¼Œåˆ™ä½¿ç”¨å›é€€ç­–ç•¥
            relaxed_threshold = app_config.ALIGNMENT_SIMILARITY_THRESHOLD * 0.7
            if best_match_ratio >= relaxed_threshold:
                self.log(f"âš ï¸ ä½¿ç”¨å›é€€å¯¹é½ç­–ç•¥ï¼Œç›¸ä¼¼åº¦: {best_match_ratio:.2f} (ä½äºæ ‡å‡†é˜ˆå€¼ {app_config.ALIGNMENT_SIMILARITY_THRESHOLD:.2f} ä½†é«˜äºå›é€€é˜ˆå€¼ {relaxed_threshold:.2f})")
                # åœ¨å›é€€ç­–ç•¥ä¸‹ï¼Œä»ç„¶è¿”å›åŒ¹é…ç»“æœï¼Œä½†è®°å½•è­¦å‘Š
                return best_match_words_ts_objects, best_match_end_index, best_match_ratio
            else:
                # å¦‚æœè¿å›é€€é˜ˆå€¼éƒ½è¾¾ä¸åˆ°ï¼Œåˆ™è¿”å›ç©ºç»“æœ
                self.log(f"âŒ å›é€€å¯¹é½ç­–ç•¥å¤±è´¥ï¼Œç›¸ä¼¼åº¦{best_match_ratio:.2f}ä½äºå›é€€é˜ˆå€¼{relaxed_threshold:.2f}ï¼Œè·³è¿‡æ­¤ç‰‡æ®µ")
                return [], start_search_index, 0.0

        return best_match_words_ts_objects, best_match_end_index, best_match_ratio

    # --- ç»“æŸæ—¶é—´ä¿®æ­£ è¾…åŠ©å‡½æ•° ---
    def _apply_end_time_correction(self, segment_words: List[TimestampedWord], raw_end_time: float, segment_start_time: float) -> float:
        """
        åº”ç”¨ç»“æŸæ—¶é—´ä¿®æ­£é€»è¾‘ï¼ˆæ£€æŸ¥è¯é—´ç©ºéš™ã€å€’äºŒè¯æ—¶é•¿ã€æœ«å°¾è¯æ—¶é•¿ï¼‰ã€‚
        """
        if not segment_words:
            return raw_end_time

        duration_threshold = 0.35  # å¼‚å¸¸æ—¶é•¿é˜ˆå€¼ (0.35s)
        gap_threshold = 0.6       # å¼‚å¸¸ç©ºéš™é˜ˆå€¼ (0.6s)
        correction_padding = 0.25  # ä¿®æ­£æ—¶ä½¿ç”¨çš„"ç•™ç™½" (0.25s)
        
        # æ£€æŸ¥1 (ç©ºéš™ä¼˜å…ˆ): æ£€æŸ¥å€’æ•°ç¬¬äºŒä¸ªè¯å’Œæœ€åä¸€ä¸ªè¯ä¹‹é—´çš„â€œç©ºéš™â€
        if len(segment_words) > 1:
            last_word = segment_words[-1]
            word_before_last = segment_words[-2]
            
            gap_duration = last_word.start_time - word_before_last.end_time
            
            if gap_duration > gap_threshold:
                self.log(f"å­—å¹•æ—¶é—´ä¼˜åŒ–: ä¿®æ­£è¯é—´å¼‚å¸¸ç©ºéš™ ({gap_duration:.2f}s)")
                # ä»¥"å€’äºŒè¯"çš„ *å¼€å§‹* æ—¶é—´ä¸ºåŸºå‡†
                new_end_time = word_before_last.start_time + correction_padding
                
                # å®‰å…¨æ£€æŸ¥
                if new_end_time < segment_start_time:
                    return segment_start_time + correction_padding
                return new_end_time # å‘½ä¸­è§„åˆ™ï¼Œç«‹å³è¿”å›

        # æ£€æŸ¥2 (å€’äºŒè¯æ—¶é•¿): (ä»…åœ¨â€œç©ºéš™â€å¹²å‡€æ—¶æ‰æ‰§è¡Œæ­¤æ£€æŸ¥)
        if len(segment_words) > 1:
            word_before_last = segment_words[-2]
            word_before_last_duration = word_before_last.end_time - word_before_last.start_time
            
            if word_before_last_duration > duration_threshold:
                self.log(f"å­—å¹•æ—¶é—´ä¼˜åŒ–: ä¿®æ­£å¼‚å¸¸è¯æ—¶é•¿ ({word_before_last_duration:.2f}s)")
                new_end_time = word_before_last.start_time + correction_padding
                
                # å®‰å…¨æ£€æŸ¥
                if new_end_time < segment_start_time:
                    return segment_start_time + correction_padding
                return new_end_time # å‘½ä¸­è§„åˆ™ï¼Œç«‹å³è¿”å›

        # æ£€æŸ¥3 (æœ«å°¾è¯æ—¶é•¿): (ä»…åœ¨â€œç©ºéš™â€å’Œâ€œå€’äºŒè¯â€éƒ½å¹²å‡€æ—¶æ‰æ‰§è¡Œæ­¤æ£€æŸ¥)
        last_word = segment_words[-1]
        last_word_duration = last_word.end_time - last_word.start_time
        
        if last_word_duration > duration_threshold:
            self.log(f"å­—å¹•æ—¶é—´ä¼˜åŒ–: ä¿®æ­£æœ«å°¾è¯å¼‚å¸¸æ—¶é•¿ ({last_word_duration:.2f}s)")
            new_end_time = last_word.start_time + correction_padding

            # å®‰å…¨æ£€æŸ¥
            if new_end_time < segment_start_time:
                return segment_start_time + correction_padding
            return new_end_time # å‘½ä¸­è§„åˆ™

        # å¦‚æœæ‰€æœ‰æ£€æŸ¥éƒ½é€šè¿‡ï¼Œè¿”å›åŸå§‹æ—¶é—´
        return raw_end_time
    # --- è¾…åŠ©å‡½æ•° ç»“æŸ ---

    def _apply_smart_split_strategy(self, sentence_text: str, sentence_words: List[TimestampedWord],
                              original_start_time: float, original_end_time: float
                             ) -> Optional[List[SubtitleEntry]]:
        """
        æ™ºèƒ½åˆ†å‰²ç­–ç•¥ï¼šå¼ºåˆ¶åœ¨æ ‡ç‚¹ç¬¦å·å¤„åˆ†å‰²ï¼Œæœ€å¤š3æ®µï¼Œå…è®¸è¶…é™

        è§„åˆ™ï¼š
        1. å¦‚æœå¥å­ä¸­æœ‰éå¥æœ«æ ‡ç‚¹ï¼Œå¼ºåˆ¶åœ¨æ ‡ç‚¹å¤„åˆ†å‰²
        2. æœ€å¤šåˆ†å‰²æˆ3æ®µï¼Œå¦‚æœåšä¸åˆ°åˆ™å…è®¸è¶…é™
        3. ä¼˜å…ˆé€‰æ‹©å¥æœ«æ ‡ç‚¹ï¼Œå…¶æ¬¡æ˜¯é€—å·ï¼Œæœ€åæ˜¯ä¸­é—´å¼ºåˆ¶åˆ†å‰²

        Args:
            sentence_text: è¦åˆ†å‰²çš„å¥å­æ–‡æœ¬
            sentence_words: è¯æ±‡åˆ—è¡¨
            original_start_time: å¼€å§‹æ—¶é—´
            original_end_time: ç»“æŸæ—¶é—´

        Returns:
            åˆ†å‰²åçš„å­—å¹•æ¡ç›®åˆ—è¡¨ï¼Œå¦‚æœä¸é€‚ç”¨æ­¤ç­–ç•¥åˆ™è¿”å›None
        """
        import re

        # æ£€æŸ¥å¥å­ä¸­æ˜¯å¦æœ‰éå¥æœ«æ ‡ç‚¹ç¬¦å·
        # ç§»é™¤å¥æœ«çš„æ ‡ç‚¹ç¬¦å·è¿›è¡Œæ£€æŸ¥
        text_without_end = re.sub(r'[ã€‚ï¼ï¼Ÿ\.\!]?$', '', sentence_text.strip())
        has_non_end_punctuation = (
            'ï¼Œ' in text_without_end or
            'ã€' in text_without_end or
            '...' in text_without_end or
            'â€¦' in text_without_end
        )

        if not has_non_end_punctuation:
            # æ²¡æœ‰éå¥æœ«æ ‡ç‚¹ï¼Œä¸åº”ç”¨æ­¤ç­–ç•¥
            return None

        self.log(f"   ğŸ”§ æ™ºèƒ½åˆ†å‰²: æ£€æµ‹åˆ°å†…å«æ ‡ç‚¹ï¼Œåº”ç”¨å¼ºåˆ¶åˆ†å‰²ç­–ç•¥")

        # å¯»æ‰¾æ‰€æœ‰å¯èƒ½çš„åˆ†å‰²ç‚¹
        split_indices = []
        for i, word_obj in enumerate(sentence_words[:-1]):  # ä¸åœ¨æœ€åä¸€ä¸ªè¯ååˆ†å‰²
            w_text = word_obj.text.strip()

            # ä¼˜å…ˆçº§1: å¥æœ«æ ‡ç‚¹ï¼ˆã€‚ï¼ï¼Ÿ. !ï¼‰
            if self.check_word_has_punctuation(w_text, app_config.FINAL_PUNCTUATION):
                split_indices.append(i)
            # ä¼˜å…ˆçº§2: çœç•¥å·ï¼ˆ...ï¼‰
            elif self.check_word_has_punctuation(w_text, app_config.ELLIPSIS_PUNCTUATION):
                split_indices.append(i)
            # ä¼˜å…ˆçº§3: é€—å·ç±»ï¼ˆï¼Œã€ï¼‰
            elif self.check_word_has_punctuation(w_text, app_config.COMMA_PUNCTUATION):
                split_indices.append(i)

        if not split_indices:
            self.log(f"   âš ï¸ æ™ºèƒ½åˆ†å‰²: è™½æœ‰æ ‡ç‚¹ä½†æœªæ‰¾åˆ°åˆé€‚åˆ†å‰²ç‚¹ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥")
            return None

        # é€‰æ‹©æœ€ä½³åˆ†å‰²ç‚¹ï¼šå°½é‡å‡åŒ€åˆ†å‰²ï¼Œä¼˜å…ˆé å‰çš„æ ‡ç‚¹
        if len(split_indices) >= 2:
            # æœ‰å¤šä¸ªåˆ†å‰²ç‚¹ï¼Œé€‰æ‹©èƒ½äº§ç”Ÿè¾ƒå‡åŒ€åˆ†å‰²çš„ç‚¹
            target_split_positions = [
                len(sentence_words) // 3,  # ä¸‰åˆ†ä¹‹ä¸€ä½ç½®
                len(sentence_words) // 2,  # äºŒåˆ†ä¹‹ä¸€ä½ç½®
                len(sentence_words) * 2 // 3  # ä¸‰åˆ†ä¹‹äºŒä½ç½®
            ]
            best_splits = []
            for target_pos in target_split_positions:
                closest_idx = min(split_indices, key=lambda i: abs(i - target_pos))
                if closest_idx not in best_splits:
                    best_splits.append(closest_idx)
            selected_splits = best_splits[:2]  # æœ€å¤šé€‰æ‹©2ä¸ªåˆ†å‰²ç‚¹ï¼Œäº§ç”Ÿ3æ®µ
        else:
            selected_splits = split_indices[:2]  # æœ€å¤š2ä¸ªåˆ†å‰²ç‚¹

        if not selected_splits:
            return None

        self.log(f"   ğŸ“ æ™ºèƒ½åˆ†å‰²: é€‰æ‹©{len(selected_splits)}ä¸ªåˆ†å‰²ç‚¹ï¼Œé¢„è®¡ç”Ÿæˆ{len(selected_splits)+1}æ®µ")

        # æ‰§è¡Œåˆ†å‰²
        result_entries = []
        start_idx = 0

        for split_idx in selected_splits:
            if split_idx < start_idx:
                continue

            # åˆ›å»ºå½“å‰æ®µ
            segment_words = sentence_words[start_idx:split_idx+1]
            segment_text = "".join([w.text for w in segment_words])
            segment_start = segment_words[0].start_time
            segment_end = segment_words[-1].end_time

            entry = SubtitleEntry(0, segment_start, segment_end, segment_text, segment_words)
            # å…è®¸è¶…é™ï¼Œè®¾ç½®æ ‡è®°
            if entry.duration > self.max_duration or len(segment_text) > self.max_chars_per_line:
                entry.is_intentionally_oversized = True
                self.log(f"   âš ï¸ æ™ºèƒ½åˆ†å‰²: æ®µè½è¶…é™ä½†æ¥å— - \"{segment_text[:20]}...\" ({entry.duration:.2f}s)")
            else:
                self.log(f"   âœ… æ™ºèƒ½åˆ†å‰²: æ®µè½æ­£å¸¸ - \"{segment_text[:20]}...\" ({entry.duration:.2f}s)")

            result_entries.append(entry)
            start_idx = split_idx + 1

        # å¤„ç†æœ€åä¸€æ®µ
        if start_idx < len(sentence_words):
            segment_words = sentence_words[start_idx:]
            segment_text = "".join([w.text for w in segment_words])
            segment_start = segment_words[0].start_time
            segment_end = original_end_time  # æœ€åä¸€æ®µä½¿ç”¨åŸå§‹ç»“æŸæ—¶é—´

            entry = SubtitleEntry(0, segment_start, segment_end, segment_text, segment_words)
            if entry.duration > self.max_duration or len(segment_text) > self.max_chars_per_line:
                entry.is_intentionally_oversized = True
                self.log(f"   âš ï¸ æ™ºèƒ½åˆ†å‰²: æœ€åæ®µè½è¶…é™ä½†æ¥å— - \"{segment_text[:20]}...\" ({entry.duration:.2f}s)")
            else:
                self.log(f"   âœ… æ™ºèƒ½åˆ†å‰²: æœ€åæ®µè½æ­£å¸¸ - \"{segment_text[:20]}...\" ({entry.duration:.2f}s)")

            result_entries.append(entry)

        self.log(f"   ğŸ¯ æ™ºèƒ½åˆ†å‰²å®Œæˆ: å…±{len(result_entries)}æ®µ")
        return result_entries

    def split_long_sentence(self, sentence_text: str, sentence_words: List[TimestampedWord],
                            original_start_time: float, original_end_time: float, _recursion_depth: int = 0,
                            override_end_time: Optional[float] = None
                           ) -> List[SubtitleEntry]:
        """
        åˆ†å‰²è¶…é•¿çš„å¥å­ï¼ŒåŸºäºæ ‡ç‚¹ç¬¦å·ä¼˜å…ˆçº§è¿›è¡Œæ™ºèƒ½åˆ†å‰²

        ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„çº é”™åæ–‡æœ¬è¿›è¡Œåˆ†å‰²ï¼Œä¿æŒAIçº é”™ç»“æœ

        Args:
            sentence_text: è¦åˆ†å‰²çš„å¥å­æ–‡æœ¬ï¼ˆå¯èƒ½æ˜¯AIçº é”™åçš„ï¼‰
            sentence_words: è¯æ±‡åˆ—è¡¨ï¼ˆåŒ…å«æ—¶é—´æˆ³ï¼Œç”¨äºæ—¶é—´å¯¹é½ï¼‰
            original_start_time: åŸå§‹å¼€å§‹æ—¶é—´
            original_end_time: åŸå§‹ç»“æŸæ—¶é—´
            _recursion_depth: é€’å½’æ·±åº¦ï¼Œç”¨äºé˜²æ­¢æ— é™é€’å½’

        Returns:
            åˆ†å‰²åçš„å­—å¹•æ¡ç›®åˆ—è¡¨
        """
        # å¸¸é‡å®šä¹‰
        MAX_RECURSION_DEPTH = 10
        MIN_SEGMENT_LENGTH = 3  # æœ€å°‘3ä¸ªè¯æ‰å°è¯•åˆ†å‰²
        MAX_SEGMENTS = 3  # æœ€å¤šåˆ†å‰²æˆ3æ®µ
        FORCE_PUNCTUATION_SPLIT = True  # æ˜¯å¦å¼ºåˆ¶åœ¨æ ‡ç‚¹ç¬¦å·å¤„åˆ†å‰²

        # é˜²æ­¢æ— é™é€’å½’
        if _recursion_depth > MAX_RECURSION_DEPTH:
            self.log(f"   è­¦å‘Šï¼šé€’å½’æ·±åº¦è¿‡æ·±({_recursion_depth})ï¼Œå¼ºåˆ¶è¿”å›")
            entry = SubtitleEntry(0, original_start_time, original_end_time, sentence_text, sentence_words)
            entry.is_intentionally_oversized = True
            return [entry]

        # é˜²æ­¢è¿‡çŸ­ç‰‡æ®µç»§ç»­åˆ†å‰²
        if len(sentence_words) <= MIN_SEGMENT_LENGTH:
            self.log(f"   ç‰‡æ®µè¿‡çŸ­({len(sentence_words)}è¯)ï¼Œåœæ­¢åˆ†å‰²")
            entry = SubtitleEntry(0, original_start_time, original_end_time, sentence_text, sentence_words)
            if entry.duration < app_config.MIN_DURATION_ABSOLUTE:
                entry.end_time = entry.start_time + app_config.MIN_DURATION_ABSOLUTE
            if entry.duration > self.max_duration or len(sentence_text) > self.max_chars_per_line:
                entry.is_intentionally_oversized = True
            return [entry]

        # æ£€æŸ¥æ˜¯å¦ä¸ºæ‹¬å·å†…å®¹ï¼Œå¦‚æœæ˜¯åˆ™ä¸åˆ†å‰²
        if self._is_bracketed_content(sentence_text.strip()):
            self.log(f"   æ£€æµ‹åˆ°æ‹¬å·å†…å®¹ï¼Œè·³è¿‡é•¿å¥åˆ†å‰²: \"{sentence_text}\"")
            entry = SubtitleEntry(0, original_start_time, original_end_time, sentence_text, sentence_words)
            if entry.duration < app_config.MIN_DURATION_ABSOLUTE:
                entry.end_time = entry.start_time + app_config.MIN_DURATION_ABSOLUTE
            if entry.duration > self.max_duration or len(sentence_text) > self.max_chars_per_line:
                entry.is_intentionally_oversized = True
            return [entry]

        # ç©ºè¯åˆ—è¡¨å¤„ç†
        if not sentence_words:
            if sentence_text.strip():
                self.log(f"è­¦å‘Š: split_long_sentence æ”¶åˆ°ç©ºè¯åˆ—è¡¨ä½†æœ‰æ–‡æœ¬: \"{sentence_text}\"ã€‚å°†åˆ›å»ºå•ä¸ªæ¡ç›®ã€‚")
                entry = SubtitleEntry(0, original_start_time, original_end_time, sentence_text, [])
                if entry.duration < app_config.MIN_DURATION_ABSOLUTE:
                    entry.end_time = entry.start_time + app_config.MIN_DURATION_ABSOLUTE
                if entry.duration > self.max_duration or len(sentence_text) > self.max_chars_per_line:
                    entry.is_intentionally_oversized = True
                return [entry]
            return []

        # å•ä¸ªè¯å¤„ç†
        if len(sentence_words) <= 1:
            entry = SubtitleEntry(0, original_start_time, original_end_time, sentence_text, sentence_words)
            if entry.duration < app_config.MIN_DURATION_ABSOLUTE:
                entry.end_time = entry.start_time + app_config.MIN_DURATION_ABSOLUTE
            if entry.duration > self.max_duration or len(sentence_text) > self.max_chars_per_line:
                entry.is_intentionally_oversized = True
            return [entry]

        # é¢„æ£€æŸ¥é•¿åº¦
        char_len = len(sentence_text)
        if char_len <= self.max_chars_per_line and (original_end_time - original_start_time) <= self.max_duration:
            return [SubtitleEntry(0, original_start_time, original_end_time, sentence_text, sentence_words)]

        # æ£€æŸ¥æ˜¯å¦éœ€è¦åº”ç”¨æ™ºèƒ½åˆ†å‰²ç­–ç•¥
        if FORCE_PUNCTUATION_SPLIT and _recursion_depth == 0:
            # åº”ç”¨æ™ºèƒ½åˆ†å‰²ç­–ç•¥ï¼šæ ‡ç‚¹å¼ºåˆ¶åˆ†å‰² + æœ€å¤š3æ®µ + å…è®¸è¶…é™
            smart_split_result = self._apply_smart_split_strategy(
                sentence_text, sentence_words, original_start_time, original_end_time
            )
            if smart_split_result:
                return smart_split_result

        entries = []
        words_to_process = list(sentence_words)

        # å¯»æ‰¾æ ‡ç‚¹ç¬¦å·åˆ†å‰²ç‚¹ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
        potential_split_indices_by_priority = {
            'final': [],      # ã€‚ï¼ï¼Ÿï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
            'semicolon': [],  # ï¼›ï¼ˆç¬¬äºŒä¼˜å…ˆçº§ï¼‰
            'ellipsis': [],   # â€¦â€¦ï¼ˆç¬¬ä¸‰ä¼˜å…ˆçº§ï¼‰
            'comma': []       # ï¼Œã€ï¼ˆç¬¬å››ä¼˜å…ˆçº§ï¼‰
        }

  
        for idx, word_obj in enumerate(words_to_process[:-1]):  # ä¸åœ¨æœ€åä¸€ä¸ªè¯ååˆ†å‰²
            w_text = word_obj.text.strip()

              # æŒ‰ä¼˜å…ˆçº§é¡ºåºæ£€æµ‹æ ‡ç‚¹ï¼šfinal > semicolon > ellipsis > comma
            if self.check_word_has_punctuation(w_text, app_config.FINAL_PUNCTUATION):
                potential_split_indices_by_priority['final'].append(idx)
            elif self.check_word_has_punctuation(w_text, app_config.ELLIPSIS_PUNCTUATION):
                # æ£€æŸ¥æ˜¯å¦åŒ…å«åˆ†å·ï¼ˆä¸­æ–‡åˆ†å·ä¼˜å…ˆçº§é«˜äºçœç•¥å·ï¼‰
                if ';' in w_text or 'ï¼›' in w_text:
                    potential_split_indices_by_priority['semicolon'].append(idx)
                else:
                    potential_split_indices_by_priority['ellipsis'].append(idx)
            elif self.check_word_has_punctuation(w_text, app_config.COMMA_PUNCTUATION):
                potential_split_indices_by_priority['comma'].append(idx)

        # é€‰æ‹©æœ€ä½³åˆ†å‰²ç‚¹ - æŒ‰ä¼˜å…ˆçº§é¡ºåºï¼šfinal > semicolon > ellipsis > comma
        best_split_index = -1
        center_pos = len(words_to_process) / 2
        find_closest = lambda indices: min(indices, key=lambda i: abs(i - center_pos)) if indices else -1

        # æŒ‰ä¼˜å…ˆçº§é¡ºåºæ£€æŸ¥
        if potential_split_indices_by_priority['final']:
            best_split_index = find_closest(potential_split_indices_by_priority['final'])
        elif potential_split_indices_by_priority['semicolon']:
            best_split_index = find_closest(potential_split_indices_by_priority['semicolon'])
        elif potential_split_indices_by_priority['ellipsis']:
            # å¯¹äºçœç•¥å·ï¼Œä¼˜å…ˆé€‰æ‹©çœŸæ­£çš„çœç•¥å·('...'æˆ–'â€¦')
            real_ellipsis_indices = []
            for idx in potential_split_indices_by_priority['ellipsis']:
                w_text = words_to_process[idx].text.strip()
                if '...' in w_text or 'â€¦' in w_text:  # çœŸæ­£çš„çœç•¥å·
                    real_ellipsis_indices.append(idx)

            if real_ellipsis_indices:
                # ä»çœŸæ­£çš„çœç•¥å·ä¸­é€‰æ‹©è·ç¦»ä¸­å¿ƒæœ€è¿‘çš„
                best_split_index = find_closest(real_ellipsis_indices)
            else:
                # å¦‚æœæ²¡æœ‰çœŸæ­£çš„çœç•¥å·ï¼Œé€‰æ‹©æœ€æ¥è¿‘çš„ä¼ªçœç•¥å·
                best_split_index = find_closest(potential_split_indices_by_priority['ellipsis'])
        elif potential_split_indices_by_priority['comma']:
            best_split_index = find_closest(potential_split_indices_by_priority['comma'])

        # å¦‚æœæ²¡æœ‰æ ‡ç‚¹ï¼Œåœ¨ä¸­é—´åˆ†å‰²
        if best_split_index == -1:
            best_split_index = len(words_to_process) // 2

        # æ‰§è¡Œåˆ†å‰²
        first_segment_words = words_to_process[:best_split_index+1]
        second_segment_words = words_to_process[best_split_index+1:]

        # æ™ºèƒ½åˆ†å‰²çº é”™åçš„æ–‡æœ¬ï¼Œä¿æŒAIçº é”™ç»“æœ
        corrected_segments = self._split_corrected_text_by_words(sentence_text, first_segment_words, second_segment_words)

        # å¤„ç†ç¬¬ä¸€æ®µ
        first_text = corrected_segments["first"]
        # å¦‚æœæ™ºèƒ½åˆ†å‰²å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹é€»è¾‘
        if not first_text:
            first_text = "".join([w.text for w in first_segment_words])
        first_start = first_segment_words[0].start_time
        # å¯¹äºç¬¬ä¸€æ®µï¼Œä½¿ç”¨è¯çš„åŸå§‹ç»“æŸæ—¶é—´ï¼›åªæœ‰åœ¨æ²¡æœ‰ç¬¬äºŒæ®µæ—¶æ‰è€ƒè™‘override_end_time
        first_end = first_segment_words[-1].end_time
        first_duration = first_end - first_start

        # æ£€æŸ¥åˆ†å‰²åçš„ç‰‡æ®µæ—¶é•¿
        if first_duration > self.max_duration:
            # é€’å½’åˆ†å‰²ï¼šå†æ¬¡è°ƒç”¨split_long_sentenceæ¥å¤„ç†è¶…é™çš„ç¬¬ä¸€æ®µ
            entries.extend(self.split_long_sentence(first_text, first_segment_words, first_start, first_end, _recursion_depth + 1, override_end_time))
        else:
            first_entry = SubtitleEntry(0, first_start, first_end, first_text, first_segment_words)

            # === [ä¿®å¤] å¯¹è¿”å›æ¡ç›®åº”ç”¨æ—¶é—´æ£€æµ‹ä¿®æ­£ ===
            if first_entry.duration < app_config.MIN_DURATION_ABSOLUTE:
                first_entry.end_time = first_entry.start_time + app_config.MIN_DURATION_ABSOLUTE

            # å¯¹ééŸ³é¢‘äº‹ä»¶ä¸”æ—¶é•¿åˆç†çš„æ¡ç›®åº”ç”¨Mode Bæ—¶é—´æ£€æµ‹
            is_audio_event = self._is_bracketed_content(first_text)
            if not is_audio_event and len(first_segment_words) > 1 and first_duration <= self.max_duration:
                # åº”ç”¨ Mode B çš„æ—¶é—´ä¿®æ­£é€»è¾‘
                corrected_end_time = self._apply_end_time_correction(
                    first_segment_words,
                    first_entry.end_time,
                    first_entry.start_time
                )
                if corrected_end_time != first_entry.end_time:
                    self.log(f"å­—å¹•æ—¶é—´ä¼˜åŒ–: å¯¹åˆ†å‰²æ¡ç›®åº”ç”¨æ—¶é—´ä¿®æ­£ (åŸæ—¶é•¿: {first_entry.duration:.2f}s -> ä¿®æ­£å: {corrected_end_time - first_entry.start_time:.2f}s)")
                    first_entry.end_time = corrected_end_time

            entries.append(first_entry)

        # å¤„ç†ç¬¬äºŒæ®µ
        if second_segment_words:  # ç¡®ä¿è¿˜æœ‰è¯æ±‡
            second_text = corrected_segments["second"]
            # å¦‚æœæ™ºèƒ½åˆ†å‰²å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹é€»è¾‘
            if not second_text:
                second_text = "".join([w.text for w in second_segment_words])
            second_start = second_segment_words[0].start_time
            # å¯¹äºç¬¬äºŒæ®µï¼ˆæœ€åä¸€æ®µï¼‰ï¼Œä½¿ç”¨override_end_timeï¼ˆå¦‚æœæä¾›äº†çš„è¯ï¼‰
            second_end = override_end_time if override_end_time is not None else second_segment_words[-1].end_time
            second_duration = second_end - second_start

            # æ£€æŸ¥åˆ†å‰²åçš„ç‰‡æ®µæ—¶é•¿
            if second_duration > self.max_duration:
                # é€’å½’åˆ†å‰²ï¼šå†æ¬¡è°ƒç”¨split_long_sentenceæ¥å¤„ç†è¶…é™çš„ç¬¬äºŒæ®µ
                entries.extend(self.split_long_sentence(second_text, second_segment_words, second_start, second_end, _recursion_depth + 1, override_end_time))
            else:
                second_entry = SubtitleEntry(0, second_start, second_end, second_text, second_segment_words)

                # === [ä¿®å¤] å¯¹è¿”å›æ¡ç›®åº”ç”¨æ—¶é—´æ£€æµ‹ä¿®æ­£ ===
                if second_entry.duration < app_config.MIN_DURATION_ABSOLUTE:
                    second_entry.end_time = second_entry.start_time + app_config.MIN_DURATION_ABSOLUTE

                # å¯¹ééŸ³é¢‘äº‹ä»¶ä¸”æ—¶é•¿åˆç†çš„æ¡ç›®åº”ç”¨Mode Bæ—¶é—´æ£€æµ‹
                is_audio_event = self._is_bracketed_content(second_text)
                if not is_audio_event and len(second_segment_words) > 1 and second_duration <= self.max_duration:
                    # åº”ç”¨ Mode B çš„æ—¶é—´ä¿®æ­£é€»è¾‘
                    corrected_end_time = self._apply_end_time_correction(
                        second_segment_words,
                        second_entry.end_time,
                        second_entry.start_time
                    )
                    if corrected_end_time != second_entry.end_time:
                        self.log(f"å­—å¹•æ—¶é—´ä¼˜åŒ–: å¯¹åˆ†å‰²æ¡ç›®åº”ç”¨æ—¶é—´ä¿®æ­£ (åŸæ—¶é•¿: {second_entry.duration:.2f}s -> ä¿®æ­£å: {corrected_end_time - second_entry.start_time:.2f}s)")
                        second_entry.end_time = corrected_end_time

                entries.append(second_entry)

        # === [æ–°å¢] å¯¹åˆ†å‰²åçš„æ¡ç›®è¿›è¡Œé—´è·éªŒè¯ ===
        if len(entries) >= 2:
            entries = self._validate_and_adjust_split_spacing(entries)

        return entries

    def _split_corrected_text_by_words(self, corrected_text: str, first_words: List[TimestampedWord],
                                     second_words: List[TimestampedWord]) -> Dict[str, str]:
        """
        æ ¹æ®è¯æ±‡åˆ†å‰²ç‚¹æ™ºèƒ½åˆ†å‰²çº é”™åçš„æ–‡æœ¬

        Args:
            corrected_text: AIçº é”™åçš„å®Œæ•´æ–‡æœ¬
            first_words: ç¬¬ä¸€æ®µè¯æ±‡ï¼ˆåŒ…å«åˆ†å‰²ç‚¹ï¼‰
            second_words: ç¬¬äºŒæ®µè¯æ±‡

        Returns:
            åŒ…å«åˆ†å‰²åä¸¤æ®µæ–‡æœ¬çš„å­—å…¸: {"first": "ç¬¬ä¸€æ®µ", "second": "ç¬¬äºŒæ®µ"}
        """
        if not second_words:
            return {"first": corrected_text, "second": ""}

        first_segment_text = "".join([w.text for w in first_words])
        second_segment_text = "".join([w.text for w in second_words])

        # é‡æ–°æ„å»ºåŸå§‹çš„è¯æ±‡é¡ºåºï¼Œç”¨äºå¯¹é½
        all_words = first_words + second_words
        original_text = "".join([w.text for w in all_words])

        # æ–¹æ³•1ï¼šç›´æ¥ä½¿ç”¨è¯æ±‡æ–‡æœ¬æ‹¼æ¥ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        # è¿™æ˜¯æœ€å‡†ç¡®çš„æ–¹æ³•ï¼Œå› ä¸ºå®ƒç›´æ¥ä½¿ç”¨è¯æ±‡åˆ†å‰²ç»“æœ
        result = {"first": first_segment_text, "second": second_segment_text}

        # éªŒè¯æ‹¼æ¥ç»“æœæ˜¯å¦ä¸åŸå§‹æ–‡æœ¬åŒ¹é…ï¼ˆå»é™¤ç©ºæ ¼åï¼‰
        combined = (first_segment_text + second_segment_text).replace(" ", "")
        original_clean = corrected_text.replace(" ", "")

        if combined == original_clean:
            return result
        else:
            # æ–¹æ³•2ï¼šæ™ºèƒ½å¯¹é½ï¼ˆfallbackï¼‰
            # åœ¨çº é”™æ–‡æœ¬ä¸­å¯»æ‰¾ç¬¬ä¸€æ®µæ–‡æœ¬çš„èµ·å§‹ä½ç½®
            first_text_clean = first_segment_text.replace(" ", "")

        # å¯»æ‰¾ç¬¬ä¸€æ®µåœ¨çº é”™æ–‡æœ¬ä¸­çš„ä½ç½®
            split_pos = -1
            if first_text_clean in corrected_text.replace(" ", ""):
                # ç›´æ¥åŒ¹é…
                corrected_clean = corrected_text.replace(" ", "")
                split_pos = corrected_clean.find(first_text_clean) + len(first_text_clean)
            else:
                # æ¨¡ç³ŠåŒ¹é…ï¼šå¯»æ‰¾æœ€æ¥è¿‘çš„åŒ¹é…
                best_match_pos = -1
                best_match_ratio = 0

                for i in range(len(corrected_text)):
                    for j in range(i + min(5, len(corrected_text) - i), len(corrected_text) + 1):
                        segment = corrected_text[i:j]
                        # è®¡ç®—ä¸ç¬¬ä¸€æ®µçš„ç›¸ä¼¼åº¦
                        if len(first_text_clean) > 0 and len(segment) > 0:
                            common_chars = sum(1 for a, b in zip(first_text_clean[:len(segment)], segment) if a == b)
                            ratio = common_chars / max(len(segment), len(first_text_clean[:len(segment)]))

                            if ratio > best_match_ratio and ratio > 0.7:  # è‡³å°‘70%åŒ¹é…
                                best_match_ratio = ratio
                                best_match_pos = i + len(segment)

                if best_match_pos > 0:
                    split_pos = best_match_pos

        # å¦‚æœæ‰¾åˆ°äº†åˆ†å‰²ä½ç½®ï¼Œä½¿ç”¨å®ƒ
            if split_pos > 0 and split_pos < len(corrected_text):
                first_text = corrected_text[:split_pos].strip()
                second_text = corrected_text[split_pos:].strip()
                result = {"first": first_text, "second": second_text}
                return result

        # æ–¹æ³•3ï¼šå›é€€åˆ°åŸå§‹é€»è¾‘
        return {"first": first_segment_text, "second": second_segment_text}

    def _validate_and_adjust_split_spacing(self, entries: List[SubtitleEntry]) -> List[SubtitleEntry]:
        """
        å¯¹åˆ†å‰²åçš„å­—å¹•æ¡ç›®è¿›è¡Œé—´è·éªŒè¯å’Œè°ƒæ•´

        ç¡®ä¿åˆ†å‰²åçš„ç›¸é‚»å­—å¹•ä¹‹é—´ä¿æŒç”¨æˆ·è®¾å®šçš„æœ€å°é—´è·è¦æ±‚

        Args:
            entries: åˆ†å‰²åçš„å­—å¹•æ¡ç›®åˆ—è¡¨

        Returns:
            ç»è¿‡é—´è·éªŒè¯å’Œè°ƒæ•´çš„å­—å¹•æ¡ç›®åˆ—è¡¨
        """
        if len(entries) < 2:
            return entries

        min_spacing_seconds = self.default_gap_ms / 1000.0
        adjustments_made = 0

        self.log(f"   ğŸ” åˆ†å‰²é—´è·éªŒè¯ï¼šæ£€æŸ¥{len(entries)}ä¸ªåˆ†å‰²æ¡ç›®çš„æœ€å°é—´è· (è¦æ±‚: {self.default_gap_ms}ms)")

        for i in range(len(entries) - 1):
            current_entry = entries[i]
            next_entry = entries[i + 1]

            # è®¡ç®—å½“å‰é—´è·
            current_gap = next_entry.start_time - current_entry.end_time

            if current_gap < min_spacing_seconds:
                self.log(f"   ğŸ” æ£€æµ‹åˆ°åˆ†å‰²é—´è·è¿‡å°ï¼šå­—å¹•{current_entry.index} -> å­—å¹•{next_entry.index} "
                        f"(å½“å‰é—´è·: {current_gap:.3f}s, è¦æ±‚æœ€å°é—´è·: {min_spacing_seconds:.3f}s)")

                # åº”ç”¨é—´è·è°ƒæ•´é€»è¾‘
                adjustment_needed = min_spacing_seconds - current_gap

                # æ£€æŸ¥è°ƒæ•´çš„å®‰å…¨æ€§ï¼šä½¿ç”¨0.35sé˜ˆå€¼
                max_safe_adjustment = 0.35

                if adjustment_needed <= max_safe_adjustment:
                    # å®‰å…¨è°ƒæ•´ï¼šç§»åŠ¨ä¸‹ä¸€ä¸ªå­—å¹•çš„å¼€å§‹æ—¶é—´
                    new_start_time = next_entry.start_time + adjustment_needed

                    # ç¡®ä¿ä¸ä¼šä¸åç»­å­—å¹•äº§ç”Ÿé‡å 
                    if i + 2 < len(entries):
                        following_entry = entries[i + 2]
                        if new_start_time + 0.1 >= following_entry.start_time:  # ä¿ç•™0.1så®‰å…¨è·ç¦»
                            self.log(f"   âš ï¸ è°ƒæ•´å—é™ï¼šä¼šä¸å­—å¹•{following_entry.index}é‡å ")
                            new_start_time = following_entry.start_time - 0.1
                            adjustment_needed = new_start_time - next_entry.start_time

                    # åº”ç”¨è°ƒæ•´
                    if adjustment_needed > 0.001:  # åªè¿›è¡Œæœ‰æ„ä¹‰çš„è°ƒæ•´
                        original_duration = next_entry.duration
                        next_entry.start_time = new_start_time
                        self.log(f"   âœ… è°ƒæ•´åˆ†å‰²å­—å¹•{next_entry.index}å¼€å§‹æ—¶é—´: +{adjustment_needed:.3f}s "
                                f"(æ—¶é•¿: {original_duration:.3f}s -> {next_entry.duration:.3f}s)")
                        adjustments_made += 1
                else:
                    # è°ƒæ•´é‡è¿‡å¤§ï¼Œè®°å½•è­¦å‘Šä½†ä¸è°ƒæ•´
                    self.log(f"   âš ï¸ è·³è¿‡è°ƒæ•´ï¼šæ‰€éœ€è°ƒæ•´é‡({adjustment_needed:.3f}s)è¶…è¿‡å®‰å…¨é˜ˆå€¼({max_safe_adjustment:.3f}s)")

        if adjustments_made == 0:
            self.log("   ğŸ” åˆ†å‰²é—´è·éªŒè¯ï¼šæœªå‘ç°éœ€è¦è°ƒæ•´çš„é—´è·é—®é¢˜")
        else:
            self.log(f"   ğŸ” åˆ†å‰²é—´è·éªŒè¯ï¼šå®Œæˆï¼Œå…±è°ƒæ•´äº† {adjustments_made} ä¸ªåˆ†å‰²å­—å¹•çš„æ—¶åº")

        return entries

    # --- æ™ºèƒ½åˆå¹¶ç®—æ³•è¾…åŠ©å‡½æ•° (ç§»æ¤è‡ª Scribe2SRT) ---
    def _filter_low_confidence_words(self, words: List[TimestampedWord]) -> List[TimestampedWord]:
        """
        è¿‡æ»¤ä½ç½®ä¿¡åº¦è¯æ±‡ï¼Œæ’é™¤åŒ…å«æ ‡ç‚¹ç¬¦å·çš„è¯æ±‡

        Args:
            words: è¯æ±‡åˆ—è¡¨

        Returns:
            è¿‡æ»¤åçš„ä½ç½®ä¿¡åº¦è¯æ±‡åˆ—è¡¨ï¼ˆå·²æ’é™¤æ ‡ç‚¹ç¬¦å·ï¼‰
        """
        all_punctuation = app_config.ALL_SPLIT_PUNCTUATION
        filtered_words = []

        for word in words:
            # åªä¿ç•™çœŸæ­£ä½ç½®ä¿¡åº¦ä¸”ä¸åŒ…å«æ ‡ç‚¹ç¬¦å·çš„è¯æ±‡
            if word.confidence < self.SONIOX_THRESHOLDS["CONF_LIMIT"]:
                if not self.check_word_has_punctuation(word.text, all_punctuation):
                    filtered_words.append(word)

        return filtered_words

    def _is_cjk(self, text: str) -> bool:
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å« CJK (ä¸­æ—¥éŸ©) å­—ç¬¦"""
        for char in text:
            if '\u4e00' <= char <= '\u9fff' or \
               '\u3040' <= char <= '\u309f' or \
               '\u30a0' <= char <= '\u30ff' or \
               '\uac00' <= char <= '\ud7af':
                return True
        return False

    def _calculate_cps(self, text: str, duration: float) -> float:
        """è®¡ç®—æ¯ç§’å­—ç¬¦æ•° (CPS)"""
        if duration <= 0: return 999.0
        # å»é™¤ç©ºç™½å­—ç¬¦è®¡ç®—å®é™…å­—ç¬¦æ•°
        char_count = len(re.sub(r'\s+', '', text))
        return char_count / duration

    def _can_merge_entries(self, entry1: SubtitleEntry, entry2: SubtitleEntry) -> tuple[bool, str]:
        """æ£€æŸ¥ä¸¤ä¸ªæ¡ç›®æ˜¯å¦å¯ä»¥åˆå¹¶"""
        # 1. æ£€æŸ¥éŸ³é¢‘äº‹ä»¶ (Audio Events)
        # ä»»ä½•åŒ…å«éŸ³é¢‘äº‹ä»¶çš„æ¡ç›®éƒ½ä¸åº”åˆå¹¶
        is_evt1 = self._is_bracketed_content(entry1.text) or (self._is_audio_event_words(entry1.words_used) if entry1.words_used else False)
        is_evt2 = self._is_bracketed_content(entry2.text) or (self._is_audio_event_words(entry2.words_used) if entry2.words_used else False)
        if is_evt1 or is_evt2: return False, "åŒ…å«éŸ³é¢‘äº‹ä»¶"

        # 2. æ£€æŸ¥æ—¶é—´é—´éš” (Gap)
        gap = entry2.start_time - entry1.end_time
        if gap > 2.0: return False, "æ—¶é—´é—´éš”è¿‡å¤§"
        
        # 3. æ£€æŸ¥åˆå¹¶åçš„æ—¶é•¿ (Duration)
        merged_duration = entry2.end_time - entry1.start_time
        if merged_duration > self.max_duration: return False, "åˆå¹¶åæ—¶é•¿è¿‡é•¿"

        # 4. æ£€æŸ¥æ–‡æœ¬é•¿åº¦å’Œ CPS
        # ç¡®å®šåˆ†éš”ç¬¦ï¼šå¦‚æœæ˜¯ä¸¤ä¸ª CJK æ–‡æœ¬ï¼Œä¸­é—´ä¸åŠ ç©ºæ ¼
        sep = "" if (self._is_cjk(entry1.text) and self._is_cjk(entry2.text)) else " "
        merged_text = entry1.text + sep + entry2.text
        
        if len(merged_text) > self.max_chars_per_line: return False, "åˆå¹¶åæ–‡æœ¬è¿‡é•¿"
        
        cps = self._calculate_cps(merged_text, merged_duration)
        # åŠ¨æ€ CPS é™åˆ¶ï¼šCJK ç¨å¾®ä¸¥æ ¼ä¸€ç‚¹ï¼ŒLatin å®½æ¾ä¸€ç‚¹
        max_cps = 13.0 if self._is_cjk(merged_text) else 18.0 
        if cps > max_cps: return False, f"åˆå¹¶åè¯­é€Ÿè¿‡å¿« (CPS: {cps:.1f})"

        return True, "OK"

    def _calculate_merge_benefit(self, entry1: SubtitleEntry, entry2: SubtitleEntry) -> float:
        """è®¡ç®—åˆå¹¶æ”¶ç›Šåˆ†æ•° (åˆ†æ•°è¶Šé«˜è¶Šå€¼å¾—åˆå¹¶)"""
        score = 0.0
        
        # 1. æ—¶é•¿æ”¶ç›Šï¼šåˆå¹¶è¿‡çŸ­çš„æ¡ç›®æ”¶ç›Šå¾ˆé«˜
        if entry1.duration < self.min_duration_target:
            score += (self.min_duration_target - entry1.duration) * 20
        if entry2.duration < self.min_duration_target:
            score += (self.min_duration_target - entry2.duration) * 20
            
        # 2. é—´éš”æ”¶ç›Šï¼šé—´éš”è¶Šå°è¶Šå¥½
        gap = entry2.start_time - entry1.end_time
        if gap < 0.3:
            score += (0.3 - gap) * 10
        elif gap < 0.5:
            score += (0.5 - gap) * 5
            
        # 3. æ–‡æœ¬é•¿åº¦æ”¶ç›Šï¼šåˆå¹¶æçŸ­æ–‡æœ¬æ”¶ç›Šè¾ƒé«˜
        if len(entry1.text) < 5: score += 5
        if len(entry2.text) < 5: score += 5
        
        return score

    def _merge_two_entries(self, entry1: SubtitleEntry, entry2: SubtitleEntry) -> SubtitleEntry:
        """æ‰§è¡Œåˆå¹¶æ“ä½œ"""
        # æ™ºèƒ½å¤„ç†ç©ºæ ¼
        sep = "" if (self._is_cjk(entry1.text) and self._is_cjk(entry2.text)) else " "
        merged_text = entry1.text + sep + entry2.text
        
        merged_words = (entry1.words_used or []) + (entry2.words_used or [])
        merged_ratio = min(entry1.alignment_ratio, entry2.alignment_ratio)
        
        return SubtitleEntry(0, entry1.start_time, entry2.end_time, merged_text, merged_words, merged_ratio)
    # --- æ™ºèƒ½åˆå¹¶ç®—æ³•ç»“æŸ ---

    def _process_mode_c_soniox(self, entries: List[SubtitleEntry], parsed_transcription: Optional[ParsedTranscription] = None) -> List[str]:
        """
        Mode C: Sonioxä¸“ç”¨å¤„ç†é€»è¾‘

        Args:
            entries: å­—å¹•æ¡ç›®åˆ—è¡¨ï¼Œä¼šè¢«ç›´æ¥ä¿®æ”¹
            parsed_transcription: è½¬å½•æ•°æ®ï¼ˆå¯é€‰ï¼ŒåŒ…å«å…ƒæ•°æ®ï¼‰

        Returns:
            List[str]: æ ¡å¯¹æç¤ºåˆ—è¡¨
        """
        self.log("--- å¼€å§‹Mode Cå¤„ç†ï¼šSonioxä¸“ç”¨æ—¶é—´ä¼˜åŒ– ---")

        low_conf_hints: List[str] = []
        i = 0

        while i < len(entries):
            curr = entries[i]
            next_entry = entries[i + 1] if i + 1 < len(entries) else None

            # 1. æ”¶é›†ä½ç½®ä¿¡åº¦è¯ (ç”¨äºç”Ÿæˆæ ¡å¯¹æŠ¥å‘Š)ï¼Œæ’é™¤æ ‡ç‚¹ç¬¦å·
            low_conf_words = self._filter_low_confidence_words(curr.words_used)
            if low_conf_words:
                # è·å–ä¸Šä¸‹æ–‡ï¼šå‰åå„ä¸€ä¸ªæ¡ç›®
                prev_text = entries[i-1].text if i > 0 else ""
                next_text = entries[i+1].text if i+1 < len(entries) else ""

                # æ ¼å¼åŒ–æ ¡å¯¹æç¤º
                low_conf_words_str = ", ".join([f"{w.text}({w.confidence:.2f})" for w in low_conf_words])
                hint = f"ä½ç½®ä¿¡åº¦è¯æ±‡: {low_conf_words_str}\n"
                hint += f"ä¸Šä¸‹æ–‡: {prev_text} [{curr.text}] {next_text}\n"
                hint += f"æ—¶é—´: {self.format_timecode(curr.start_time)} --> {self.format_timecode(curr.end_time)}\n"
                hint += "-" * 50
                low_conf_hints.append(hint)

            if next_entry:
                gap = next_entry.start_time - curr.end_time

                # 2. æ€¥é€Ÿè¿è¯»å¤„ç† (é€»è¾‘â‘¢)
                if gap < self.SONIOX_THRESHOLDS["RAPID_GAP"]:
                    self.log(f"è¿è¯»åˆå¹¶: é—´è·{gap:.2f}s < {self.SONIOX_THRESHOLDS['RAPID_GAP']}s, åˆå¹¶æ¡ç›®")
                    merged_entry = self._merge_two_entries(curr, next_entry)
                    # ç”¨åˆå¹¶åçš„æ¡ç›®æ›¿æ¢å½“å‰æ¡ç›®
                    entries[i] = merged_entry
                    entries.pop(i + 1)  # ç§»é™¤å·²åˆå¹¶çš„ä¸‹ä¸€ä¸ªæ¡ç›®
                    continue  # è·³è¿‡i+1çš„å¤„ç†

                # 3. å¼‚å¸¸å¤§é—´è·ä¿®æ­£ (é€»è¾‘â‘  - ä»…é’ˆå¯¹ä½ç½®ä¿¡åº¦å¥å°¾)
                if curr.words_used:
                    last_word = curr.words_used[-1]
                    if (last_word.confidence < self.SONIOX_THRESHOLDS["CONF_LIMIT"] and
                        gap > self.SONIOX_THRESHOLDS["LARGE_GAP"]):
                        self.log(f"å¼‚å¸¸ä¿®æ­£: ä½ç½®ä¿¡åº¦({last_word.confidence:.2f}) + å¤§é—´è·({gap:.2f}s), æ‰§è¡Œä¸­ç‚¹åˆ‡æ–­")
                        curr.end_time += (gap / 2)  # ä¸­ç‚¹åˆ‡æ–­
                        gap = next_entry.start_time - curr.end_time  # æ›´æ–°gap

                # 4. èˆ’é€‚åº¦ä¼˜åŒ– (é€»è¾‘â‘¡ & å¼€å§‹æ—¶é—´ä¼˜åŒ–)
                if gap > self.SONIOX_THRESHOLDS["EXT_GAP_MIN"]:
                    # åªæœ‰ç©ºé—´è¶³å¤Ÿå¤§ï¼Œæ‰åŒæ—¶åš"åŠ å°¾å·´"å’Œ"å‰æ‘‡"
                    curr.end_time += self.SONIOX_THRESHOLDS["TAIL_LEN"]       # åŠ å°¾å·´
                    next_entry.start_time -= self.SONIOX_THRESHOLDS["START_PAD"]  # ä¸‹ä¸€å¥å‰æ‘‡
                    self.log(f"èˆ’é€‚åº¦ä¼˜åŒ–: åŠ å°¾å·´{self.SONIOX_THRESHOLDS['TAIL_LEN']}s, å‰æ‘‡{self.SONIOX_THRESHOLDS['START_PAD']}s")

                # 5. ç‰©ç†é˜²é‡å å…œåº•
                if curr.end_time > next_entry.start_time:
                    self.log(f"é˜²é‡å ä¿®æ­£: å¼ºåˆ¶åˆ†ç¦»é‡å æ¡ç›®")
                    curr.end_time = next_entry.start_time - 0.01

            i += 1

        self.log(f"--- Mode Cæ—¶é—´ä¼˜åŒ–å®Œæˆï¼Œç”Ÿæˆ{len(low_conf_hints)}æ¡æ ¡å¯¹æç¤º ---")
        return low_conf_hints

    def _apply_mode_b_time_optimization(self, entries: List[SubtitleEntry]) -> None:
        """
        Mode B: ElevenLabså…¼å®¹æ—¶é—´ä¼˜åŒ–ç­–ç•¥

        å¯¹æ¯ä¸ªå­—å¹•æ¡ç›®å•ç‹¬è¿›è¡Œ3æ­¥æ£€æµ‹å’Œæ—¶é—´ä¿®æ­£ï¼Œç„¶ååˆ¤æ–­æ˜¯å¦éœ€è¦åˆ†å‰²
        """
        self.log("--- å¼€å§‹Mode Bæ—¶é—´ä¼˜åŒ–ï¼šä¸€å¥ä¸€å¥ä¼˜åŒ–æ—¶é—´æˆ³å¯¹æ¯” ---")

        optimized_entries = []

        for entry in entries:
            # è·³è¿‡éŸ³é¢‘äº‹ä»¶
            if self._is_audio_event_words(entry.words_used):
                optimized_entries.append(entry)
                continue

            # è·³è¿‡æ‹¬å·å†…å®¹
            if self._is_bracketed_content(entry.text):
                optimized_entries.append(entry)
                continue

            # è·³è¿‡è¯æ•°ä¸è¶³çš„æ¡ç›®
            if len(entry.words_used) <= 1:
                optimized_entries.append(entry)
                continue

            # ç¬¬ä¸€æ­¥ï¼šåº”ç”¨3æ­¥æ—¶é—´ä¿®æ­£
            original_end_time = entry.end_time
            corrected_end_time = self._apply_end_time_correction(entry.words_used, entry.end_time, entry.start_time)

            # ç¬¬äºŒæ­¥ï¼šé‡æ–°è®¡ç®—æ—¶é•¿ï¼Œåˆ¤æ–­æ˜¯å¦è¶…é™
            corrected_duration = max(0.001, corrected_end_time - entry.start_time)

            # ç¬¬ä¸‰æ­¥ï¼šæ ¹æ®ä¿®æ­£åçš„æ—¶é•¿å†³å®šå¤„ç†æ–¹å¼
            if corrected_duration > self.max_duration or len(entry.text) > self.max_chars_per_line:
                # ä¿®æ­£åä»ç„¶è¶…é™ï¼Œéœ€è¦è¿›è¡Œåˆ†å‰²
                self.log(f"   âš ï¸ Mode B: ä¿®æ­£åä»è¶…é™ï¼Œéœ€åˆ†å‰²: \"{entry.text[:30]}...\" (ä¿®æ­£åæ—¶é•¿: {corrected_duration:.2f}s)")

                # ä½¿ç”¨ä¿®æ­£åçš„æ—¶é—´è¿›è¡Œåˆ†å‰²
                original_text_for_splitting = "".join([w.text for w in entry.words_used])
                split_entries = self.split_long_sentence(
                    original_text_for_splitting,
                    entry.words_used,
                    entry.start_time,
                    corrected_end_time,  # ä½¿ç”¨ä¿®æ­£åçš„æ—¶é—´
                    0,
                    corrected_end_time  # ä¼ é€’override_end_time
                )

                # è®¾ç½®alignment_ratio
                for split_entry in split_entries:
                    split_entry.alignment_ratio = entry.alignment_ratio

                optimized_entries.extend(split_entries)
            else:
                # ä¿®æ­£åä¸è¶…é™ï¼Œä½¿ç”¨ä¿®æ­£åçš„æ—¶é—´
                if corrected_end_time != original_end_time:
                    self.log(f"   âœ¨ Mode B: æ—¶é—´ä¿®æ­£é¿å…äº†åˆ†å‰²: \"{entry.text[:30]}...\" (åŸæ—¶é•¿: {entry.duration:.2f}s -> ä¿®æ­£å: {corrected_duration:.2f}s)")

                # åˆ›å»ºä½¿ç”¨ä¿®æ­£åæ—¶é—´çš„æ–°æ¡ç›®
                optimized_entry = SubtitleEntry(
                    entry.index,
                    entry.start_time,
                    corrected_end_time,
                    entry.text,
                    entry.words_used,
                    entry.alignment_ratio
                )
                optimized_entries.append(optimized_entry)

        # æ›¿æ¢åŸentries
        entries.clear()
        entries.extend(optimized_entries)

        self.log(f"--- Mode Bæ—¶é—´ä¼˜åŒ–å®Œæˆï¼Œå¤„ç†äº†{len(optimized_entries)}ä¸ªæ¡ç›® ---")

    def _apply_mode_b_merge_optimization(self, entries: List[SubtitleEntry]) -> None:
        """
        Mode B: åŸºäºä¼˜åŒ–åæ—¶é—´æˆ³çš„åˆå¹¶ä¼˜åŒ–

        åœ¨æ—¶é—´ä¼˜åŒ–å®Œæˆåï¼ŒåŸºäºä¼˜åŒ–åçš„æ—¶é—´æˆ³è¿›è¡Œæ™ºèƒ½åˆå¹¶å†³ç­–
        """
        self.log("--- å¼€å§‹Mode Båˆå¹¶ä¼˜åŒ–ï¼šåŸºäºä¼˜åŒ–åæ—¶é—´æˆ³ ---")

        # Mode Bä½¿ç”¨é€‚ä¸­çš„åˆå¹¶ç­–ç•¥
        merge_gap_threshold = 0.8  # ä¸åŸæœ‰é€»è¾‘ä¿æŒä¸€è‡´
        self.log(f"Mode B: ä½¿ç”¨é€‚ä¸­çš„åˆå¹¶ç­–ç•¥ (é—´éš™é˜ˆå€¼: {merge_gap_threshold}s)")

        merged_entries: List[SubtitleEntry] = []
        idx_merge = 0
        total_entries = len(entries)

        while idx_merge < total_entries:
            if not self._is_worker_running():
                self.log("ä»»åŠ¡è¢«ç”¨æˆ·ä¸­æ–­(Mode Båˆå¹¶é˜¶æ®µ)ã€‚"); return

            current_entry = entries[idx_merge]
            merged_this_iteration = False

            # å°è¯•ä¸ä¸‹ä¸€æ¡åˆå¹¶ï¼ˆåŸºäºä¼˜åŒ–åçš„æ—¶é—´æˆ³ï¼‰
            if idx_merge + 1 < len(entries):
                next_entry = entries[idx_merge + 1]

                # æ£€æŸ¥æ˜¯å¦æ»¡è¶³åˆå¹¶çš„åŸºæœ¬ç¡¬ä»¶æ€§æ¡ä»¶
                can_merge, reason = self._can_merge_entries(current_entry, next_entry)

                if can_merge:
                    # è®¡ç®—åˆå¹¶æ”¶ç›Šï¼ˆåŸºäºä¼˜åŒ–åçš„æ—¶é—´æˆ³ï¼‰
                    benefit = self._calculate_merge_benefit(current_entry, next_entry)

                    # åªæœ‰æ”¶ç›Šè¶…è¿‡é˜ˆå€¼æ‰åˆå¹¶ (Mode B é»˜è®¤é˜ˆå€¼ 5.0)
                    if benefit > 5.0:
                        self.log(f"   Mode Båˆå¹¶ (æ”¶ç›Š {benefit:.1f}): \"{current_entry.text[:15]}...\" + \"{next_entry.text[:15]}...\"")
                        merged_entry = self._merge_two_entries(current_entry, next_entry)
                        merged_entries.append(merged_entry)
                        idx_merge += 2
                        merged_this_iteration = True
                    else:
                        pass

            if not merged_this_iteration:
                merged_entries.append(current_entry)
                idx_merge += 1

            # ã€ä¿®å¤ã€‘ç§»é™¤ç‹¬ç«‹æ–¹æ³•ä¸­çš„è¿›åº¦æ›´æ–°ï¼Œè½¬ç”±ä¸»æ–¹æ³•process_to_srtå¤„ç†
            # è¿™æ ·å¯ä»¥ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„åŠ¨æ€æƒé‡åˆ†é…
            # current_phase2_progress_component = int(((idx_merge) / total_entries if total_entries > 0 else 1) * self.WEIGHT_MERGE)
            # self._emit_srt_progress(current_phase2_progress_component, 100)

        self.log(f"--- Mode Båˆå¹¶ä¼˜åŒ–å®Œæˆï¼Œå¤„ç†äº†{len(merged_entries)}ä¸ªæ¡ç›® ---")

        # æ›¿æ¢åŸentries
        entries.clear()
        entries.extend(merged_entries)

    def _apply_mode_a_time_optimization(self, entries: List[SubtitleEntry]) -> None:
        """
        Mode A: åŸºç¡€æ—¶é—´ä¼˜åŒ–ç­–ç•¥

        é€‚ç”¨äºWhisperã€Deepgramã€AssemblyAIç­‰æ— ç‰¹è°ƒç­–ç•¥çš„JSONæ ¼å¼ã€‚
        åªè¿›è¡Œå¿…è¦çš„å¤„ç†ï¼Œè·³è¿‡å¤æ‚çš„æ—¶é—´ä¼˜åŒ–ç®—æ³•ã€‚

        Args:
            entries: å­—å¹•æ¡ç›®åˆ—è¡¨ï¼Œä¼šè¢«ç›´æ¥ä¿®æ”¹
        """
        self.log("--- å¼€å§‹Mode Aæ—¶é—´ä¼˜åŒ–ï¼šæœ€å°å¿…è¦å¤„ç† ---")

        # Mode Aåªè¿›è¡Œæœ€åŸºç¡€çš„å®‰å…¨æ£€æŸ¥ï¼Œä¸è¿›è¡Œä»»ä½•å¤æ‚æ—¶é—´ä¼˜åŒ–
        # 1. ç¡®ä¿æ—¶é—´æˆ³åˆç†æ€§ï¼ˆç»“æŸæ—¶é—´ä¸æ—©äºå¼€å§‹æ—¶é—´ï¼‰
        # 2. åº”ç”¨ç»å¯¹æœ€å°æ—¶é•¿è¦æ±‚

        min_duration_absolute = app_config.DEFAULT_MIN_DURATION_ABSOLUTE

        for i, entry in enumerate(entries):
            # ç¡®ä¿ç»“æŸæ—¶é—´è‡³å°‘æ¯”å¼€å§‹æ—¶é—´æ™š1æ¯«ç§’
            if entry.end_time <= entry.start_time:
                self.log(f"åŸºç¡€ä¿®æ­£: æ¡ç›®{i+1}ç»“æŸæ—¶é—´ä¸æ—©äºå¼€å§‹æ—¶é—´")
                entry.end_time = entry.start_time + 0.001

            # åº”ç”¨ç»å¯¹æœ€å°æ—¶é•¿è¦æ±‚ï¼ˆä½†ä¸è¿›è¡Œå…¶ä»–æ—¶é•¿ä¼˜åŒ–ï¼‰
            current_duration = entry.duration
            if current_duration < min_duration_absolute:
                self.log(f"åŸºç¡€ä¿®æ­£: æ¡ç›®{i+1}æ—¶é•¿{current_duration:.2f}s < {min_duration_absolute}sï¼Œè°ƒæ•´")
                entry.end_time = entry.start_time + min_duration_absolute

        self.log(f"--- Mode Aæ—¶é—´ä¼˜åŒ–å®Œæˆï¼Œå¯¹{len(entries)}ä¸ªæ¡ç›®è¿›è¡ŒåŸºç¡€å®‰å…¨æ£€æŸ¥ ---")

    def _apply_mode_c_optimization_to_entries(self, entries: List[SubtitleEntry], parsed_transcription: Optional[ParsedTranscription] = None) -> List[str]:
        """
        Mode C: åœ¨æœ€ç»ˆæ ¼å¼åŒ–å‰å¯¹entriesåº”ç”¨Sonioxä¸“ç”¨ä¼˜åŒ–
        è¿™åœ¨Phase 3æœŸé—´è°ƒç”¨ï¼Œä¼šå½±å“åç»­çš„æ—¶é—´ä¼˜åŒ–é€»è¾‘

        Args:
            entries: å­—å¹•æ¡ç›®åˆ—è¡¨
            parsed_transcription: è½¬å½•æ•°æ®ï¼ˆå¯é€‰ï¼ŒåŒ…å«å…ƒæ•°æ®ï¼‰
        """
        self.log("--- Mode Cé¢„ä¼˜åŒ–: åº”ç”¨Sonioxç½®ä¿¡åº¦å’Œæ—¶é—´è°ƒæ•´ ---")
        hints = []

        # é¦–å…ˆæ”¶é›†ä½ç½®ä¿¡åº¦è¯æ±‡ç”¨äºæ ¡å¯¹æç¤ºï¼Œæ’é™¤æ ‡ç‚¹ç¬¦å·
        for i, entry in enumerate(entries):
            low_conf_words = self._filter_low_confidence_words(entry.words_used)
            if low_conf_words:
                # è·å–ä¸Šä¸‹æ–‡ï¼šå‰åå„ä¸€ä¸ªæ¡ç›®
                prev_text = entries[i-1].text if i > 0 else ""
                next_text = entries[i+1].text if i+1 < len(entries) else ""

                # æ ¼å¼åŒ–æ ¡å¯¹æç¤º
                low_conf_words_str = ", ".join([f"{w.text}({w.confidence:.2f})" for w in low_conf_words])
                hint = f"ä½ç½®ä¿¡åº¦è¯æ±‡: {low_conf_words_str}\n"
                hint += f"ä¸Šä¸‹æ–‡: {prev_text} [{entry.text}] {next_text}\n"
                hint += f"æ—¶é—´: {self.format_timecode(entry.start_time)} --> {self.format_timecode(entry.end_time)}\n"
                hint += "-" * 50
                hints.append(hint)

        # ç„¶ååº”ç”¨æ—¶é—´ä¼˜åŒ–é€»è¾‘
        i = 0
        while i < len(entries):
            curr = entries[i]
            next_entry = entries[i + 1] if i + 1 < len(entries) else None

            if next_entry:
                gap = next_entry.start_time - curr.end_time

                # è¿è¯»åˆå¹¶é¢„å¤„ç† - æ£€æŸ¥åˆå¹¶åæ˜¯å¦è¶…é™
                if gap < self.SONIOX_THRESHOLDS["RAPID_GAP"]:
                    # é¢„è®¡ç®—åˆå¹¶åçš„æ—¶é•¿ï¼Œé˜²æ­¢è¶…é™
                    merged_duration = next_entry.end_time - curr.start_time
                    if merged_duration > self.max_duration:
                        self.log(f"Mode Cè¿è¯»åˆå¹¶è·³è¿‡: é—´éš™{gap:.2f}sä½†åˆå¹¶åæ—¶é•¿{merged_duration:.2f}s > {self.max_duration}s")
                    else:
                        merged_entry = self._merge_two_entries(curr, next_entry)
                        entries[i] = merged_entry
                        entries.pop(i + 1)
                        self.log(f"Mode Cè¿è¯»åˆå¹¶: é—´éš™{gap:.2f}s < {self.SONIOX_THRESHOLDS['RAPID_GAP']}s")
                        continue

                # å¼‚å¸¸å¤§é—´è·ä¿®æ­£ (ä»…é’ˆå¯¹ä½ç½®ä¿¡åº¦å¥å°¾)
                if curr.words_used:
                    last_word = curr.words_used[-1]
                    if (last_word.confidence < self.SONIOX_THRESHOLDS["CONF_LIMIT"] and
                        gap > self.SONIOX_THRESHOLDS["LARGE_GAP"]):
                        self.log(f"Mode Cå¼‚å¸¸ä¿®æ­£: ä½ç½®ä¿¡åº¦({last_word.confidence:.2f}) + å¤§é—´è·({gap:.2f}s)")
                        curr.end_time += (gap / 2)  # ä¸­ç‚¹åˆ‡æ–­
                        gap = next_entry.start_time - curr.end_time

                # èˆ’é€‚åº¦ä¼˜åŒ–é¢„å¤„ç†
                if gap > self.SONIOX_THRESHOLDS["EXT_GAP_MIN"]:
                    curr.end_time += self.SONIOX_THRESHOLDS["TAIL_LEN"]
                    next_entry.start_time -= self.SONIOX_THRESHOLDS["START_PAD"]
                    self.log(f"Mode Cèˆ’é€‚åº¦ä¼˜åŒ–: é—´éš™{gap:.2f}såŠ å°¾å·´å’Œå‰æ‘‡")

                # ç‰©ç†é˜²é‡å å…œåº•
                if curr.end_time > next_entry.start_time:
                    self.log(f"Mode Cé˜²é‡å ä¿®æ­£: å¼ºåˆ¶åˆ†ç¦»é‡å æ¡ç›®")
                    curr.end_time = next_entry.start_time - 0.01

            i += 1

        # ç‰¹æ®Šå¤„ç†ï¼šä¸ºæœ€åä¸€ä¸ªå­—å¹•åŸºäºéŸ³é¢‘å®é™…ç»“æŸæ—¶é—´è¿›è¡Œä¼˜åŒ–
        if len(entries) > 0 and parsed_transcription:
            last_entry = entries[-1]
            soniox_metadata = parsed_transcription.soniox_metadata

            if soniox_metadata and "audio_duration" in soniox_metadata:
                # è·å–éŸ³é¢‘å®é™…ç»“æŸæ—¶é—´ï¼ˆæ¯«ç§’è½¬ç§’ï¼‰
                audio_end_time = soniox_metadata["audio_duration"] / 1000.0

                # å¦‚æœæœ€åä¸€ä¸ªå­—å¹•çš„ç»“æŸæ—¶é—´ç¦»éŸ³é¢‘ç»“æŸè¿˜æœ‰ç©ºé—´ï¼Œåˆ™å»¶é•¿
                gap_to_end = audio_end_time - last_entry.end_time

                if gap_to_end > self.SONIOX_THRESHOLDS["TAIL_LEN"]:
                    # æœ‰è¶³å¤Ÿç©ºé—´ï¼Œæ·»åŠ å°¾å·´
                    last_entry.end_time += self.SONIOX_THRESHOLDS["TAIL_LEN"]
                    self.log(f"Mode Cæœ€åä¸€ä¸ªå­—å¹•ä¼˜åŒ–: éŸ³é¢‘ç»“æŸ{audio_end_time:.2f}sï¼Œå­—å¹•å»¶é•¿{self.SONIOX_THRESHOLDS['TAIL_LEN']}s")
                else:
                    # ç©ºé—´ä¸è¶³ï¼Œå»¶é•¿åˆ°æ¥è¿‘éŸ³é¢‘ç»“æŸ
                    extension = gap_to_end * 0.8  # å»¶é•¿åˆ°è·ç¦»éŸ³é¢‘ç»“æŸè¿˜æœ‰20%ç©ºé—´
                    if extension > 0.1:  # è‡³å°‘å»¶é•¿0.1ç§’æ‰æœ‰æ„ä¹‰
                        last_entry.end_time += extension
                        self.log(f"Mode Cæœ€åä¸€ä¸ªå­—å¹•ä¼˜åŒ–: éŸ³é¢‘ç»“æŸ{audio_end_time:.2f}sï¼Œå­—å¹•å»¶é•¿{extension:.2f}s")
            elif soniox_metadata:
                self.log(f"Mode Cæœ€åä¸€ä¸ªå­—å¹•ä¼˜åŒ–: æ£€æµ‹åˆ°å…ƒæ•°æ®ä½†ç¼ºå°‘audio_durationï¼Œè·³è¿‡ä¼˜åŒ–")
            else:
                # æ²¡æœ‰å…ƒæ•°æ®ï¼Œå¼ºåˆ¶å»¶é•¿0.3ç§’ä½œä¸ºå…œåº•
                tail_extension = 0.3
                last_entry.end_time += tail_extension
                self.log(f"Mode Cæœ€åä¸€ä¸ªå­—å¹•ä¼˜åŒ–: æ— å…ƒæ•°æ®ï¼Œå¼ºåˆ¶å»¶é•¿{tail_extension}s")

        self.log(f"--- Mode Cé¢„ä¼˜åŒ–å®Œæˆï¼Œæ”¶é›†åˆ°{len(hints)}æ¡æ ¡å¯¹æç¤º ---")
        return hints

    def _split_comfort_optimized_entry(self, entry: SubtitleEntry) -> List[SubtitleEntry]:
        """
        å¯¹å·²ç»æ·»åŠ äº†èˆ’é€‚åº¦æ—¶é—´çš„è¶…é™ç‰‡æ®µè¿›è¡Œç‰¹æ®Šåˆ†å‰²
        ä¸é‡å¤æ·»åŠ èˆ’é€‚åº¦æ—¶é—´ï¼Œåªæ ¹æ®æ ‡ç‚¹ç¬¦å·è¿›è¡Œåˆ†å‰²
        """
        self.log(f"   ç‰¹æ®Šåˆ†å‰²ï¼šå¤„ç†èˆ’é€‚åº¦ä¼˜åŒ–åçš„è¶…é™ç‰‡æ®µ (æ—¶é•¿: {entry.duration:.2f}s, è¯æ±‡æ•°: {len(entry.words_used)})")

        # å¦‚æœè¯æ±‡å¤ªå°‘æ— æ³•åˆ†å‰²ï¼Œè¿”å›åŸç‰‡æ®µ
        if len(entry.words_used) <= 1:
            self.log(f"   ç‰¹æ®Šåˆ†å‰²ï¼šè¯æ±‡æ•°å¤ªå°‘({len(entry.words_used)})ï¼Œæ— æ³•åˆ†å‰²")
            return [entry]

        # æŸ¥æ‰¾åˆ†å‰²ç‚¹ï¼ˆæ ‡ç‚¹ç¬¦å·ï¼‰
        split_points = []
        for i, word in enumerate(entry.words_used):
            if self.check_word_has_punctuation(word.text, app_config.ALL_SPLIT_PUNCTUATION):
                split_points.append(i)

        if not split_points:
            self.log(f"   ç‰¹æ®Šåˆ†å‰²ï¼šæœªæ‰¾åˆ°æ ‡ç‚¹ç¬¦å·åˆ†å‰²ç‚¹")
            return [entry]

        # é€‰æ‹©æœ€ä½³åˆ†å‰²ç‚¹ï¼ˆé è¿‘ä¸­é—´ä½ç½®çš„æ ‡ç‚¹ç¬¦å·ï¼‰
        middle_pos = len(entry.words_used) // 2
        best_point = min(split_points, key=lambda x: abs(x - middle_pos))

        self.log(f"   ç‰¹æ®Šåˆ†å‰²ï¼šé€‰æ‹©åˆ†å‰²ç‚¹{best_point}ï¼Œè¯æ±‡: '{entry.words_used[best_point].text}'")

        # åˆ†å‰²è¯æ±‡åˆ—è¡¨
        first_words = entry.words_used[:best_point + 1]
        second_words = entry.words_used[best_point + 1:]

        if not first_words or not second_words:
            self.log(f"   ç‰¹æ®Šåˆ†å‰²ï¼šåˆ†å‰²åæŸéƒ¨åˆ†ä¸ºç©ºï¼Œæ”¾å¼ƒåˆ†å‰²")
            return [entry]

        # è®¡ç®—æ—¶é—´åˆ†é…
        total_duration = entry.end_time - entry.start_time

        # ä½¿ç”¨è¯æ±‡æ•°æ¯”ä¾‹åˆ†é…æ—¶é—´ï¼Œå› ä¸ºèˆ’é€‚åº¦ä¼˜åŒ–ååŸè¯æ±‡æ—¶é—´æˆ³å¯èƒ½å·²ä¸å‡†ç¡®
        first_ratio = len(first_words) / len(entry.words_used)
        first_duration = total_duration * first_ratio
        second_duration = total_duration - first_duration

        # è®¡ç®—åˆ†å‰²ç‚¹çš„åŸºç¡€æ—¶é—´
        split_base_time = entry.start_time + first_duration

        # åŸºäºåŸå§‹è¯æ±‡æ—¶é—´æˆ³è¿›è¡Œèˆ’é€‚åº¦ä¼˜åŒ–
        if first_words and second_words:
            # è·å–ç¬¬ä¸€ç‰‡æ®µæœ€åä¸€ä¸ªè¯å’Œç¬¬äºŒç‰‡æ®µç¬¬ä¸€ä¸ªè¯çš„åŸå§‹æ—¶é—´æˆ³
            last_word_first = first_words[-1]  # "ã§ã€"
            first_word_second = second_words[0]  # "ã‚"

            # è®¡ç®—åŸå§‹é—´éš™ï¼ˆç§’ï¼‰
            original_gap = first_word_second.start_time - last_word_first.end_time
            self.log(f"   ç‰¹æ®Šåˆ†å‰²é—´éš™åˆ†æï¼š'{last_word_first.text}'ç»“æŸ({last_word_first.end_time:.3f}s) åˆ° '{first_word_second.text}'å¼€å§‹({first_word_second.start_time:.3f}s)ï¼Œé—´éš™={original_gap:.3f}s")

            # åˆ¤æ–­æ˜¯å¦éœ€è¦è¿›è¡Œèˆ’é€‚åº¦ä¼˜åŒ–
            if original_gap > self.SONIOX_THRESHOLDS["EXT_GAP_MIN"]:
                # é—´éš™è¶³å¤Ÿå¤§ï¼Œå¯ä»¥æ·»åŠ å°¾å·´å’Œå‰æ‘‡
                # ç¬¬ä¸€ç‰‡æ®µæ·»åŠ å°¾å·´ï¼ˆä½†ä¸èƒ½è¶…è¿‡ç¬¬äºŒç‰‡æ®µå¼€å§‹æ—¶é—´çš„ä¸€åŠï¼‰
                max_tail_space = original_gap / 2
                first_end_adjustment = min(self.SONIOX_THRESHOLDS["TAIL_LEN"], max_tail_space)

                # ç¬¬äºŒç‰‡æ®µæ·»åŠ å‰æ‘‡
                second_start_adjustment = min(self.SONIOX_THRESHOLDS["START_PAD"],
                                           original_gap - first_end_adjustment)

                self.log(f"   ç‰¹æ®Šåˆ†å‰²èˆ’é€‚åº¦ä¼˜åŒ–ï¼šé—´éš™{original_gap:.3f}s > {self.SONIOX_THRESHOLDS['EXT_GAP_MIN']}sï¼Œæ·»åŠ å°¾å·´{first_end_adjustment:.3f}sï¼Œå‰æ‘‡{second_start_adjustment:.3f}s")

                # ä½¿ç”¨åŸå§‹è¯æ±‡æ—¶é—´æˆ³åˆ›å»ºåˆ†å‰²åçš„æ¡ç›®ï¼Œåº”ç”¨èˆ’é€‚åº¦ä¼˜åŒ–
                first_entry = SubtitleEntry(
                    index=entry.index,
                    start_time=entry.start_time,
                    end_time=last_word_first.end_time + first_end_adjustment,  # ä½¿ç”¨åŸå§‹ç»“æŸæ—¶é—´+å°¾å·´
                    text="".join([w.text for w in first_words]),
                    words_used=first_words
                )

                second_entry = SubtitleEntry(
                    index=entry.index + 1,
                    start_time=first_word_second.start_time - second_start_adjustment,  # ä½¿ç”¨åŸå§‹å¼€å§‹æ—¶é—´-å‰æ‘‡
                    end_time=entry.end_time,
                    text="".join([w.text for w in second_words]),
                    words_used=second_words
                )
            else:
                self.log(f"   ç‰¹æ®Šåˆ†å‰²è·³è¿‡ä¼˜åŒ–ï¼šé—´éš™{original_gap:.3f}s <= {self.SONIOX_THRESHOLDS['EXT_GAP_MIN']}sï¼Œä¿æŒåŸå§‹æ—¶é—´")

                # ä¸è¿›è¡Œä¼˜åŒ–ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹è¯æ±‡æ—¶é—´æˆ³
                first_entry = SubtitleEntry(
                    index=entry.index,
                    start_time=entry.start_time,
                    end_time=last_word_first.end_time,  # ä½¿ç”¨åŸå§‹ç»“æŸæ—¶é—´
                    text="".join([w.text for w in first_words]),
                    words_used=first_words
                )

                second_entry = SubtitleEntry(
                    index=entry.index + 1,
                    start_time=first_word_second.start_time,  # ä½¿ç”¨åŸå§‹å¼€å§‹æ—¶é—´
                    end_time=entry.end_time,
                    text="".join([w.text for w in second_words]),
                    words_used=second_words
                )
        else:
            # æ²¡æœ‰è¯æ±‡çš„è¾¹ç•Œæƒ…å†µï¼Œä½¿ç”¨ç®€å•çš„æ¯”ä¾‹åˆ†å‰²
            first_entry = SubtitleEntry(
                index=entry.index,
                start_time=entry.start_time,
                end_time=entry.start_time + first_duration,
                text="".join([w.text for w in first_words]),
                words_used=first_words
            )

            second_entry = SubtitleEntry(
                index=entry.index + 1,
                start_time=entry.start_time + first_duration,
                end_time=entry.end_time,
                text="".join([w.text for w in second_words]),
                words_used=second_words
            )

        self.log(f"   ç‰¹æ®Šåˆ†å‰²å®Œæˆï¼šç‰‡æ®µ1({first_entry.duration:.2f}s, {len(first_words)}è¯), ç‰‡æ®µ2({second_entry.duration:.2f}s, {len(second_words)}è¯)")

        # === [æ–°å¢] å¯¹èˆ’é€‚åº¦ä¼˜åŒ–åˆ†å‰²åçš„æ¡ç›®è¿›è¡Œé—´è·éªŒè¯ ===
        split_entries = [first_entry, second_entry]
        split_entries = self._validate_and_adjust_split_spacing(split_entries)

        return split_entries

    def _apply_mode_a_optimization_to_entries(self, entries: List[SubtitleEntry]) -> None:
        """
        Mode A: åœ¨æœ€ç»ˆæ ¼å¼åŒ–å‰å¯¹entriesåº”ç”¨åŸºç¡€ä¼˜åŒ–
        è¿™åœ¨Phase 3æœŸé—´è°ƒç”¨ï¼Œç¡®ä¿åªè¿›è¡Œæœ€å°å¿…è¦å¤„ç†
        """
        self.log("--- Mode Aé¢„ä¼˜åŒ–: åº”ç”¨åŸºç¡€å®‰å…¨æ£€æŸ¥ ---")

        # åªè¿›è¡Œæœ€åŸºç¡€çš„å®‰å…¨æ£€æŸ¥
        for entry in entries:
            if entry.end_time <= entry.start_time:
                entry.end_time = entry.start_time + 0.001

    # --- AI é”™è¯æ ¡å¯¹æ–¹æ³• (ä»…ç”¨äºSonioxæ¨¡å¼) ---
    def _mark_low_confidence_words(self, words: List[TimestampedWord]) -> List[TimestampedWord]:
        """
        æ ‡è®°ä½ç½®ä¿¡åº¦è¯æ±‡ï¼Œç”¨äºåç»­çº é”™

        Args:
            words: Sonioxè¿”å›çš„å¸¦ç½®ä¿¡åº¦çš„è¯æ±‡åˆ—è¡¨

        Returns:
            æ ‡è®°åçš„è¯æ±‡åˆ—è¡¨ï¼Œä½ç½®ä¿¡åº¦è¯æ±‡ä¼šè¢«ç‰¹æ®Šæ ‡è®°
        """
        marked_words = []
        for word in words:
            # åˆ›å»ºè¯æ±‡å‰¯æœ¬ä»¥é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
            marked_word = TimestampedWord(
                text=word.text,
                start_time=word.start_time,
                end_time=word.end_time,
                speaker_id=word.speaker_id,
                confidence=word.confidence
            )

            # å¦‚æœç½®ä¿¡åº¦ä½äºé˜ˆå€¼ï¼Œæ·»åŠ æ ‡è®°
            if word.confidence < self.SONIOX_THRESHOLDS["CONF_LIMIT"]:
                marked_word.text = f"ã€{word.text}ã€‘"

            marked_words.append(marked_word)

        return marked_words

    def _apply_soniox_ultimate_optimization(self, srt_lines: List[str]) -> List[str]:
        """
        Sonioxä¸“ç”¨ç»ˆæä¼˜åŒ–ï¼šåŠ¨æ€å‰ç§»å­—å¹•å¼€å§‹æ—¶é—´
        æ ¹æ®ä¸Šä¸€ä¸ªå­—å¹•ç»“æŸæ—¶é—´åˆ°å½“å‰å­—å¹•å¼€å§‹æ—¶é—´çš„è·ç¦»é™¤ä»¥25æ¥è®¡ç®—å‰ç§»é‡

        Args:
            srt_lines: å·²ç”Ÿæˆçš„SRTè¡Œåˆ—è¡¨

        Returns:
            ä¼˜åŒ–åçš„SRTè¡Œåˆ—è¡¨
        """
        if not srt_lines:
            return srt_lines

        
        # ç¬¬ä¸€æ­¥ï¼šæå–æ‰€æœ‰å­—å¹•ä¿¡æ¯
        subtitles = []

        for entry_str in srt_lines:
            entry_lines = entry_str.strip().split('\n')
            if len(entry_lines) >= 3:
                # ç¬¬ä¸€è¡Œåº”è¯¥æ˜¯å­—å¹•ç¼–å·
                if entry_lines[0].strip().isdigit():
                    current_num = int(entry_lines[0].strip())

                    # ç¬¬äºŒè¡Œåº”è¯¥æ˜¯æ—¶é—´æˆ³
                    if '-->' in entry_lines[1]:
                        time_line = entry_lines[1].strip()
                        start_str, end_str = time_line.split(' --> ')

                        start_time = self._parse_srt_time(start_str)
                        end_time = self._parse_srt_time(end_str)

                        # å‰©ä¸‹çš„è¡Œæ˜¯å­—å¹•å†…å®¹
                        content_lines = []
                        for content_line in entry_lines[2:]:
                            if content_line.strip():  # åªæ·»åŠ éç©ºè¡Œ
                                content_lines.append(content_line.strip())

                        subtitles.append({
                            'number': current_num,
                            'start': start_time,
                            'end': end_time,
                            'content': content_lines,
                            'entry_str': entry_str  # ä¿å­˜åŸå§‹æ¡ç›®å­—ç¬¦ä¸²
                        })

        if not subtitles:
            return srt_lines

        # ç¬¬äºŒæ­¥ï¼šè¿›è¡Œä¼˜åŒ–å¹¶é‡æ–°æ„å»ºSRT
        optimized_entries = []

        # å…ˆè®¡ç®—æ‰€æœ‰è°ƒæ•´é‡
        adjustments = {}
        for subtitle in subtitles:
            current_num = subtitle['number']
            current_start = subtitle['start']
            current_end = subtitle['end']

            adjustment = 0.0
            if current_num == 1:
                # ç¬¬ä¸€ä¸ªå­—å¹•ï¼šä»0ç§’åˆ°å¼€å§‹æ—¶é—´çš„è·ç¦»é™¤ä»¥25ï¼Œæœ€å¤§ä¸è¶…è¿‡0.6ç§’
                adjustment = min(current_start / 25.0, 0.6)
                if adjustment > 0.001:  # åªè®°å½•æœ‰æ„ä¹‰çš„è°ƒæ•´
                    self.log(f"   âš¡ ç»ˆæä¼˜åŒ–ï¼šå­—å¹•{current_num} å‰ç§» {adjustment:.3f}s")
            else:
                # æ‰¾åˆ°ä¸Šä¸€ä¸ªå­—å¹•
                prev_subtitle = None
                for s in subtitles:
                    if s['number'] == current_num - 1:
                        prev_subtitle = s
                        break

                if prev_subtitle:
                    gap = current_start - prev_subtitle['end']
                    adjustment = min(gap / 20.0, 0.5)  # æœ€å¤§ä¸è¶…è¿‡0.5ç§’
                    if adjustment > 0.001:  # åªè®°å½•æœ‰æ„ä¹‰çš„è°ƒæ•´
                        self.log(f"   âš¡ ç»ˆæä¼˜åŒ–ï¼šå­—å¹•{current_num} å‰ç§» {adjustment:.3f}s")

            adjustments[current_num] = adjustment

        # é‡æ–°æ„å»ºSRTæ¡ç›®
        for i, subtitle in enumerate(subtitles):
            current_num = subtitle['number']
            current_start = subtitle['start']
            current_end = subtitle['end']
            adjustment = adjustments[current_num]

            # åº”ç”¨ä¼˜åŒ–
            if adjustment > 0.001:
                new_start = max(0.001, current_start - adjustment)

                # ã€ä¿®å¤æ ¸å¿ƒã€‘ï¼šç›´æ¥ä½¿ç”¨åˆ—è¡¨ä¸­çš„ä¸Šä¸€ä¸ªå…ƒç´ ï¼Œè€Œä¸æ˜¯é€šè¿‡åºå·æŸ¥æ‰¾
                prev_end = 0.0
                if i > 0:
                    # è·å–åˆ—è¡¨ä¸­çš„å‰ä¸€ä¸ªæ¡ç›®
                    prev_subtitle = subtitles[i - 1]
                    # è®¡ç®—å‰ä¸€ä¸ªæ¡ç›®ç»è¿‡è°ƒæ•´åçš„ç»“æŸæ—¶é—´
                    prev_adjustment = adjustments.get(prev_subtitle['number'], 0.0)
                    if prev_adjustment > 0.001:
                        # å¦‚æœå‰ä¸€ä¸ªæ¡ç›®ä¹Ÿè¢«è°ƒæ•´äº†ï¼Œéœ€è¦è®¡ç®—å…¶æ–°çš„å¼€å§‹æ—¶é—´
                        prev_new_start = max(0.001, prev_subtitle['start'] - prev_adjustment)
                        # ç¡®ä¿å‰ä¸€ä¸ªæ¡ç›®çš„æ—¶é—´é€»è¾‘æ­£ç¡®
                        if prev_new_start <= prev_end:
                            prev_new_start = prev_end + 0.001
                        # å‰ä¸€ä¸ªæ¡ç›®çš„ç»“æŸæ—¶é—´ä¿æŒä¸å˜ï¼Œæ‰€ä»¥ä½¿ç”¨åŸå§‹ç»“æŸæ—¶é—´
                        prev_end = prev_subtitle['end']
                    else:
                        # å‰ä¸€ä¸ªæ¡ç›®æ²¡æœ‰è¢«è°ƒæ•´ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹ç»“æŸæ—¶é—´
                        prev_end = prev_subtitle['end']

                # å®‰å…¨æ£€æŸ¥ï¼šé˜²æ­¢ä¸ä¸Šä¸€ä¸ªå­—å¹•é‡å 
                if new_start <= prev_end:
                    new_start = prev_end + 0.001

                # é‡æ–°æ ¼å¼åŒ–æ—¶é—´æˆ³
                new_start_str = self._format_timecode(new_start)
                new_end_str = self._format_timecode(current_end)
                time_line = f"{new_start_str} --> {new_end_str}"
            else:
                # ä¸è°ƒæ•´ï¼Œä½¿ç”¨åŸå§‹æ—¶é—´æˆ³
                original_lines = subtitle['entry_str'].strip().split('\n')
                time_line = original_lines[1]  # åŸå§‹æ—¶é—´æˆ³è¡Œ

            # æ„å»ºæ–°æ¡ç›®
            entry_lines = [
                str(current_num),
                time_line
            ]
            entry_lines.extend(subtitle['content'])
            entry_lines.append("")  # ç©ºè¡Œ

            optimized_entries.append('\n'.join(entry_lines))

        # è¿”å›ä¸åŸå§‹æ ¼å¼ç›¸åŒçš„å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆæ¯ä¸ªæ¡ç›®åŒ…å«å®Œæ•´å†…å®¹+ç©ºè¡Œï¼‰
        result_list = []
        for entry in optimized_entries:
            result_list.append(entry + '\n')  # ç¡®ä¿æ¯ä¸ªæ¡ç›®ä»¥æ¢è¡Œç»“æŸ

        return result_list

    def _parse_srt_time(self, time_str: str) -> float:
        """å°†SRTæ—¶é—´æ ¼å¼è½¬æ¢ä¸ºç§’æ•°"""
        # æ ¼å¼: 00:03:47,330
        try:
            parts = time_str.split(':')
            hours = int(parts[0])
            minutes = int(parts[1])
            seconds_parts = parts[2].split(',')
            seconds = int(seconds_parts[0])
            milliseconds = int(seconds_parts[1])

            total_seconds = hours * 3600 + minutes * 60 + seconds + milliseconds / 1000.0
            return total_seconds
        except:
            return 0.0

    def _format_timecode(self, seconds: float) -> str:
        """å°†ç§’æ•°è½¬æ¢ä¸ºSRTæ—¶é—´æ ¼å¼"""
        try:
            hours = int(seconds // 3600)
            remaining = seconds % 3600
            minutes = int(remaining // 60)
            secs = int(remaining % 60)
            milliseconds = int((remaining % 1) * 1000)

            return f"{hours:02d}:{minutes:02d}:{secs:02d},{milliseconds:03d}"
        except:
            return "00:00:00,000"

    def _apply_word_level_spacing_validation(self, entries: List[SubtitleEntry]) -> List[SubtitleEntry]:
        """
        Sonioxæ¨¡å¼ä¸“ç”¨è¯çº§é—´è·éªŒè¯ï¼šåœ¨ç»ˆæä¼˜åŒ–å®Œæˆåæ£€æŸ¥å’Œä¿®æ­£å­—å¹•é—´è·

        æ ¹æ®ç”¨æˆ·è¦æ±‚ï¼šå¯¹äºsonioxæ¨¡å¼ï¼Œç›´æ¥åœ¨Sonioxç»ˆæä¼˜åŒ–å®Œæˆä¹‹åï¼Œ
        æŸ¥çœ‹æ˜¯å¦æœ‰ä¸¤ä¸ªå­—å¹•çš„é—´è·å°äºç”¨æˆ·è®¾å®šçš„æœ€å°é—´è·çš„æƒ…å†µï¼Œ
        ä½¿ç”¨0.35sé˜ˆå€¼å’Œè¯çº§æ—¶é—´æˆ³è¿›è¡Œç²¾ç»†åŒ–è°ƒæ•´

        è°ƒæ•´é€»è¾‘ï¼š
        1. åˆ¤æ–­å‰ä¸€ä¸ªå­—å¹•çš„å€’æ•°ç¬¬äºŒä¸ªè¯çš„ç»“å°¾æ—¶é—´å’Œæœ€åä¸€ä¸ªè¯çš„å¼€å§‹æ—¶é—´çš„è·ç¦»æ˜¯å¦è¶…è¿‡0.35
        2. è‹¥è¶…è¿‡äº†ï¼Œåˆ™å°†ç¬¬ä¸€ä¸ªå­—å¹•çš„ç»“æŸæ—¶é—´è®¾ç½®ä¸ºç¬¬ä¸€ä¸ªå­—å¹•çš„å€’æ•°ç¬¬äºŒä¸ªè¯çš„ç»“å°¾æ—¶é—´åŠ 0.35
        3. ç„¶åå°†ç¬¬äºŒä¸ªå­—å¹•çš„å¼€å§‹æ—¶é—´è°ƒæ•´ä¸ºç¬¬ä¸€ä¸ªå­—å¹•æ–°çš„ç»“å°¾æ—¶é—´å†åŠ ä¸Šç”¨æˆ·è®¾å®šçš„æœ€å°é—´è·
        4. å¦‚æœæ²¡æœ‰è¶…è¿‡ï¼Œå°±è¿›è¡Œåå‘åˆ¤æ–­

        Args:
            entries: å·²å®Œæˆç»ˆæä¼˜åŒ–çš„SubtitleEntryåˆ—è¡¨

        Returns:
            ç»è¿‡è¯çº§é—´è·éªŒè¯å’Œä¿®æ­£çš„SubtitleEntryåˆ—è¡¨
        """
        if len(entries) < 2:
            self.log("   ğŸ¯ è¯çº§é—´è·éªŒè¯ï¼šæ ·æœ¬æ•°é‡ä¸è¶³ï¼Œè·³è¿‡æ£€æŸ¥")
            return entries

        self.log(f"   ğŸ¯ è¯çº§é—´è·éªŒè¯ï¼šå¼€å§‹æ£€æŸ¥æœ€å°é—´è· (ç”¨æˆ·è®¾å®š: {self.default_gap_ms}ms)")
        self.log(f"   ğŸ” è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥{len(entries)}ä¸ªå­—å¹•æ¡ç›®çš„è¯çº§é—´è·")

        min_spacing_seconds = self.default_gap_ms / 1000.0
        max_word_gap = 0.35  # 0.35sé˜ˆå€¼
        adjustments_made = 0

        for i in range(len(entries) - 1):
            current_entry = entries[i]
            next_entry = entries[i + 1]

            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„è¯æ±‡è¿›è¡Œåˆ†æ
            if len(current_entry.words_used) < 2 or len(next_entry.words_used) < 2:
                self.log(f"   ğŸ” å­—å¹•{current_entry.index}->{next_entry.index}: è¯æ±‡æ•°é‡ä¸è¶³ï¼Œè·³è¿‡è¯çº§åˆ†æ")
                # å›é€€åˆ°ç®€å•é—´è·æ£€æŸ¥
                simple_gap = next_entry.start_time - current_entry.end_time
                if simple_gap < min_spacing_seconds:
                    self.log(f"   ğŸ” æ£€æµ‹åˆ°ç®€å•é—´è·è¿‡å°ï¼šå­—å¹•{current_entry.index} -> å­—å¹•{next_entry.index} "
                            f"(å½“å‰é—´è·: {simple_gap:.3f}s, è¦æ±‚æœ€å°é—´è·: {min_spacing_seconds:.3f}s)")
                    next_entry.start_time = current_entry.end_time + min_spacing_seconds
                    adjustments_made += 1
                continue

            current_gap = next_entry.start_time - current_entry.end_time

            if current_gap < min_spacing_seconds:
                self.log(f"   ğŸ” æ£€æµ‹åˆ°é—´è·è¿‡å°ï¼šå­—å¹•{current_entry.index} -> å­—å¹•{next_entry.index} "
                        f"(å½“å‰é—´è·: {current_gap:.3f}s, è¦æ±‚æœ€å°é—´è·: {min_spacing_seconds:.3f}s)")

                # åº”ç”¨ç”¨æˆ·æŒ‡å®šçš„è¯çº§æ—¶é—´æˆ³è°ƒæ•´é€»è¾‘
                adjustment_made = self._apply_user_word_spacing_logic(current_entry, next_entry, min_spacing_seconds, max_word_gap)
                if adjustment_made:
                    adjustments_made += 1

        if adjustments_made == 0:
            self.log("   ğŸ¯ è¯çº§é—´è·éªŒè¯ï¼šæœªå‘ç°éœ€è¦è°ƒæ•´çš„é—´è·é—®é¢˜")
        else:
            self.log(f"   ğŸ¯ è¯çº§é—´è·éªŒè¯ï¼šå®Œæˆï¼Œå…±è°ƒæ•´äº† {adjustments_made} ä¸ªå­—å¹•çš„æ—¶åº")

        return entries

    def _apply_user_word_spacing_logic(self, current_entry: SubtitleEntry, next_entry: SubtitleEntry,
                                    min_spacing_seconds: float, max_word_gap: float) -> bool:
        """
        åº”ç”¨ç”¨æˆ·çš„è¯çº§æ—¶é—´æˆ³é—´è·è°ƒæ•´é€»è¾‘

        æ ¹æ®ç”¨æˆ·çš„è¯¦ç»†è¦æ±‚è¿›è¡Œè¯çº§åˆ†æï¼š

        Args:
            current_entry: å½“å‰å­—å¹•æ¡ç›®
            next_entry: ä¸‹ä¸€ä¸ªå­—å¹•æ¡ç›®
            min_spacing_seconds: ç”¨æˆ·è®¾å®šçš„æœ€å°é—´è·
            max_word_gap: æœ€å¤§è¯é—´é—´è·é˜ˆå€¼(0.35s)

        Returns:
            æ˜¯å¦è¿›è¡Œäº†è°ƒæ•´
        """
        current_words = current_entry.words_used
        next_words = next_entry.words_used

        # æ£€æŸ¥å‰ä¸€ä¸ªå­—å¹•çš„å€’æ•°ç¬¬äºŒä¸ªè¯çš„ç»“å°¾æ—¶é—´å’Œæœ€åä¸€ä¸ªè¯çš„å¼€å§‹æ—¶é—´çš„è·ç¦»
        if len(current_words) >= 2:
            # è·å–å½“å‰å­—å¹•å€’æ•°ç¬¬äºŒä¸ªè¯çš„ç»“å°¾æ—¶é—´å’Œæœ€åä¸€ä¸ªè¯çš„å¼€å§‹æ—¶é—´
            second_last_word = current_words[-2]
            last_word_current = current_words[-1]

            # æ£€æŸ¥å½“å‰å­—å¹•å†…éƒ¨çš„è¯é—´è·ç¦»
            current_word_gap = last_word_current.start_time - second_last_word.end_time

            self.log(f"   ğŸ” è¯çº§åˆ†æï¼šå½“å‰å­—å¹•{current_entry.index}è¯é—´è·ç¦» = {current_word_gap:.3f}s")

            if current_word_gap > max_word_gap:
                # è¶…è¿‡0.35sï¼Œåº”ç”¨ç¬¬ä¸€ç§è°ƒæ•´ç­–ç•¥
                # å°†ç¬¬ä¸€ä¸ªå­—å¹•çš„ç»“æŸæ—¶é—´è®¾ç½®ä¸ºå€’æ•°ç¬¬äºŒä¸ªè¯çš„ç»“å°¾æ—¶é—´åŠ 0.35
                new_current_end_time = second_last_word.end_time + max_word_gap
                original_current_end = current_entry.end_time
                current_entry.end_time = new_current_end_time

                # å°†ç¬¬äºŒä¸ªå­—å¹•çš„å¼€å§‹æ—¶é—´è°ƒæ•´ä¸ºç¬¬ä¸€ä¸ªå­—å¹•æ–°çš„ç»“å°¾æ—¶é—´å†åŠ ä¸Šç”¨æˆ·è®¾å®šçš„æœ€å°é—´è·
                new_next_start_time = new_current_end_time + min_spacing_seconds
                original_next_start = next_entry.start_time
                next_entry.start_time = new_next_start_time

                self.log(f"   âœ… è¯çº§è°ƒæ•´ç­–ç•¥1ï¼šå­—å¹•{current_entry.index}ç»“æŸæ—¶é—´ {original_current_end:.3f}s -> {new_current_end_time:.3f}s")
                self.log(f"   âœ… è¯çº§è°ƒæ•´ç­–ç•¥1ï¼šå­—å¹•{next_entry.index}å¼€å§‹æ—¶é—´ {original_next_start:.3f}s -> {new_next_start_time:.3f}s")
                self.log(f"   ğŸ“ è¯çº§è°ƒæ•´åŸå› ï¼šå½“å‰å­—å¹•å†…è¯é—´è·ç¦»({current_word_gap:.3f}s) > {max_word_gap:.3f}s")

                return True

        # å¦‚æœç¬¬ä¸€ç§ç­–ç•¥ä¸é€‚ç”¨ï¼Œæ£€æŸ¥ä¸‹ä¸€ä¸ªå­—å¹•çš„ç¬¬äºŒä¸ªè¯çš„å¼€å§‹æ—¶é—´å’Œç¬¬ä¸€ä¸ªè¯çš„ç»“æŸæ—¶é—´çš„è·ç¦»
        if len(next_words) >= 2:
            # è·å–ä¸‹ä¸€ä¸ªå­—å¹•çš„ç¬¬ä¸€ä¸ªè¯å’Œç¬¬äºŒä¸ªè¯
            first_word_next = next_words[0]
            second_word_next = next_words[1]

            # æ£€æŸ¥ä¸‹ä¸€ä¸ªå­—å¹•å†…éƒ¨çš„è¯é—´è·ç¦»
            next_word_gap = second_word_next.start_time - first_word_next.end_time

            self.log(f"   ğŸ” è¯çº§åˆ†æï¼šä¸‹ä¸€ä¸ªå­—å¹•{next_entry.index}è¯é—´è·ç¦» = {next_word_gap:.3f}s")

            if next_word_gap > max_word_gap:
                # è¶…è¿‡0.35sï¼Œåº”ç”¨ç¬¬äºŒç§è°ƒæ•´ç­–ç•¥
                # å°†ç¬¬äºŒä¸ªå­—å¹•çš„å¼€å§‹æ—¶é—´è®¾ç½®ä¸ºç¬¬äºŒä¸ªå­—å¹•çš„ç¬¬äºŒä¸ªè¯çš„å¼€å§‹æ—¶é—´å‡å»0.35
                new_next_start_time = second_word_next.start_time - max_word_gap
                original_next_start = next_entry.start_time

                # ç¡®ä¿æ–°çš„å¼€å§‹æ—¶é—´ä¸æ—©äºå½“å‰å­—å¹•ç»“æŸæ—¶é—´
                if new_next_start_time < current_entry.end_time:
                    new_next_start_time = current_entry.end_time + min_spacing_seconds
                    self.log(f"   âš ï¸ è¯çº§è°ƒæ•´é™åˆ¶ï¼šæ–°çš„å¼€å§‹æ—¶é—´æ—©äºå½“å‰å­—å¹•ç»“æŸæ—¶é—´ï¼Œè°ƒæ•´ä¸º {new_next_start_time:.3f}s")

                # å°†ç¬¬ä¸€ä¸ªå­—å¹•çš„ç»“æŸæ—¶é—´è°ƒæ•´ä¸ºç¬¬äºŒä¸ªå­—å¹•æ–°çš„å¼€å§‹æ—¶é—´å†å‡ç”¨æˆ·è®¾å®šçš„æœ€å°é—´è·
                new_current_end_time = new_next_start_time - min_spacing_seconds
                original_current_end = current_entry.end_time

                # ç¡®ä¿é€»è¾‘æ­£ç¡®
                if new_current_end_time < original_current_end:
                    new_current_end_time = original_current_end  # ä¸ç¼©çŸ­å½“å‰å­—å¹•

                current_entry.end_time = new_current_end_time
                next_entry.start_time = new_next_start_time

                self.log(f"   âœ… è¯çº§è°ƒæ•´ç­–ç•¥2ï¼šå­—å¹•{next_entry.index}å¼€å§‹æ—¶é—´ {original_next_start:.3f}s -> {new_next_start_time:.3f}s")
                self.log(f"   âœ… è¯çº§è°ƒæ•´ç­–ç•¥2ï¼šå­—å¹•{current_entry.index}ç»“æŸæ—¶é—´ {original_current_end:.3f}s -> {new_current_end_time:.3f}s")
                self.log(f"   ğŸ“ è¯çº§è°ƒæ•´åŸå› ï¼šä¸‹ä¸€ä¸ªå­—å¹•å†…è¯é—´è·ç¦»({next_word_gap:.3f}s) > {max_word_gap:.3f}s")

                return True

        # å¦‚æœéƒ½ä¸é€‚ç”¨ï¼Œè¿›è¡Œç®€å•é—´è·è°ƒæ•´
        simple_gap = next_entry.start_time - current_entry.end_time
        if simple_gap < min_spacing_seconds:
            next_entry.start_time = current_entry.end_time + min_spacing_seconds
            self.log(f"   âœ… ç®€å•é—´è·è°ƒæ•´ï¼šå­—å¹•{next_entry.index}å¼€å§‹æ—¶é—´è°ƒæ•´åˆ°ä¿è¯{min_spacing_seconds:.3f}sé—´è·")
            return True

        return False

    def _parse_srt_entries_from_strings(self, srt_strings: List[str]) -> List[Dict]:
        """
        ä»SRTå­—ç¬¦ä¸²åˆ—è¡¨è§£æå­—å¹•æ¡ç›®

        Args:
            srt_strings: SRTæ ¼å¼çš„å­—ç¬¦ä¸²åˆ—è¡¨

        Returns:
            è§£æåçš„å­—å¹•æ¡ç›®å­—å…¸åˆ—è¡¨
        """
        parsed_subtitles = []

        for i, srt_entry in enumerate(srt_strings):
            # åˆ†å‰²å®Œæ•´æ¡ç›®çš„è¡Œ
            entry_lines = srt_entry.strip().split('\n')
            if len(entry_lines) < 2:  # è‡³å°‘éœ€è¦åºå·å’Œæ—¶é—´æˆ³è¡Œ
                continue

            # è§£æåºå·
            try:
                subtitle_number = int(entry_lines[0].strip())
            except:
                continue

            # è§£ææ—¶é—´æˆ³
            time_line = entry_lines[1]
            if '-->' not in time_line:
                continue

            try:
                time_parts = time_line.split(' --> ')
                start_time = self._parse_srt_time(time_parts[0])
                end_time = self._parse_srt_time(time_parts[1])

                # è·å–å†…å®¹æ–‡æœ¬
                content_lines = entry_lines[2:] if len(entry_lines) > 2 else []

                parsed_subtitles.append({
                    'number': subtitle_number,
                    'start': start_time,
                    'end': end_time,
                    'content': content_lines,
                    'entry_str': srt_entry  # ä¿å­˜åŸå§‹å­—ç¬¦ä¸²ç”¨äºé‡æ„
                })

            except Exception as e:
                self.log(f"   âš ï¸ å­—å¹•{subtitle_number}æ—¶é—´æˆ³è§£æå¤±è´¥: {str(e)}")
                continue

        return parsed_subtitles

    def _build_srt_strings_from_parsed_entries(self, parsed_entries: List[Dict]) -> List[str]:
        """
        ä»è§£æçš„å­—å¹•æ¡ç›®é‡æ–°æ„å»ºSRTå­—ç¬¦ä¸²åˆ—è¡¨

        Args:
            parsed_entries: è§£æçš„å­—å¹•æ¡ç›®å­—å…¸åˆ—è¡¨

        Returns:
            SRTæ ¼å¼çš„å­—ç¬¦ä¸²åˆ—è¡¨
        """
        result_srt_lines = []
        for subtitle in parsed_entries:
            if all(k in subtitle for k in ['number', 'start', 'end', 'content']):
                # é‡æ–°æ ¼å¼åŒ–æ—¶é—´æˆ³
                start_time_str = self._format_timecode(subtitle['start'])
                end_time_str = self._format_timecode(subtitle['end'])
                time_line = f"{start_time_str} --> {end_time_str}"

                # æ„å»ºæ¡ç›®
                entry_lines = [
                    str(subtitle['number']),
                    time_line
                ]
                entry_lines.extend(subtitle['content'])
                entry_lines.append("")  # ç©ºè¡Œ

                result_srt_lines.append('\n'.join(entry_lines) + '\n')

        return result_srt_lines

    def _reconstruct_subtitle_entry_from_srt_string(self, srt_string: str) -> Optional[SubtitleEntry]:
        """
        ä»SRTæ ¼å¼å­—ç¬¦ä¸²é‡æ„SubtitleEntryå¯¹è±¡å¹¶å…³è”è¯çº§æ•°æ®

        Args:
            srt_string: SRTæ ¼å¼çš„å­—ç¬¦ä¸²ï¼ˆåŒ…å«åºå·ã€æ—¶é—´æˆ³ã€æ–‡æœ¬ï¼‰

        Returns:
            é‡æ„çš„SubtitleEntryå¯¹è±¡ï¼ŒåŒ…å«è¯çº§æ•°æ®ï¼›å¤±è´¥è¿”å›None
        """
        try:
            lines = srt_string.strip().split('\n')
            if len(lines) < 3:  # è‡³å°‘éœ€è¦åºå·ã€æ—¶é—´æˆ³ã€æ–‡æœ¬
                return None

            # è§£æåºå·
            subtitle_number = int(lines[0].strip())

            # è§£ææ—¶é—´æˆ³
            time_line = lines[1]
            if '-->' not in time_line:
                return None

            time_parts = time_line.split(' --> ')
            start_time = self._parse_srt_time(time_parts[0])
            end_time = self._parse_srt_time(time_parts[1])

            # æå–æ–‡æœ¬å†…å®¹
            text_content = ''.join(lines[2:]) if len(lines) > 2 else ""

            # æŸ¥æ‰¾å¯¹åº”çš„è¯çº§æ•°æ®
            # éœ€è¦åœ¨å¤„ç†è¿‡ç¨‹ä¸­ä¿å­˜çš„è¯çº§æ•°æ®æ˜ å°„è¡¨
            word_data_for_entry = self._find_word_data_for_time_range(start_time, end_time)

            return SubtitleEntry(
                index=subtitle_number,
                start_time=start_time,
                end_time=end_time,
                text=text_content,
                words_used=word_data_for_entry
            )

        except Exception as e:
            self.log(f"   âš ï¸ é‡æ„å­—å¹•æ¡ç›®å¤±è´¥: {str(e)}")
            return None

    def _find_word_data_for_time_range(self, start_time: float, end_time: float) -> List[TimestampedWord]:
        """
        æ ¹æ®æ—¶é—´èŒƒå›´æŸ¥æ‰¾å¯¹åº”çš„è¯çº§æ•°æ®

        æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªç®€åŒ–å®ç°ï¼Œå®é™…ä¸­éœ€è¦åœ¨æ•´ä¸ªå¤„ç†è¿‡ç¨‹ä¸­ä¿å­˜è¯çº§æ˜ å°„

        Args:
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´

        Returns:
            è¯¥æ—¶é—´èŒƒå›´å†…çš„è¯æ±‡åˆ—è¡¨
        """
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–å®ç°ï¼Œå®é™…åº”è¯¥ä»å…¨å±€è¯çº§æ•°æ®ä¸­æŸ¥æ‰¾
        # æš‚æ—¶è¿”å›ç©ºåˆ—è¡¨ï¼Œè®©é—´è·éªŒè¯å›é€€åˆ°ç®€å•æ¨¡å¼
        return []

    def _prepare_correction_prompt(self, segments: List[str], words: List[TimestampedWord]) -> List[str]:
        """
        [åºŸå¼ƒæ–¹æ³•] å‡†å¤‡AIçº é”™çš„æç¤ºè¯ï¼Œä½¿ç”¨æ»‘åŠ¨çª—å£æä¾›ä¸Šä¸‹æ–‡

        æ³¨æ„ï¼šæ­¤æ–¹æ³•å·²è¢« _build_smart_correction_prompt æ›¿ä»£ï¼Œä¿ç•™ä»…ä¸ºå‘åå…¼å®¹
        æ–°çš„æ™ºèƒ½çº é”™ä½¿ç”¨æ›´å®Œå–„çš„ä¸Šä¸‹æ–‡æå–å’Œæ‰¹æ¬¡å¤„ç†é€»è¾‘

        Args:
            segments: éœ€è¦çº é”™çš„æ–‡æœ¬ç‰‡æ®µåˆ—è¡¨
            words: Sonioxè¿”å›çš„è¯æ±‡åˆ—è¡¨ï¼ˆåŒ…å«ç½®ä¿¡åº¦ä¿¡æ¯ï¼‰

        Returns:
            çº é”™æç¤ºè¯åˆ—è¡¨ï¼Œæ¯ä¸ªæç¤ºè¯å¯¹åº”ä¸€ä¸ªbatch
        """
        # æ ‡è®°ä½ç½®ä¿¡åº¦è¯æ±‡
        marked_words = self._mark_low_confidence_words(words)

        # æ„å»ºä¸Šä¸‹æ–‡æ–‡æœ¬
        context_text = "".join([w.text for w in marked_words])

        # å‡†å¤‡çº é”™æç¤ºè¯
        prompts = []
        batch_size = 5  # æ¯ä¸ªbatchå¤„ç†5ä¸ªç‰‡æ®µï¼Œæ§åˆ¶tokenæ¶ˆè€—

        for i in range(0, len(segments), batch_size):
            batch_segments = segments[i:i + batch_size]

            # æ„å»ºä¸Šä¸‹æ–‡çª—å£ï¼ˆå½“å‰æ‰¹æ¬¡å‰åå„ä¸€ä¸ªç‰‡æ®µï¼‰
            context_start = max(0, i - 1)
            context_end = min(len(segments), i + batch_size + 1)
            context_segments = segments[context_start:context_end]

            # æ‰¾åˆ°å¯¹åº”çš„æ—¶é—´çª—å£è¯æ±‡
            segment_start_time = None
            segment_end_time = None

            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥æ ¹æ®æ—¶é—´æˆ³æ‰¾åˆ°å¯¹åº”è¯æ±‡
            # ä¸ºäº†å®ç°ç®€å•ï¼Œæˆ‘ä»¬ä½¿ç”¨æ‰€æœ‰è¯æ±‡ä½œä¸ºä¸Šä¸‹æ–‡
            relevant_words = marked_words

            # æ„å»ºå¸¦æ ‡è®°çš„æ–‡æœ¬
            marked_text = "".join([w.text for w in relevant_words])

            # æ„å»ºå®Œæ•´çš„çº é”™æç¤º
            prompt = f"""{app_config.DEEPSEEK_SYSTEM_PROMPT_CORRECTION}

ä»¥ä¸‹æ˜¯éœ€è¦çº é”™çš„æ–‡æœ¬ï¼ˆå·²ç”¨ã€ã€‘æ ‡è®°ä½ç½®ä¿¡åº¦è¯æ±‡ï¼‰ï¼š

{marked_text}

è¯·é‡ç‚¹å…³æ³¨ä»¥ä¸‹ç‰‡æ®µçš„çº é”™ï¼š
{chr(10).join([f"{i+j}. {seg}" for j, seg in enumerate(batch_segments)])}
"""

            prompts.append(prompt)

        return prompts

    def _identify_segments_requiring_correction(self, segments: List[str], words: List[TimestampedWord], srt_entries: List[Dict] = None) -> List[int]:
        """
        åŸºäºæ—¶é—´æˆ³ç²¾ç¡®è¯†åˆ«éœ€è¦çº é”™çš„ç‰‡æ®µ

        Args:
            segments: æ–‡æœ¬ç‰‡æ®µåˆ—è¡¨ï¼ˆLLMåˆ†å‰²åçš„ç‰‡æ®µï¼‰
            words: Sonioxè¿”å›çš„è¯æ±‡åˆ—è¡¨ï¼ˆåŒ…å«ç½®ä¿¡åº¦ä¿¡æ¯ï¼‰
            srt_entries: SRTæ¡ç›®åˆ—è¡¨ï¼ˆåŒ…å«æ—¶é—´ä¿¡æ¯ï¼‰

        Returns:
            éœ€è¦çº é”™çš„ç‰‡æ®µç´¢å¼•åˆ—è¡¨
        """
        # 1. æ”¶é›†ä½ç½®ä¿¡åº¦è¯ï¼ˆå¸¦æ—¶é—´æˆ³çš„å¯¹è±¡ï¼‰
        low_conf_word_objects = []
        all_punctuation = app_config.ALL_SPLIT_PUNCTUATION  # ä½¿ç”¨åˆå¹¶çš„æ ‡ç‚¹ç¬¦å·é›†åˆ

        for word in words:
            # è·³è¿‡åŒ…å«æ ‡ç‚¹ç¬¦å·çš„è¯æ±‡ï¼ˆåŒ…æ‹¬æ ‡ç‚¹ç¬¦å·æœ¬èº«å’Œä»¥æ ‡ç‚¹ç»“å°¾çš„è¯æ±‡ï¼‰
            if self.check_word_has_punctuation(word.text, all_punctuation):
                continue

            # è·³è¿‡å•ä¸ªå­—ç¬¦çš„è¯æ±‡ï¼ˆé™¤éæ˜¯æ±‰å­—ã€å¹³å‡åæˆ–ç‰‡å‡åï¼‰
            text = word.text.strip()
            if len(text) == 1:
                # æ£€æŸ¥æ˜¯å¦ä¸º CJK æ±‰å­— (å¸¸ç”¨ + æ‰©å±•A)
                is_cjk = ('\u4e00' <= text <= '\u9fff') or ('\u3400' <= text <= '\u4dbf')

                # æ£€æŸ¥æ˜¯å¦ä¸º å¹³å‡å (\u3040-\u309f) æˆ– ç‰‡å‡å (\u30a0-\u30ff)
                is_kana = ('\u3040' <= text <= '\u30ff')

                # å¦‚æœæ—¢ä¸æ˜¯æ±‰å­—ä¹Ÿä¸æ˜¯å‡åï¼ˆæ¯”å¦‚å•ä¸ªè‹±æ–‡å­—æ¯æˆ–æ•°å­—ï¼‰ï¼Œåˆ™è·³è¿‡
                if not (is_cjk or is_kana):
                    continue

            # é˜ˆå€¼æ£€æŸ¥
            if word.confidence < app_config.DEFAULT_SONIOX_LOW_CONFIDENCE_THRESHOLD:
                low_conf_word_objects.append(word)

        if not low_conf_word_objects:
            return []

        segments_to_correct = []

        # 2. å¦‚æœæœ‰æ—¶é—´ä¿¡æ¯ï¼Œä½¿ç”¨æ—¶é—´è½´åŒ¹é…ï¼ˆç²¾å‡†ï¼‰
        if srt_entries and segments:
            for i, entry in enumerate(srt_entries):
                # è¾¹ç•Œæ£€æŸ¥ï¼šç¡®ä¿ç´¢å¼•ä¸è¶…å‡ºèŒƒå›´
                if i >= len(segments):
                    break

                # è§£æ SRT æ—¶é—´å­—ç¬¦ä¸²ä¸ºç§’æ•°
                time_str = entry.get('time', '')
                if not time_str or '-->' not in time_str:
                    continue

                start_str, end_str = time_str.split(' --> ')
                try:
                    seg_start = self._parse_srt_time(start_str.strip())
                    seg_end = self._parse_srt_time(end_str.strip())
                except Exception:
                    # æ—¶é—´è§£æå¤±è´¥ï¼Œè·³è¿‡è¿™ä¸ªæ¡ç›®
                    continue

                # è¾¹ç•Œæ£€æŸ¥ï¼šç¡®ä¿æ—¶é—´èŒƒå›´æœ‰æ•ˆ
                if seg_start >= seg_end:
                    continue

                # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•ä½ç½®ä¿¡åº¦è¯è½åœ¨è¿™ä¸ªæ—¶é—´æ®µå†…
                has_error = False
                for bad_word in low_conf_word_objects:
                    # è¾¹ç•Œæ£€æŸ¥ï¼šç¡®ä¿wordå¯¹è±¡æœ‰æ—¶é—´å±æ€§
                    if not hasattr(bad_word, 'start_time') or not hasattr(bad_word, 'end_time'):
                        continue

                    # è¾¹ç•Œæ£€æŸ¥ï¼šç¡®ä¿æ—¶é—´èŒƒå›´æœ‰æ•ˆ
                    if bad_word.start_time >= bad_word.end_time:
                        continue

                    # è®¡ç®—è¯çš„ä¸­ç‚¹æ—¶é—´
                    word_mid = (bad_word.start_time + bad_word.end_time) / 2

                    # åˆ¤å®šæ¡ä»¶ï¼šè¯çš„ä¸­ç‚¹åœ¨ç‰‡æ®µèŒƒå›´å†…ï¼ˆæ”¾å®½ 0.1ç§’ å®¹å·®ï¼‰
                    if (seg_start - 0.1) <= word_mid <= (seg_end + 0.1):
                        has_error = True
                        break  # æ‰¾åˆ°ä¸€ä¸ªå°±è¶³å¤Ÿæ ‡è®°è¯¥æ®µ

                if has_error:
                    segments_to_correct.append(i)

        # 3. å¦‚æœæ²¡æœ‰æ—¶é—´ä¿¡æ¯ï¼ˆå…œåº•ï¼‰ï¼Œå›é€€åˆ°æ–‡æœ¬åŒ¹é…ï¼ˆä½†ä¸æ¨èï¼‰
        else:
            self.log("âš ï¸ è­¦å‘Šï¼šç¼ºå°‘æ—¶é—´ä¿¡æ¯ï¼Œå›é€€åˆ°æ¨¡ç³Šæ–‡æœ¬åŒ¹é…ï¼Œå¯èƒ½å¯¼è‡´è¿‡åº¦çº é”™")

            # å°†å¯¹è±¡è½¬æ¢ä¸ºæ–‡æœ¬åˆ—è¡¨è¿›è¡Œå›é€€åŒ¹é…
            low_confidence_texts = [word.text for word in low_conf_word_objects]

            for i, segment in enumerate(segments):
                # æ£€æŸ¥è¿™ä¸ªç‰‡æ®µä¸­æ˜¯å¦åŒ…å«ä»»ä½•ä½ç½®ä¿¡åº¦è¯æ±‡
                for low_conf_text in low_confidence_texts:
                    if low_conf_text in segment:
                        segments_to_correct.append(i)
                        break

        self.log(f"ğŸ“Š ç²¾ç¡®è¯†åˆ«ç»“æœ: {len(segments)} ä¸ªç‰‡æ®µä¸­ï¼Œ{len(segments_to_correct)} ä¸ªéœ€è¦çº é”™")
        return segments_to_correct

    def _prepare_smart_correction_batches(self, segments: List[str], words: List[TimestampedWord],
                                         target_segments: List[int]) -> List[List[int]]:
        """
        åˆ›å»ºæ™ºèƒ½çº é”™æ‰¹æ¬¡ï¼ŒåŒ…å«ç›®æ ‡ç‰‡æ®µå’Œä¸Šä¸‹æ–‡

        Args:
            segments: æ‰€æœ‰ç‰‡æ®µ
            words: è¯æ±‡åˆ—è¡¨
            target_segments: éœ€è¦çº é”™çš„ç‰‡æ®µç´¢å¼•

        Returns:
            çº é”™æ‰¹æ¬¡çš„ç‰‡æ®µç´¢å¼•åˆ—è¡¨ï¼Œæ¯ä¸ªæ‰¹æ¬¡åŒ…å«ç›®æ ‡ç‰‡æ®µ+ä¸Šä¸‹æ–‡
        """
        if not target_segments:
            return []

        BATCH_SIZE = 15  # æ¯ä¸ªæ‰¹æ¬¡æœ€å¤š15ä¸ªç›®æ ‡ç‰‡æ®µ
        batches = []

        # å°†ç›®æ ‡ç‰‡æ®µæŒ‰ç´¢å¼•æ’åºï¼Œç¡®ä¿æŒ‰é¡ºåºå¤„ç†
        sorted_targets = sorted(target_segments)

        # åˆ†æ‰¹å¤„ç†ç›®æ ‡ç‰‡æ®µï¼Œæ¯æ‰¹æœ€å¤šBATCH_SIZEä¸ª
        for i in range(0, len(sorted_targets), BATCH_SIZE):
            batch_target_indices = sorted_targets[i:i + BATCH_SIZE]
            # æ‰¹æ¬¡æ•°é‡å·²åœ¨å¤„ç†æ—¥å¿—ä¸­æ˜¾ç¤º

            # ä½¿ç”¨seté¿å…é‡å¤ï¼Œåˆ†ç¦»ç›®æ ‡ç‰‡æ®µå’Œä¸Šä¸‹æ–‡ç‰‡æ®µçš„å¤„ç†
            target_indices_set = set(batch_target_indices)
            context_indices_set = set()

            # æ”¶é›†ä¸Šä¸‹æ–‡ç´¢å¼•ï¼ˆæ’é™¤ç›®æ ‡ç´¢å¼•ä»¥é¿å…é‡å¤ï¼‰
            for target_idx in batch_target_indices:
                # æ·»åŠ å‰ä¸€ä¸ªç‰‡æ®µä½œä¸ºä¸Šä¸‹æ–‡
                if target_idx > 0:
                    prev_idx = target_idx - 1
                    # åªæœ‰å½“å‰ä¸€ä¸ªç‰‡æ®µä¸æ˜¯ç›®æ ‡ç‰‡æ®µæ—¶æ‰æ·»åŠ ä¸ºä¸Šä¸‹æ–‡
                    if prev_idx not in sorted_targets:
                        context_indices_set.add(prev_idx)

                # æ·»åŠ åä¸€ä¸ªç‰‡æ®µä½œä¸ºä¸Šä¸‹æ–‡
                if target_idx + 1 < len(segments):
                    next_idx = target_idx + 1
                    # åªæœ‰å½“åä¸€ä¸ªç‰‡æ®µä¸æ˜¯ç›®æ ‡ç‰‡æ®µæ—¶æ‰æ·»åŠ ä¸ºä¸Šä¸‹æ–‡
                    if next_idx not in sorted_targets:
                        context_indices_set.add(next_idx)

            # åˆå¹¶ç›®æ ‡ç´¢å¼•å’Œä¸Šä¸‹æ–‡ç´¢å¼•ï¼Œå¹¶æ’åº
            all_indices = target_indices_set | context_indices_set
            batch_indices = sorted(all_indices)

            # æ·»åŠ ä¸Šä¸‹æ–‡åæ£€æŸ¥æ‰¹æ¬¡å¤§å°ï¼Œå¦‚æœè¿‡å¤§åˆ™æˆªæ–­ä¸Šä¸‹æ–‡
            if len(batch_indices) > BATCH_SIZE + 10:  # å…è®¸ä¸€å®šçš„ä¸Šä¸‹æ–‡ç©ºé—´
                # ä¼˜å…ˆä¿ç•™ç›®æ ‡ç‰‡æ®µï¼Œå»æ‰ä¸€äº›ä¸Šä¸‹æ–‡
                core_indices = [idx for idx in batch_indices if idx in batch_target_indices]
                remaining_slots = BATCH_SIZE - len(core_indices)

                if remaining_slots > 0:
                    # æ·»åŠ å¿…è¦çš„ä¸Šä¸‹æ–‡
                    context_indices = [idx for idx in batch_indices if idx not in core_indices]
                    batch_indices = sorted(core_indices + context_indices[:remaining_slots])
                else:
                    batch_indices = core_indices

            batches.append(batch_indices)
            # æœ€ç»ˆæ‰¹æ¬¡ä¿¡æ¯åœ¨å¤„ç†æ—¶æ˜¾ç¤ºï¼Œè¿™é‡Œä¸å†é‡å¤

        return batches

    def _smart_context_extraction(self, full_text: str, batch_target_segments: List[str], max_length: int = 3000) -> str:
        """åŸºäºæ‰¹æ¬¡çš„æ™ºèƒ½ä¸Šä¸‹æ–‡æå–ï¼šåŠ¨æ€æå–æ‰¹æ¬¡ç›¸å…³ä¸Šä¸‹æ–‡"""

        if len(full_text) <= max_length:
            # å¦‚æœå…¨æ–‡ä¸è¶…è¿‡é™åˆ¶ï¼Œç›´æ¥è¿”å›å…¨æ–‡
            return full_text

  # ä½¿ç”¨configä¸­å®šä¹‰çš„å®Œæ•´æ ‡ç‚¹ç¬¦å·é›†åˆ
        sentence_endings = app_config.FINAL_PUNCTUATION | {'â€¦', 'â€¥'}  # æ·»åŠ å•å­—ç¬¦çœç•¥å·
        multi_char_endings = list(app_config.ELLIPSIS_PUNCTUATION)  # ä½¿ç”¨configä¸­çš„å®Œæ•´çœç•¥å·é›†åˆ

        # æ‰¾åˆ°æ‰€æœ‰å¥å­è¾¹ç•Œä½ç½®
        sentence_boundaries = [0]
        pos = 0
        while pos < len(full_text):
            # æ£€æŸ¥å•ä¸ªå­—ç¬¦æ ‡ç‚¹
            if full_text[pos] in sentence_endings:
                sentence_boundaries.append(pos + 1)
            # æ£€æŸ¥å¤šå­—ç¬¦æ ‡ç‚¹
            elif pos >= 2 and full_text[pos-2:pos+1] in multi_char_endings:
                sentence_boundaries.append(pos + 1)
            pos += 1

        # æ·»åŠ å…¨æ–‡ç»“æŸä½œä¸ºè¾¹ç•Œ
        if len(full_text) not in sentence_boundaries:
            sentence_boundaries.append(len(full_text))

        # æ‰¾åˆ°æ‰¹æ¬¡ä¸­ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªç›®æ ‡ç‰‡æ®µåœ¨å…¨æ–‡ä¸­çš„ä½ç½®
        batch_positions = []
        for segment in batch_target_segments:
            pos = full_text.find(segment)
            if pos != -1:
                batch_positions.append((pos, pos + len(segment)))

        if not batch_positions:
            # å¦‚æœæ‰¾ä¸åˆ°ä»»ä½•ç›®æ ‡ç‰‡æ®µï¼Œè¿”å›ä¸­é—´3000å­—ç¬¦
            center = len(full_text) // 2
            start = max(0, center - max_length // 2)
            end = min(len(full_text), start + max_length)
            return full_text[start:end]

        # æ’åºä½ç½®
        batch_positions.sort()
        target_start = batch_positions[0][0]  # æ‰¹æ¬¡ç¬¬ä¸€ä¸ªç‰‡æ®µçš„å¼€å§‹ä½ç½®
        target_end = batch_positions[-1][1]   # æ‰¹æ¬¡æœ€åä¸€ä¸ªç‰‡æ®µçš„ç»“æŸä½ç½®

        # è®¡ç®—æ‰¹æ¬¡è¦†ç›–çš„æ–‡æœ¬é•¿åº¦
        batch_distance = target_end - target_start

        if batch_distance >= max_length:
            # æƒ…å†µBï¼šæ‰¹æ¬¡æœ¬èº«è·ç¦»å°±è¶…è¿‡3000ï¼Œåªèƒ½æˆªæ–­åˆ°æœ€è¿‘çš„å¥å­è¾¹ç•Œ
            # å‘å‰æ‰¾æœ€è¿‘çš„å¥å­ç»“æŸæ ‡ç‚¹
            context_start = target_start
            for boundary in reversed(sentence_boundaries):
                if boundary < target_start:
                    context_start = boundary
                    break

            # å‘åæ‰¾æœ€è¿‘çš„å¥å­ç»“æŸæ ‡ç‚¹
            context_end = target_end
            for boundary in sentence_boundaries:
                if boundary > target_end:
                    context_end = boundary
                    break

            # å³ä½¿è¿™æ ·è¿˜æ˜¯å¯èƒ½è¶…è¿‡3000ï¼Œè¿›ä¸€æ­¥æˆªæ–­åˆ°3000å­—ç¬¦
            if context_end - context_start > max_length:
                center = (context_start + context_end) // 2
                context_start = max(0, center - max_length // 2)
                context_end = context_start + max_length

        else:
            # æƒ…å†µAï¼šæ‰¹æ¬¡è·ç¦»å°äº3000ï¼Œå°½å¯èƒ½æ‰©å±•åˆ°æ¥è¿‘3000å­—ç¬¦çš„æœ€è¿‘å¥å­è¾¹ç•Œ
            # ç›®æ ‡ï¼šåœ¨ä¸è¶…è¿‡3000å­—ç¬¦çš„å‰æä¸‹ï¼Œå°½å¯èƒ½åŒ…å«æ›´å¤šå®Œæ•´çš„å¥å­

            # æ‰¾åˆ°æ‰¹æ¬¡æ‰€åœ¨å¥å­çš„ç´¢å¼•èŒƒå›´
            batch_sentence_start = 0
            batch_sentence_end = len(sentence_boundaries) - 1

            for i in range(len(sentence_boundaries) - 1):
                if sentence_boundaries[i] <= target_start < sentence_boundaries[i+1]:
                    batch_sentence_start = i
                    break

            for i in range(len(sentence_boundaries) - 1):
                if sentence_boundaries[i] <= target_end <= sentence_boundaries[i+1]:
                    batch_sentence_end = i + 1
                    break

            # ä»æ‰¹æ¬¡æ‰€åœ¨å¥å­å¼€å§‹ï¼Œå‘ä¸¤ä¾§æ‰©å±•ç›´åˆ°æ¥è¿‘3000å­—ç¬¦
            left_idx = batch_sentence_start
            right_idx = batch_sentence_end

            while left_idx > 0 or right_idx < len(sentence_boundaries) - 1:
                current_length = sentence_boundaries[right_idx] - sentence_boundaries[left_idx]

                if current_length >= max_length:
                    break

                # ä¼˜å…ˆæ‰©å±•å¥å­è¾ƒå°‘çš„ä¸€ä¾§ï¼Œä¿æŒå¹³è¡¡
                can_expand_left = left_idx > 0
                can_expand_right = right_idx < len(sentence_boundaries) - 1

                if can_expand_left and can_expand_right:
                    # æ¯”è¾ƒä¸¤ä¾§å¯ä»¥æ‰©å±•çš„é•¿åº¦
                    left_expand_len = sentence_boundaries[left_idx] - sentence_boundaries[left_idx-1]
                    right_expand_len = sentence_boundaries[right_idx+1] - sentence_boundaries[right_idx]

                    # ä¼˜å…ˆæ‰©å±•è¾ƒçŸ­çš„ä¸€ä¾§
                    if left_expand_len <= right_expand_len:
                        left_idx -= 1
                    else:
                        right_idx += 1
                elif can_expand_left:
                    left_idx -= 1
                elif can_expand_right:
                    right_idx += 1
                else:
                    break

            # è·å–ç»“æœè¾¹ç•Œ
            context_start = sentence_boundaries[left_idx]
            context_end = sentence_boundaries[right_idx]

            # ç¡®ä¿ä¸è¶…å‡ºå…¨æ–‡è¾¹ç•Œ
            context_start = max(0, context_start)
            context_end = min(len(full_text), context_end)

            # å¦‚æœè¿˜æ˜¯è¶…è¿‡é™åˆ¶ï¼Œè¿›ä¸€æ­¥æˆªæ–­åˆ°3000å­—ç¬¦
            final_length = context_end - context_start
            if final_length > max_length:
                center = (context_start + context_end) // 2
                context_start = max(0, center - max_length // 2)
                context_end = context_start + max_length

        result = full_text[context_start:context_end]

        # æ·»åŠ æˆªæ–­æç¤º
        if context_start > 0 or context_end < len(full_text):
            result = "..." + result + "...\n\nï¼ˆæ³¨ï¼šä¸Šä¸‹æ–‡å› é•¿åº¦é™åˆ¶è¢«æ™ºèƒ½æˆªå–ï¼‰"

        return result

    def _build_smart_correction_prompt(self, batch_segments: List[str], low_confidence_words: List[str] = None,
                                   all_segments: List[str] = None, target_indices: List[int] = None,
                                   target_local_indices: List[int] = None) -> str:
        """
        ä¸ºæ™ºèƒ½çº é”™æ„å»ºä¸“ç”¨æç¤ºè¯ï¼ˆå®Œæ•´ä¸Šä¸‹æ–‡+ç²¾ç¡®å®šä½æ–¹æ¡ˆï¼‰

        Args:
            batch_segments: å½“å‰æ‰¹æ¬¡çš„ç‰‡æ®µåˆ—è¡¨
            low_confidence_words: ä½ç½®ä¿¡åº¦è¯æ±‡åˆ—è¡¨
            all_segments: å®Œæ•´çš„è½¬å½•æ–‡æœ¬ç‰‡æ®µåˆ—è¡¨ï¼ˆç”¨äºæä¾›ä¸Šä¸‹æ–‡ï¼‰
            target_indices: å½“å‰æ‰¹æ¬¡å¯¹åº”çš„æ‰€æœ‰ç‰‡æ®µä¸­çš„ç´¢å¼•
            target_local_indices: å½“å‰æ‰¹æ¬¡ä¸­éœ€è¦çº é”™çš„ç›®æ ‡ç‰‡æ®µçš„å±€éƒ¨ç´¢å¼•åˆ—è¡¨

        Returns:
            æ™ºèƒ½çº é”™æç¤ºè¯
        """
        # åœ¨ç‰‡æ®µä¸­æ ‡è®°ä½ç½®ä¿¡åº¦è¯æ±‡
        marked_segments = []
        for segment in batch_segments:
            marked_segment = segment
            if low_confidence_words:
                for low_conf_word in low_confidence_words:
                    if low_conf_word in marked_segment:
                        marked_segment = marked_segment.replace(low_conf_word, f"ã€{low_conf_word}ã€‘")
            marked_segments.append(marked_segment)

        # æ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡
        full_context = ""
        if all_segments:
            full_context = "".join(all_segments)
            # ä½¿ç”¨æ–°çš„æ‰¹æ¬¡æ™ºèƒ½ä¸Šä¸‹æ–‡æå–
            full_context = self._smart_context_extraction(full_context, batch_segments, 3000)

        # æ„å»ºæ™ºèƒ½æç¤ºè¯
        if full_context:
            # ç¡®ä¿target_local_indicesä¸ä¸ºNoneï¼Œé»˜è®¤ä¸ºæ‰€æœ‰ç´¢å¼•
            if target_local_indices is None:
                target_local_indices = [i for i in range(len(marked_segments))]

            # æ„å»ºå±€éƒ¨ç‰‡æ®µåˆ—è¡¨å­—ç¬¦ä¸²
            formatted_segments = chr(10).join([f"{i}. {seg}" for i, seg in enumerate(marked_segments)])

            # === å…³é”®ä¿®æ”¹ï¼šPrompt æ¨¡æ¿ ===
            prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹è¦æ±‚è¿›è¡ŒASRé”™è¯æ ¡å¯¹ï¼š

{app_config.DEEPSEEK_SYSTEM_PROMPT_CORRECTION}

## å®Œæ•´è½¬å½•ä¸Šä¸‹æ–‡
(ä»…ä¾›å‚è€ƒï¼Œç”¨äºç†è§£è¯­å¢ƒ)
{full_context}

## å½“å‰çº é”™ä»»åŠ¡
ä»¥ä¸‹ç‰‡æ®µåˆ—è¡¨åŒ…å«ã€ä¸»è¦ç›®æ ‡ã€‘å’Œã€å±€éƒ¨ä¸Šä¸‹æ–‡ã€‘ã€‚
è¯·ä»”ç»†é˜…è¯»å¹¶æ‰§è¡Œä»¥ä¸‹æ ¸å¿ƒæŒ‡ä»¤ï¼š

### æ ¸å¿ƒæŒ‡ä»¤ï¼š
1. **é’ˆå¯¹ã€ç›®æ ‡ç´¢å¼•ã€‘ç‰‡æ®µ ({target_local_indices})**ï¼š
   - **å¿…é¡»**ï¼šé‡ç‚¹æ ¡å¯¹ï¼Œä¿®æ­£æ‰€æœ‰æ ‡è®°çš„ã€ä½ç½®ä¿¡åº¦è¯æ±‡ã€‘åŠå…¶ä»–æ½œåœ¨é”™è¯¯ã€‚

2. **é’ˆå¯¹ã€éç›®æ ‡ï¼ˆä¸Šä¸‹æ–‡ï¼‰ç‰‡æ®µã€‘**ï¼š
   - **é»˜è®¤åŸåˆ™**ï¼š**ä¸è¦ä¿®æ”¹**ï¼Œç›´æ¥å¿½ç•¥æˆ–è¿”å›åŸæ–‡æœ¬ã€‚
   - **ä¾‹å¤–æ¡æ¬¾**ï¼šå¦‚æœä½ åœ¨ä¸Šä¸‹æ–‡ä¸­å‘ç°äº†**æå…¶æ˜æ˜¾çš„ASRé”™è¯¯**ï¼ˆä¾‹å¦‚ï¼šä¸¥é‡çš„åŒéŸ³å­—é”™è¯¯å¦‚"æ°—ç­’"->"äº€é ­"ã€ä¹±ç ã€æ˜æ˜¾ä¸åˆé€»è¾‘çš„è¯ï¼‰ï¼Œ**å…è®¸ä¸”å»ºè®®**ä½ å¯¹å…¶è¿›è¡Œä¿®æ­£ã€‚
   - **ä¸¥ç¦æ“ä½œ**ï¼šä¸¥ç¦ä»…ä¸ºäº†æ¶¦è‰²æ–‡ç¬”ã€æ”¹å˜è¯­æ°”æˆ–ç²¾ç®€å¥å­è€Œä¿®æ”¹ä¸Šä¸‹æ–‡ã€‚

### å¾…å¤„ç†ç‰‡æ®µåˆ—è¡¨ï¼š
{formatted_segments}

### è¾“å‡ºè¦æ±‚ï¼š
è¯·ä¸¥æ ¼æŒ‰ç…§ JSON æ ¼å¼è¿”å›ä¿®æ­£ç»“æœï¼š
{{"ç‰‡æ®µç´¢å¼•": "çº é”™åæ–‡æœ¬"}}

**æ³¨æ„**ï¼š
- å¦‚æœæŸç‰‡æ®µï¼ˆæ— è®ºæ˜¯ç›®æ ‡è¿˜æ˜¯ä¸Šä¸‹æ–‡ï¼‰**å®Œå…¨æ— éœ€ä¿®æ”¹**ï¼Œè¯·**ä¸è¦**åŒ…å«åœ¨è¿”å›çš„ JSON ä¸­ï¼Œä»¥èŠ‚çœèµ„æºã€‚
- ä»…è¿”å›æœ‰å®é™…å˜åŠ¨çš„ç‰‡æ®µã€‚"""
        else:
            # ç¡®ä¿target_local_indicesä¸ä¸ºNoneï¼Œé»˜è®¤ä¸ºæ‰€æœ‰ç´¢å¼•
            if target_local_indices is None:
                target_local_indices = [i for i in range(len(marked_segments))]

            # æ„å»ºå±€éƒ¨ç‰‡æ®µåˆ—è¡¨å­—ç¬¦ä¸²
            formatted_segments = chr(10).join([f"{i}. {seg}" for i, seg in enumerate(marked_segments)])

            # å›é€€åˆ°åŸå§‹æ–¹æ¡ˆ
            prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹è¦æ±‚è¿›è¡ŒASRé”™è¯æ ¡å¯¹ï¼š

{app_config.DEEPSEEK_SYSTEM_PROMPT_CORRECTION}

ä»¥ä¸‹ç‰‡æ®µåˆ—è¡¨åŒ…å«ã€ä¸»è¦ç›®æ ‡ã€‘å’Œã€å±€éƒ¨ä¸Šä¸‹æ–‡ã€‘ã€‚
è¯·ä»”ç»†é˜…è¯»å¹¶æ‰§è¡Œä»¥ä¸‹æ ¸å¿ƒæŒ‡ä»¤ï¼š

### æ ¸å¿ƒæŒ‡ä»¤ï¼š
1. **é’ˆå¯¹ã€ç›®æ ‡ç´¢å¼•ã€‘ç‰‡æ®µ ({target_local_indices})**ï¼š
   - **å¿…é¡»**ï¼šé‡ç‚¹æ ¡å¯¹ï¼Œä¿®æ­£æ‰€æœ‰æ ‡è®°çš„ã€ä½ç½®ä¿¡åº¦è¯æ±‡ã€‘åŠå…¶ä»–æ½œåœ¨é”™è¯¯ã€‚

2. **é’ˆå¯¹ã€éç›®æ ‡ï¼ˆä¸Šä¸‹æ–‡ï¼‰ç‰‡æ®µã€‘**ï¼š
   - **é»˜è®¤åŸåˆ™**ï¼š**ä¸è¦ä¿®æ”¹**ï¼Œç›´æ¥å¿½ç•¥æˆ–è¿”å›åŸæ–‡æœ¬ã€‚
   - **ä¾‹å¤–æ¡æ¬¾**ï¼šå¦‚æœä½ åœ¨ä¸Šä¸‹æ–‡ä¸­å‘ç°äº†**æå…¶æ˜æ˜¾çš„ASRé”™è¯¯**ï¼ˆä¾‹å¦‚ï¼šä¸¥é‡çš„åŒéŸ³å­—é”™è¯¯å¦‚"æ°—ç­’"->"äº€é ­"ã€ä¹±ç ã€æ˜æ˜¾ä¸åˆé€»è¾‘çš„è¯ï¼‰ï¼Œ**å…è®¸ä¸”å»ºè®®**ä½ å¯¹å…¶è¿›è¡Œä¿®æ­£ã€‚
   - **ä¸¥ç¦æ“ä½œ**ï¼šä¸¥ç¦ä»…ä¸ºäº†æ¶¦è‰²æ–‡ç¬”ã€æ”¹å˜è¯­æ°”æˆ–ç²¾ç®€å¥å­è€Œä¿®æ”¹ä¸Šä¸‹æ–‡ã€‚

### å¾…å¤„ç†ç‰‡æ®µåˆ—è¡¨ï¼š
{formatted_segments}

### è¾“å‡ºè¦æ±‚ï¼š
è¯·ä¸¥æ ¼æŒ‰ç…§ JSON æ ¼å¼è¿”å›ä¿®æ­£ç»“æœï¼š
{{"ç‰‡æ®µç´¢å¼•": "çº é”™åæ–‡æœ¬"}}

**æ³¨æ„**ï¼š
- å¦‚æœæŸç‰‡æ®µï¼ˆæ— è®ºæ˜¯ç›®æ ‡è¿˜æ˜¯ä¸Šä¸‹æ–‡ï¼‰**å®Œå…¨æ— éœ€ä¿®æ”¹**ï¼Œè¯·**ä¸è¦**åŒ…å«åœ¨è¿”å›çš„ JSON ä¸­ï¼Œä»¥èŠ‚çœèµ„æºã€‚
- ä»…è¿”å›æœ‰å®é™…å˜åŠ¨çš„ç‰‡æ®µã€‚"""

        return prompt

    def _apply_post_srt_ai_correction(self, srt_content: str, words: List[TimestampedWord]) -> tuple[str, List[str]]:
        """
        åœ¨SRTç”Ÿæˆå®Œæˆåè¿›è¡ŒAIæ ¡å¯¹ï¼ˆåå¤„ç†æ¨¡å¼ï¼‰

        Args:
            srt_content: å®Œæ•´çš„SRTå†…å®¹
            words: åŸå§‹è¯æ±‡åˆ—è¡¨ï¼ˆç”¨äºæ”¶é›†ä½ç½®ä¿¡åº¦è¯æ±‡ï¼‰

        Returns:
            tuple: (æ ¡å¯¹åçš„SRTå†…å®¹, æ ¡å¯¹æç¤ºåˆ—è¡¨)
        """
        if not srt_content.strip():
            return srt_content, []

        self.log("ğŸ¤– å¼€å§‹SRTåå¤„ç†AIæ ¡å¯¹")
        correction_hints: List[str] = []

        # ã€æ–°å¢ã€‘è·å–å½“å‰è¿›åº¦åç§»å’ŒèŒƒå›´ï¼Œç”¨äºAIçº é”™é˜¶æ®µçš„ç»†åˆ†è¿›åº¦
        current_offset = self._current_progress_offset
        total_range = self._current_progress_range

        try:
            # 1. è§£æSRTå†…å®¹ä¸ºæ¡ç›®åˆ—è¡¨
            srt_entries = self._parse_srt_content(srt_content)
            if not srt_entries:
                self.log("âš ï¸ SRTå†…å®¹è§£æä¸ºç©ºï¼Œè·³è¿‡AIæ ¡å¯¹")
                return srt_content, []

            self.log(f"   è§£æåˆ° {len(srt_entries)} ä¸ªSRTæ¡ç›®")

            # 2. æ”¶é›†ä½ç½®ä¿¡åº¦è¯æ±‡
            low_conf_words = self._collect_low_confidence_words(words)
            if not low_conf_words:
                self.log("âœ… æœªå‘ç°ä½ç½®ä¿¡åº¦è¯æ±‡ï¼Œè·³è¿‡AIæ ¡å¯¹")
                return srt_content, []

            # ç»Ÿè®¡ä¿¡æ¯åœ¨åç»­æ—¥å¿—ä¸­æ˜¾ç¤ºï¼Œè¿™é‡Œä¸å†é‡å¤

            # 3. æå–éœ€è¦æ ¡å¯¹çš„æ–‡æœ¬ç‰‡æ®µ
            text_segments = [entry['text'] for entry in srt_entries]

            # 4. æ ‡è®°ä½ç½®ä¿¡åº¦è¯æ±‡
            marked_segments = self._mark_low_confidence_words_in_segments(text_segments, low_conf_words)

            # 5. æ£€æŸ¥æ˜¯å¦éœ€è¦æ ¡å¯¹
            has_corrections = any('ã€' in seg for seg in marked_segments)
            if not has_corrections:
                self.log("âœ… æ²¡æœ‰éœ€è¦æ ¡å¯¹çš„å†…å®¹ï¼Œè·³è¿‡AIæ ¡å¯¹")
                return srt_content, []

            self.log(f"   æ£€æµ‹åˆ°éœ€è¦æ ¡å¯¹çš„ç‰‡æ®µæ•°é‡: {sum(1 for seg in marked_segments if 'ã€' in seg)}")

            # 6. æ‰§è¡ŒAIæ ¡å¯¹ï¼ˆä¼ é€’srt_entriesä»¥æ”¯æŒæ—¶é—´æˆ³åŒ¹é…ï¼‰
            corrected_segments, ai_correction_hints = self._perform_text_correction(marked_segments, words, srt_entries)
            correction_hints.extend(ai_correction_hints)

            # 7. é‡æ–°ç”ŸæˆSRTå†…å®¹
            corrected_srt_content = self._rebuild_srt_content(srt_entries, corrected_segments)

            # 8. æ¸…ç†AIå¯èƒ½é”™è¯¯æ·»åŠ çš„ã€ã€‘ç¬¦å·
            corrected_srt_content = self._clean_bracket_symbols(corrected_srt_content)

            # æ ¡å¯¹ç»Ÿè®¡å·²åœ¨å…¶ä»–åœ°æ–¹æ˜¾ç¤ºï¼Œé¿å…é‡å¤
            return corrected_srt_content, correction_hints

        except Exception as e:
            error_msg = f"âŒ SRTåå¤„ç†AIæ ¡å¯¹å¤±è´¥: {str(e)}"
            self.log(error_msg)
            correction_hints.append(error_msg)
            return srt_content, correction_hints

    def _parse_srt_content(self, srt_content: str) -> List[Dict]:
        """
        å°†SRTå†…å®¹è§£æä¸ºæ¡ç›®åˆ—è¡¨

        Args:
            srt_content: SRTæ ¼å¼çš„æ–‡æœ¬å†…å®¹

        Returns:
            List[Dict]: åŒ…å«ç´¢å¼•ã€æ—¶é—´æˆ³å’Œæ–‡æœ¬çš„æ¡ç›®åˆ—è¡¨
        """
        entries = []
        lines = srt_content.strip().split('\n')

        i = 0
        while i < len(lines):
            line = lines[i].strip()

            # è·³è¿‡ç©ºè¡Œ
            if not line:
                i += 1
                continue

            # è§£æåºå·
            try:
                index = int(line)
                i += 1

                # è§£ææ—¶é—´æˆ³
                if i >= len(lines):
                    break
                time_line = lines[i].strip()
                i += 1

                # è§£ææ–‡æœ¬ï¼ˆå¯èƒ½å¤šè¡Œï¼‰
                text_lines = []
                while i < len(lines) and lines[i].strip():
                    text_lines.append(lines[i].strip())
                    i += 1

                text = '\n'.join(text_lines)

                if text:  # ç¡®ä¿æœ‰æ–‡æœ¬å†…å®¹
                    entries.append({
                        'index': index,
                        'time': time_line,
                        'text': text
                    })
            except (ValueError, IndexError):
                # è§£æå¤±è´¥ï¼Œè·³è¿‡å½“å‰è¡Œ
                i += 1
                continue

        return entries

    def _collect_low_confidence_words(self, words: List[TimestampedWord]) -> List[TimestampedWord]:
        """
        æ”¶é›†ä½ç½®ä¿¡åº¦è¯æ±‡

        Args:
            words: è¯æ±‡åˆ—è¡¨

        Returns:
            List[TimestampedWord]: ä½ç½®ä¿¡åº¦è¯æ±‡åˆ—è¡¨
        """
        low_conf_words = []
        for word in words:
            if hasattr(word, 'confidence') and word.confidence is not None:
                if word.confidence < app_config.DEFAULT_SONIOX_LOW_CONFIDENCE_THRESHOLD:
                    low_conf_words.append(word)
        return low_conf_words

    def _mark_low_confidence_words_in_segments(self, segments: List[str], low_conf_words: List[TimestampedWord]) -> List[str]:
        """
        åœ¨æ–‡æœ¬ç‰‡æ®µä¸­æ ‡è®°ä½ç½®ä¿¡åº¦è¯æ±‡ï¼ˆåŸºäºæ—¶é—´æˆ³çš„ç²¾ç¡®æ ‡è®°ï¼‰

        Args:
            segments: æ–‡æœ¬ç‰‡æ®µåˆ—è¡¨
            low_conf_words: ä½ç½®ä¿¡åº¦è¯æ±‡åˆ—è¡¨

        Returns:
            List[str]: æ ‡è®°äº†ä½ç½®ä¿¡åº¦è¯æ±‡çš„æ–‡æœ¬ç‰‡æ®µåˆ—è¡¨
        """
        marked_segments = []

        for segment in segments:
            # é‡æ–°æ„å»ºæ–‡æœ¬ï¼Œåªåœ¨ä½ç½®ä¿¡åº¦è¯æ±‡çš„å…·ä½“ä½ç½®æ·»åŠ æ ‡è®°
            marked_text = self._rebuild_text_with_precise_marking(segment, low_conf_words)
            marked_segments.append(marked_text)

        return marked_segments

    def _rebuild_text_with_precise_marking(self, text: str, low_conf_words: List[TimestampedWord]) -> str:
        """
        åŸºäºæ—¶é—´æˆ³ç²¾ç¡®é‡å»ºå¸¦æ ‡è®°çš„æ–‡æœ¬

        Args:
            text: åŸå§‹æ–‡æœ¬ç‰‡æ®µ
            low_conf_words: ä½ç½®ä¿¡åº¦è¯æ±‡åˆ—è¡¨ï¼ˆå¸¦æ—¶é—´æˆ³ä¿¡æ¯ï¼‰

        Returns:
            str: åœ¨æ­£ç¡®ä½ç½®æ·»åŠ äº†ã€ã€‘æ ‡è®°çš„æ–‡æœ¬
        """
        if not low_conf_words:
            return text

        # å°†ä½ç½®ä¿¡åº¦è¯æ±‡æŒ‰å¼€å§‹æ—¶é—´æ’åºï¼Œç¡®ä¿æŒ‰é¡ºåºå¤„ç†
        sorted_low_conf_words = sorted(low_conf_words, key=lambda w: w.start_time)

        # é€å­—ç¬¦é‡å»ºæ–‡æœ¬
        result = []
        current_pos = 0
        text_len = len(text)

        # è®°å½•å“ªäº›ä½ç½®å·²ç»è¢«æ ‡è®°ï¼ˆé¿å…é‡å¤æ ‡è®°ï¼‰
        marked_ranges = []

        for low_conf_word in sorted_low_conf_words:
            word_text = low_conf_word.text.strip()

            # è·³è¿‡ä¸ç¬¦åˆæ ‡è®°æ¡ä»¶çš„è¯æ±‡
            if not word_text:
                continue
            if not any(c.isalnum() for c in word_text):
                continue
            punctuation_count = sum(1 for c in word_text if not c.isalnum())
            if punctuation_count > len(word_text) / 2:
                continue

            # åœ¨æ–‡æœ¬ä¸­æŸ¥æ‰¾è¿™ä¸ªè¯
            search_start = current_pos
            while search_start < text_len:
                found_pos = text.find(word_text, search_start)
                if found_pos == -1:
                    break

                # æ£€æŸ¥è¿™ä¸ªä½ç½®æ˜¯å¦å·²ç»è¢«æ ‡è®°
                is_already_marked = False
                for marked_start, marked_end in marked_ranges:
                    if found_pos >= marked_start and found_pos + len(word_text) <= marked_end:
                        is_already_marked = True
                        break

                if not is_already_marked:
                    # æ·»åŠ æ ‡è®°å‰çš„æ­£å¸¸æ–‡æœ¬
                    result.append(text[current_pos:found_pos])

                    # æ·»åŠ æ ‡è®°çš„è¯æ±‡
                    result.append(f"ã€{word_text}ã€‘")

                    # æ›´æ–°ä½ç½®å’Œè®°å½•æ ‡è®°èŒƒå›´
                    current_pos = found_pos + len(word_text)
                    marked_ranges.append((found_pos, current_pos))
                    break
                else:
                    # å·²ç»æ ‡è®°ï¼Œè·³è¿‡
                    search_start = found_pos + 1

        # æ·»åŠ å‰©ä½™çš„æ–‡æœ¬
        if current_pos < text_len:
            result.append(text[current_pos:])

        return ''.join(result)

    def _clean_bracket_symbols(self, text: str) -> str:
        """
        æ¸…ç†æ–‡æœ¬ä¸­AIå¯èƒ½é”™è¯¯æ·»åŠ çš„ã€ã€‘ç¬¦å·

        Args:
            text: éœ€è¦æ¸…ç†çš„æ–‡æœ¬

        Returns:
            str: æ¸…ç†åçš„æ–‡æœ¬
        """
        import re

        # 1. å¤„ç†å•ä¸ªå­—ç¬¦è¢«é”™è¯¯æ ‡è®°çš„æƒ…å†µï¼Œå¦‚"å¥³ã€æ€§ã€‘" -> "å¥³æ€§"
        # å…ˆç§»é™¤æ‰€æœ‰ã€ã€‘ï¼Œä¿ç•™å†…å®¹
        text = re.sub(r'ã€([^ã€‘]+)ã€‘', r'\1', text)

        # 2. å¤„ç†åµŒå¥—çš„ã€ã€‘ç¬¦å·ï¼ˆå¦‚æœè¿˜æœ‰æ®‹ç•™çš„è¯ï¼‰
        while 'ã€ã€' in text:
            text = re.sub(r'ã€ã€([^ã€‘]+)ã€‘ã€‘', r'ã€\1ã€‘', text)

        # 3. å¤„ç†ç©ºçš„ã€ã€‘ç¬¦å·
        text = re.sub(r'ã€\s*ã€‘', '', text)

        # 4. å¤„ç†åŒ…å«æ ‡ç‚¹ç¬¦å·çš„ã€ã€‘ç¬¦å·ï¼Œä¿ç•™æ ‡ç‚¹å¤–çš„å†…å®¹
        text = re.sub(r'ã€([^\s]*[ã€ã€‚ï¼ï¼Ÿ,.!?]+[^\s]*)ã€‘', r'\1', text)

        # 5. æœ€åç¡®ä¿æ²¡æœ‰æ®‹ç•™çš„æœªé—­åˆã€ã€‘ç¬¦å·
        # ç§»é™¤å­¤ç«‹çš„ã€æˆ–ã€‘
        text = re.sub(r'ã€(?![^ã€‘]*ã€‘)', '', text)  # ç§»é™¤æ²¡æœ‰å¯¹åº”ã€‘çš„ã€
        text = re.sub(r'(?<!ã€)ã€‘', '', text)      # ç§»é™¤æ²¡æœ‰å¯¹åº”ã€çš„ã€‘

        return text

    def _perform_text_correction(self, marked_segments: List[str], words: List[TimestampedWord], srt_entries: List[Dict] = None) -> tuple[List[str], List[str]]:
        """
        æ‰§è¡Œæ–‡æœ¬çº é”™ï¼ˆå¤ç”¨ç°æœ‰çš„LLMçº é”™é€»è¾‘ï¼‰

        Args:
            marked_segments: æ ‡è®°äº†ä½ç½®ä¿¡åº¦è¯æ±‡çš„æ–‡æœ¬ç‰‡æ®µåˆ—è¡¨
            words: åŸå§‹è¯æ±‡åˆ—è¡¨
            srt_entries: SRTæ¡ç›®åˆ—è¡¨ï¼ˆåŒ…å«æ—¶é—´ä¿¡æ¯ï¼Œç”¨äºç²¾å‡†å®šä½ï¼‰

        Returns:
            tuple: (çº é”™åçš„æ–‡æœ¬ç‰‡æ®µåˆ—è¡¨, æ ¡å¯¹æç¤ºåˆ—è¡¨)
        """
        # å¤ç”¨ç°æœ‰çš„_batch_correct_with_llmæ–¹æ³•ï¼Œä¼ é€’srt_entriesä»¥æ”¯æŒæ—¶é—´æˆ³åŒ¹é…
        return self._batch_correct_with_llm(marked_segments, words, srt_entries)

    def _analyze_text_change(self, original_text: str, corrected_text: str) -> dict:
        """
        åˆ†ææ–‡æœ¬å˜åŒ–çš„ç±»å‹ï¼šåŒºåˆ†"ç‰©ç†ä¿®æ”¹"ä¸"å®è´¨ä¿®æ”¹"

        é€»è¾‘ï¼š
        1. has_change: åªè¦å­—ç¬¦ä¸²ä¸åŒå°±æ˜¯True -> ç”¨äºå†³å®šæ˜¯å¦æ›´æ–°SRTæ–‡ä»¶ï¼ˆä¿ç•™æ¶¦è‰²ï¼‰
        2. is_content_change: åªæœ‰æ ‡å‡†åŒ–åä»ä¸åŒæ‰æ˜¯True -> ç”¨äºç»Ÿè®¡çº é”™æ•°é‡ï¼ˆè¿‡æ»¤æ ‡ç‚¹å·®å¼‚ï¼‰
        """
        import re

        # 1. é¢„å¤„ç†ï¼šå»é™¤ã€ã€‘æ ‡è®°
        unmarked_original = re.sub(r'ã€([^ã€‘]+)ã€‘', r'\1', original_text)

        # 2. å®šä¹‰æ ‡å‡†åŒ–å‡½æ•°ï¼ˆæ ¸å¿ƒä¿®æ”¹ç‚¹ï¼‰
        def normalize_text(text: str) -> str:
            # å»é™¤é¦–å°¾ç©ºç™½
            text = text.strip()

            # === æ ¸å¿ƒï¼šç»Ÿä¸€çœç•¥å·æ ¼å¼ ===
            # å°†å„ç§å˜ä½“çš„çœç•¥å·ç»Ÿä¸€æ›¿æ¢ä¸ºæ ‡å‡†ASCIIçœç•¥å·ï¼Œæ¶ˆé™¤æ ¼å¼å·®å¼‚
            # é¡ºåºå¾ˆé‡è¦ï¼šå…ˆå¤„ç†é•¿çš„ï¼Œå†å¤„ç†çŸ­çš„
            text = text.replace('â€¦â€¦', '...').replace('......', '...').replace('â€¦', '...')

            # å¯é€‰ï¼šç»Ÿä¸€å…¨åŠè§’é€—å¥å·ï¼ˆè§†éœ€æ±‚å¼€å¯ï¼Œé˜²æ­¢ "hello," vs "helloï¼Œ" è¢«ç®—ä½œé”™è¯¯ï¼‰
            # text = text.replace('ï¼Œ', ',').replace('ã€‚', '.')

            # å»é™¤æ ‡ç‚¹ç¬¦å·å‘¨è¾¹çš„å¤šä½™ç©ºæ ¼ï¼ˆé˜²æ­¢ "Hello ." vs "Hello."ï¼‰
            text = re.sub(r'\s*([ï¼Œã€‚ï¼ï¼Ÿã€ï¼šï¼›,.!?])\s*', r'\1', text)

            # å°†è¿ç»­çš„ç©ºæ ¼åˆå¹¶ä¸ºä¸€ä¸ª
            text = re.sub(r'\s+', ' ', text)

            return text

        # 3. è·å–æ ‡å‡†åŒ–åçš„æ–‡æœ¬
        normalized_unmarked = normalize_text(unmarked_original)
        normalized_corrected = normalize_text(corrected_text)

        # 4. æ‰§è¡ŒåŒé‡åˆ¤å®š
        # ç‰©ç†åˆ¤å®šï¼šåªè¦æœ‰å˜åŠ¨ï¼ˆåŒ…æ‹¬æ ‡ç‚¹æ¶¦è‰²ï¼‰ï¼Œå°±è§†ä¸º True
        has_change = original_text != corrected_text

        # é€»è¾‘åˆ¤å®šï¼šåªæœ‰å®è´¨å†…å®¹å˜äº†ï¼Œæ‰è§†ä¸º True
        is_content_change = normalized_unmarked != normalized_corrected

        # 5. å‡†å¤‡æ—¥å¿—ç”¨çš„æˆªæ–­æ–‡æœ¬
        max_display_length = 40
        before_text = unmarked_original if 'ã€' in original_text else original_text
        before_short = before_text[:max_display_length]
        after_short = corrected_text[:max_display_length]

        return {
            "has_change": has_change,           # æ§åˆ¶æ˜¯å¦å†™å…¥ SRT
            "is_content_change": is_content_change, # æ§åˆ¶æ˜¯å¦è®¡å…¥æŠ¥å‘Šç»Ÿè®¡
            "before": before_text,
            "after": corrected_text,
            "before_short": before_short,
            "after_short": after_short,
            "original_text": original_text,
            "unmarked_original": unmarked_original
        }

    def _rebuild_srt_content(self, original_entries: List[Dict], corrected_texts: List[str]) -> str:
        """
        ä½¿ç”¨æ ¡å¯¹åçš„æ–‡æœ¬é‡æ–°æ„å»ºSRTå†…å®¹

        Args:
            original_entries: åŸå§‹SRTæ¡ç›®åˆ—è¡¨
            corrected_texts: æ ¡å¯¹åçš„æ–‡æœ¬åˆ—è¡¨

        Returns:
            str: é‡æ–°æ„å»ºçš„SRTå†…å®¹
        """
        rebuilt_lines = []

        for i, entry in enumerate(original_entries):
            # ä½¿ç”¨å¯¹åº”çš„çº é”™æ–‡æœ¬ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            corrected_text = corrected_texts[i] if i < len(corrected_texts) else entry['text']

            # æ„å»ºSRTæ¡ç›®
            rebuilt_lines.append(str(entry['index']))
            rebuilt_lines.append(entry['time'])
            rebuilt_lines.append(corrected_text)
            rebuilt_lines.append('')  # ç©ºè¡Œåˆ†éš”

        return '\n'.join(rebuilt_lines).strip()

    def _batch_correct_with_llm(self, segments: List[str], words: List[TimestampedWord], srt_entries: List[Dict] = None) -> tuple[List[str], List[str]]:
        """
        ä½¿ç”¨LLMæ™ºèƒ½çº æ­£ä½ç½®ä¿¡åº¦è¯æ±‡

        Args:
            segments: åŸå§‹æ–‡æœ¬ç‰‡æ®µåˆ—è¡¨
            words: Sonioxè¿”å›çš„è¯æ±‡åˆ—è¡¨ï¼ˆåŒ…å«ç½®ä¿¡åº¦ä¿¡æ¯ï¼‰

        Returns:
            tuple: (çº æ­£åçš„æ–‡æœ¬ç‰‡æ®µåˆ—è¡¨, æ ¡å¯¹æç¤ºåˆ—è¡¨)
        """
        if not segments:
            return segments, []

        # ç¬¬0æ­¥ï¼šè·å–æ‰€æœ‰ä½ç½®ä¿¡åº¦è¯æ±‡ï¼ˆç”¨äºæç¤ºå’Œè¯†åˆ«ï¼‰
        low_confidence_words = []
        for word in words:
            if word.confidence < app_config.DEFAULT_SONIOX_LOW_CONFIDENCE_THRESHOLD:
                low_confidence_words.append(word.text)

        # ç¬¬1æ­¥ï¼šæ™ºèƒ½è¯†åˆ«éœ€è¦çº é”™çš„ç‰‡æ®µï¼ˆä¼ é€’srt_entriesä»¥æ”¯æŒæ—¶é—´æˆ³åŒ¹é…ï¼‰
        target_segments = self._identify_segments_requiring_correction(segments, words, srt_entries)

        if not target_segments:
            self.log("âšª æœªå‘ç°éœ€è¦çº é”™çš„ç‰‡æ®µ")
            return segments, []

        # ç»Ÿè®¡åŒ…å«ä½ç½®ä¿¡åº¦è¯æ±‡çš„ç‰‡æ®µæ•°é‡
        segments_with_corrections = set()
        for i in target_segments:
            segments_with_corrections.add(i)

        # è®¡ç®—è¿‡æ»¤åçš„è¯æ±‡æ•°é‡
        filtered_low_conf_count = 0
        for word_text in low_confidence_words:
            # low_confidence_words ä¸­å·²ç»å­˜å‚¨äº†å­—ç¬¦ä¸²ï¼ˆword.textï¼‰
            word_text = word_text.strip()
            if (word_text and
                word_text.strip() and  # éç©º
                not all(c in ' ã€ã€‚ï¼ï¼Ÿ,.!?ãƒ¼â€¦' for c in word_text)):  # ä¸æ˜¯çº¯æ ‡ç‚¹ç¬¦å·
                filtered_low_conf_count += 1

        correction_hints = []
        if low_confidence_words:
            correction_hints.append(f"ğŸ“Š å‘ç° {len(low_confidence_words)} ä¸ªä½ç½®ä¿¡åº¦è¯æ±‡")
            correction_hints.append(f"ğŸ¯ {len(segments_with_corrections)} ä¸ªç‰‡æ®µéœ€è¦AIæ ¡å¯¹")

        self.log(f"ğŸ¤– å¼€å§‹AIæ ¡å¯¹:")
        # åªæ˜¾ç¤ºæœ€é‡è¦çš„ç»Ÿè®¡ä¿¡æ¯ï¼šå»é™¤ç¬¦å·åçš„ä½ç½®ä¿¡åº¦è¯æ±‡æ•°é‡
        if filtered_low_conf_count != len(low_confidence_words):
            self.log(f"   â€¢ ğŸ“Š å‘ç° {filtered_low_conf_count} ä¸ªä½ç½®ä¿¡åº¦è¯æ±‡")
        else:
            self.log(f"   â€¢ ğŸ“Š å‘ç° {filtered_low_conf_count} ä¸ªä½ç½®ä¿¡åº¦è¯æ±‡")
        self.log(f"   â€¢ ğŸ¯ {len(segments_with_corrections)} ä¸ªç‰‡æ®µéœ€è¦æ ¡å¯¹")

        # ä¸å†æ˜¾ç¤ºç‰‡æ®µé¢„è§ˆï¼Œç®€åŒ–ç”¨æˆ·æ—¥å¿—

        # ç¬¬2æ­¥ï¼šåˆ›å»ºæ™ºèƒ½æ‰¹æ¬¡ï¼ˆåŒ…å«ä¸Šä¸‹æ–‡ï¼‰- ç¡®ä¿åªå¤„ç†çœŸæ­£éœ€è¦æ ¡å¯¹çš„ç‰‡æ®µ
        segment_batches = self._prepare_smart_correction_batches(segments, words, target_segments)

        # === ä¿®æ”¹ç‚¹ 1: ç§»é™¤è¿‡æ—¶çš„éªŒè¯è­¦å‘Šï¼Œæ”¹ç”¨ç²¾å‡†ç»Ÿè®¡ ===

        # ç»Ÿè®¡æ‰€æœ‰æ‰¹æ¬¡é‡Œå®é™…åŒ…å«çš„"ç›®æ ‡"æ€»æ•°
        global_target_set = set(target_segments)
        count_targets_in_batches = 0
        total_payload_size = 0

        for batch in segment_batches:
            total_payload_size += len(batch)
            for idx in batch:
                if idx in global_target_set:
                    count_targets_in_batches += 1

        self.log(f"ğŸ” æ‰¹æ¬¡æ„å»ºç»Ÿè®¡ï¼šç›®æ ‡è¦†ç›– {count_targets_in_batches}/{len(target_segments)}ï¼Œæ€»è½½è· {total_payload_size} ç‰‡æ®µ(å«ä¸Šä¸‹æ–‡)")

        # åªæœ‰å½“"åŒ…å«çš„ç›®æ ‡æ•°"ä¸ç­‰äº"åŸæœ¬çš„ç›®æ ‡æ•°"æ—¶ï¼Œæ‰æŠ¥è­¦
        if count_targets_in_batches != len(target_segments):
            self.log(f"âš ï¸ ä¸¥é‡è­¦å‘Šï¼šæ‰¹æ¬¡æ„å»ºä¸¢å¤±äº†éƒ¨åˆ†ç›®æ ‡ï¼({count_targets_in_batches} vs {len(target_segments)})")

        # å°†å…¨å±€ç›®æ ‡è½¬æ¢ä¸ºé›†åˆï¼Œæé«˜æŸ¥æ‰¾æ•ˆç‡
        global_target_set = set(target_segments)

        # å°†å…¨å±€ç›®æ ‡è½¬æ¢ä¸ºé›†åˆï¼Œæé«˜æŸ¥æ‰¾æ•ˆç‡ï¼ˆé˜²æ­¢é‡å¤å®šä¹‰ï¼‰
        if not hasattr(self, '_global_target_set'):
            self._global_target_set = set(target_segments)

        # è°ƒç”¨LLMè¿›è¡Œçº é”™
        original_segments = list(segments)  # ä¿å­˜åŸå§‹å¸¦æ ‡è®°çš„å‰¯æœ¬ï¼Œç”¨äºæ¯”è¾ƒ
        corrected_segments = list(segments)  # åˆ›å»ºå‰¯æœ¬ç”¨äºä¿®æ”¹
        total_corrections = 0

        for batch_idx, batch_indices in enumerate(segment_batches):
            batch_segments = [segments[i] for i in batch_indices]
            try:
                # === ä¿®æ”¹ç‚¹ 2: ä¼˜åŒ–å¾ªç¯å†…çš„æ—¥å¿—æ˜¾ç¤ºï¼Œæ¶ˆé™¤æ­§ä¹‰ ===
                # è®¡ç®—å½“å‰æ‰¹æ¬¡é‡Œçš„ç›®æ ‡æ•°
                current_batch_targets = sum(1 for i in batch_indices if i in self._global_target_set)

                # ä¼˜åŒ–æ—¥å¿—ï¼šæ˜ç¡®æ˜¾ç¤º (æ€»è½½è· vs ç›®æ ‡æ•°)
                self.log(f"ğŸ”¥ å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{len(segment_batches)} (è½½è·: {len(batch_indices)}ç‰‡æ®µ | å«ç›®æ ‡: {current_batch_targets}ä¸ª)")

                # === å…³é”®ä¿®æ”¹å¼€å§‹ ===
                # è®¡ç®—å½“å‰æ‰¹æ¬¡ä¸­ï¼Œå“ªäº›æ˜¯çœŸæ­£çš„ä»»åŠ¡ç›®æ ‡ï¼ˆLocal Indexï¼‰
                # batch_indices åŒ…å«äº† [é‚»å±…, ç›®æ ‡, é‚»å±…]
                # æˆ‘ä»¬éœ€è¦æ‰¾å‡º "ç›®æ ‡" åœ¨ batch_segments ä¸­çš„ä¸‹æ ‡ (0, 1, 2...)
                real_task_local_indices = []

                for local_idx, global_idx in enumerate(batch_indices):
                    if global_idx in self._global_target_set:
                        real_task_local_indices.append(local_idx)

                # å¦‚æœè®¡ç®—å‡ºæ²¡æœ‰ç›®æ ‡ï¼ˆç†è®ºä¸Šä¸å¯èƒ½ï¼Œé˜²å®ˆæ€§ç¼–ç¨‹ï¼‰ï¼Œåˆ™è·³è¿‡
                if not real_task_local_indices:
                    self.log(f"  âš ï¸ æ‰¹æ¬¡ {batch_idx + 1} ä¸­æœªæ‰¾åˆ°ç›®æ ‡ç‰‡æ®µï¼Œè·³è¿‡")
                    continue

                # === å…³é”®ä¿®æ”¹ç»“æŸ ===

                # ä¸ºæ™ºèƒ½çº é”™æ„å»ºä¸“ç”¨promptï¼ˆåŒ…å«å®Œæ•´ä¸Šä¸‹æ–‡ï¼‰
                smart_prompt = self._build_smart_correction_prompt(
                    batch_segments,
                    low_confidence_words,
                    all_segments=segments,  # ä¼ å…¥å®Œæ•´ç‰‡æ®µåˆ—è¡¨ä½œä¸ºä¸Šä¸‹æ–‡
                    target_indices=batch_indices,  # ä¼ å…¥å½“å‰æ‰¹æ¬¡çš„å®é™…ç´¢å¼•
                    target_local_indices=real_task_local_indices  # ä¼ å…¥çœŸå®ä»»åŠ¡ç´¢å¼•
                )
                response = self._call_llm_api(smart_prompt, batch_segments)

                # è§£æLLMå“åº”
                corrections = self._parse_llm_correction_response(response)

                if not corrections:
                    self.log(f"  âšª æ— çº é”™ç»“æœ")
                    continue

                # åº”ç”¨çº é”™ç»“æœ
                batch_corrections = 0

                for correction in corrections:
                    # LLMè¿”å›çš„æ˜¯æ‰¹æ¬¡å†…ç›¸å¯¹ç´¢å¼•ï¼ˆ0, 1, 2...ï¼‰ï¼Œå¯¹åº”batch_segmentsä¸­çš„ä½ç½®
                    relative_idx = correction.get("segment_index", -1)
                    corrected_text = correction["corrected_text"]

                    # æ£€æŸ¥ç›¸å¯¹ç´¢å¼•æ˜¯å¦æœ‰æ•ˆ
                    if 0 <= relative_idx < len(batch_segments):
                        # é€šè¿‡ç›¸å¯¹ç´¢å¼•æ‰¾åˆ°å¯¹åº”çš„ç»å¯¹ç´¢å¼•
                        if relative_idx < len(batch_indices):
                            actual_idx = batch_indices[relative_idx]

                            if actual_idx < len(original_segments):
                                # ä½¿ç”¨åŸå§‹å¸¦æ ‡è®°çš„æ–‡æœ¬è¿›è¡Œæ¯”è¾ƒ
                                original_text = original_segments[actual_idx]

                                # åˆ†æä¿®æ”¹ç±»å‹ï¼šçœŸæ­£ä¿®æ”¹ vs æ ‡è®°å»é™¤
                                change_info = self._analyze_text_change(original_text, corrected_text)

                                if change_info["has_change"]:
                                    if change_info["is_content_change"]:
                                        # çœŸæ­£çš„å†…å®¹ä¿®æ”¹
                                        self.log(f"  ğŸ”§ ç‰‡æ®µ{actual_idx + 1}: {change_info['before_short']}... â†’ {change_info['after_short']}...")
                                        batch_corrections += 1
                                        total_corrections += 1
                                        # æ·»åŠ åˆ°çœŸæ­£ä¿®æ”¹çš„è®°å½•ä¸­
                                        correction_hints.append(f"ğŸ”§ ç‰‡æ®µ{actual_idx + 1}: {change_info['before']} â†’ {change_info['after']}")
                                    else:
                                        # ä»…å»é™¤æ ‡è®°ï¼Œä¸æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
                                        pass

                                # æ— è®ºæ˜¯å¦æœ‰å˜åŒ–éƒ½åº”ç”¨ä¿®æ­£ï¼ˆå› ä¸ºLLMå¯èƒ½å»é™¤ã€ã€‘æ ‡è®°ï¼‰
                                corrected_segments[actual_idx] = corrected_text

                if batch_corrections == 0:
                    self.log(f"  âšª æ— å®é™…å˜åŒ–")

            except Exception as e:
                self.log(f"  âŒ æ‰¹æ¬¡å¤±è´¥: {e}")
                continue

        # æœ€ç»ˆç»Ÿè®¡å°†åœ¨å‡½æ•°ç»“æŸæ—¶æ˜¾ç¤ºï¼Œé¿å…é‡å¤ä¿¡æ¯

        # ä»correction_hintsä¸­ç»Ÿè®¡çœŸæ­£ä¿®æ”¹çš„æ•°é‡
        content_corrections = len([h for h in correction_hints if h.startswith("ğŸ”§ ç‰‡æ®µ")])
        mark_removals = len([h for h in correction_hints if "å»é™¤æ ‡è®°" in str(h)]) if hasattr(self, '_log_messages') else None

        # ç”Ÿæˆé‡æ–°ç»„ç»‡çš„æ ¡å¯¹æ€»ç»“
        # è·å–ç»Ÿè®¡ä¿¡æ¯ï¼ˆä½¿ç”¨ä¸æ ‡è®°é€»è¾‘ç›¸åŒçš„è¿‡æ»¤æ¡ä»¶ï¼‰
        filtered_low_conf_words = []
        for word in words:
            if word.confidence < app_config.DEFAULT_SONIOX_LOW_CONFIDENCE_THRESHOLD:
                word_text = word.text.strip()

                # åº”ç”¨æ›´å®½æ¾çš„è¿‡æ»¤æ¡ä»¶ï¼Œä¸»è¦è¿‡æ»¤çº¯æ ‡ç‚¹ç¬¦å·å’Œç©ºç™½
                if (word_text and
                    word_text.strip() and  # éç©º
                    not all(c in ' ã€ã€‚ï¼ï¼Ÿ,.!?ãƒ¼â€¦' for c in word_text)):  # ä¸æ˜¯çº¯æ ‡ç‚¹ç¬¦å·
                    filtered_low_conf_words.append(word)

        low_conf_words_count = len(filtered_low_conf_words)
        segments_needing_correction = len(target_segments) if 'target_segments' in locals() else 0

        # æ„å»ºæŠ¥å‘Šæ ‡é¢˜å’Œæ€»ä½“ç»Ÿè®¡
        summary = f"ğŸ¯ AIæ ¡å¯¹æŠ¥å‘Šï¼š"
        summary += f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡ï¼š"
        summary += f"\n   â€¢ å‘ç° {low_conf_words_count} ä¸ªä½ç½®ä¿¡åº¦è¯æ±‡"
        summary += f"\n   â€¢ {segments_needing_correction} ä¸ªç‰‡æ®µéœ€è¦AIæ ¡å¯¹"
        summary += f"\n   â€¢ ğŸ”§ çœŸæ­£ä¿®æ”¹äº† {content_corrections} ä¸ªç‰‡æ®µçš„å†…å®¹"
        if total_corrections > content_corrections:
            summary += f"\n   â€¢ âœ¨ å»é™¤äº† {total_corrections - content_corrections} ä¸ªç‰‡æ®µçš„ã€ã€‘æ ‡è®°"

        # å¤„ç†ä¿®æ”¹è¯¦æƒ…
        detail_hints = [h for h in correction_hints if h.startswith("ğŸ”§ ç‰‡æ®µ")]
        if detail_hints:
            # åˆ†ç¦»ç»Ÿè®¡ä¿¡æ¯å’Œè¯¦ç»†ä¿¡æ¯
            stats_only = summary
            details_section = "\nğŸ“‹ å…·ä½“ä¿®æ”¹è¯¦æƒ…ï¼š\n" + "â”€" * 58

            # è¿‡æ»¤æ‰æ‰€æœ‰ç»Ÿè®¡ç±»ä¿¡æ¯ï¼Œåªä¿ç•™å…·ä½“çš„ä¿®æ”¹è¯¦æƒ…
            filtered_hints = []
            for h in correction_hints:
                # è·³è¿‡å„ç§ç»Ÿè®¡ä¿¡æ¯
                if (h.startswith("ğŸ”§ ç‰‡æ®µ") or
                    h.startswith("ğŸ“Š å‘ç°") or
                    h.startswith("ğŸ¯ ") and "ä¸ªç‰‡æ®µéœ€è¦AIæ ¡å¯¹" in h or
                    "ä½ç½®ä¿¡åº¦è¯æ±‡" in h and ":" in h):
                    continue
                filtered_hints.append(h)

            # é‡æ–°ç»„ç»‡hintsï¼šç»Ÿè®¡ + åˆ†éš”çº¿ + è¯¦æƒ…
            correction_hints = filtered_hints
            correction_hints.insert(0, stats_only)
            correction_hints.insert(1, details_section)
            correction_hints.extend(detail_hints)
        else:
            # æ²¡æœ‰å…·ä½“ä¿®æ”¹çš„æƒ…å†µ - é‡æ–°æ„å»ºhintsè€Œä¸æ˜¯æ’å…¥
            if content_corrections == 0 and total_corrections == 0:
                summary = "ğŸ¯ AIæ ¡å¯¹æŠ¥å‘Šï¼šæ— éœ€ä»»ä½•ä¿®æ”¹"
            elif content_corrections == 0 and total_corrections > 0:
                summary = f"ğŸ¯ AIæ ¡å¯¹æŠ¥å‘Šï¼šä»…å»é™¤äº† {total_corrections} ä¸ªç‰‡æ®µçš„ã€ã€‘æ ‡è®°"

            # é‡æ–°æ„å»ºcorrection_hintsï¼Œæ¸…é™¤ä¹‹å‰çš„ç»Ÿè®¡ä¿¡æ¯
            correction_hints = [summary, "â”€" * 50]

        return corrected_segments, correction_hints

    def _is_reasoning_model(self, model_name: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºreasoningæ¨¡å‹ï¼ˆéœ€è¦ç‰¹æ®Šå‚æ•°å¤„ç†ï¼‰
        
        Reasoningæ¨¡å‹ç‰¹å¾ï¼š
        1. ä½¿ç”¨ max_completion_tokens è€Œä¸æ˜¯ max_tokens
        2. ä¸æ”¯æŒ temperature ç­‰é‡‡æ ·å‚æ•°
        
        åŒ…æ‹¬ï¼š
        - oç³»åˆ—: o1, o1-mini, o3, o3-mini, o4-mini ç­‰
        - gpt-5ç³»åˆ—: gpt-5, gpt-5.1, gpt-5.2, gpt-5.3 åŠå…¶å˜ä½“
        
        Args:
            model_name: æ¨¡å‹åç§°
            
        Returns:
            bool: å¦‚æœæ˜¯reasoningæ¨¡å‹è¿”å›True
        """
        if not model_name:
            return False
        
        import re
        model_lower = model_name.lower()
        
        # oç³»åˆ— reasoningæ¨¡å‹
        # åŒ¹é…: o1, o1-xxx, o3, o3-xxx, o4, o4-xxx ç­‰
        if re.match(r'^o\d+', model_lower):
            return True
        
        # gpt-5ç³»åˆ—åŠå…¶æ‰€æœ‰å˜ä½“
        # åŒ¹é…: gpt-5, gpt-5.x, gpt-5-xxx, gpt5-xxx ç­‰
        if re.match(r'^gpt-?5', model_lower):
            return True
        
        return False

    def _call_llm_api(self, prompt: str, batch_segments: List[str]) -> str:
        """
        è°ƒç”¨LLM APIè¿›è¡Œæ–‡æœ¬çº é”™

        Args:
            prompt: çº é”™æç¤ºè¯

        Returns:
            LLM APIå“åº”æ–‡æœ¬
        """
        try:
            import requests
            import json

            # è·å–LLM APIé…ç½®
            api_config = self.get_current_llm_config_for_api_call()

            api_key = api_config['api_key']
            input_base_url = api_config.get('custom_api_base_url_str', app_config.DEFAULT_LLM_API_BASE_URL)
            model_name = api_config.get('custom_model_name', app_config.DEFAULT_LLM_MODEL_NAME)
            temperature = api_config.get('custom_temperature', app_config.DEFAULT_LLM_TEMPERATURE)

            # å¤„ç†API URLï¼Œç¡®ä¿åŒ…å«æ­£ç¡®çš„è·¯å¾„
            if not input_base_url:
                # ä½¿ç”¨é»˜è®¤URLå¹¶æ·»åŠ å®Œæ•´è·¯å¾„
                base_url = app_config.DEFAULT_LLM_API_BASE_URL
                if not base_url.endswith('/'):
                    base_url += '/'
                base_url += "v1/chat/completions"
            else:
                raw_url = input_base_url.strip()
                # æ£€æŸ¥æ˜¯å¦æ˜¯å®Œæ•´URLï¼ˆä»¥#ç»“å°¾ï¼‰
                if raw_url.endswith('#'):
                    base_url = raw_url[:-1]  # ç§»é™¤#æ ‡è®°
                else:
                    # æ ¹æ®APIç±»å‹æ·»åŠ æ­£ç¡®çš„è·¯å¾„
                    if "api.anthropic.com" in raw_url:
                        base_url = raw_url
                        if not base_url.endswith('/'):
                            base_url += '/'
                        base_url += "v1/messages"
                    elif "generativelanguage.googleapis.com" in raw_url:
                        base_url = raw_url
                        # Gemini APIç›´æ¥ä½¿ç”¨base_urlï¼Œåœ¨è¯·æ±‚æ—¶æ·»åŠ ?keyå‚æ•°
                    else:
                        # OpenAIå…¼å®¹æ ¼å¼
                        base_url = raw_url
                        if not base_url.endswith('/'):
                            base_url += '/'
                        base_url += "v1/chat/completions"

            # ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„å®Œæ•´promptï¼Œé¿å…é‡å¤æ„å»º
            # promptå‚æ•°å·²ç»åŒ…å«äº†å®Œæ•´çš„çº é”™æŒ‡ä»¤
            full_prompt = prompt

            self.log(f"ğŸ“ è°ƒç”¨LLM API: {model_name}")

            # æ„å»ºAPIè¯·æ±‚
            if "generativelanguage.googleapis.com" in base_url:
                # Gemini APIæ ¼å¼
                payload = {
                    "contents": [{"parts": [{"text": full_prompt}]}],
                    "generationConfig": {
                        "temperature": temperature,
                        "max_tokens": 4000
                    }
                }
                headers = {"Content-Type": "application/json"}
                # Gemini APIä½¿ç”¨ä¸åŒçš„è®¤è¯æ–¹å¼
                response = requests.post(f"{base_url}?key={api_key}", headers=headers, json=payload, timeout=180)
            elif "/v1/messages" in base_url or "api.anthropic.com" in base_url:
                # Claude APIæ ¼å¼
                payload = {
                    "model": model_name,
                    "max_tokens": 4000,
                    "messages": [{"role": "user", "content": full_prompt}]
                }
                if temperature is not None:
                    payload["temperature"] = temperature

                headers = {
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {api_key}",
                    "anthropic-version": "2023-06-01"
                }
                response = requests.post(base_url, headers=headers, json=payload, timeout=180)
            else:
                # OpenAIå…¼å®¹æ ¼å¼ - å°†å®Œæ•´promptæ‹†åˆ†ä¸ºsystemå’Œuseréƒ¨åˆ†
                # ä»full_promptä¸­æå–system_promptéƒ¨åˆ†
                lines = full_prompt.split('\n')
                system_content = ""
                user_content = ""

                # æ‰¾åˆ°ç³»ç»Ÿæç¤ºè¯éƒ¨åˆ†
                if lines and ("ASRé”™è¯æ ¡å¯¹" in lines[0] or "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ASR" in lines[0]):
                    # å¦‚æœç¬¬ä¸€è¡ŒåŒ…å«ç³»ç»Ÿæç¤ºï¼Œåˆ™åˆ†å‰²
                    system_content = '\n'.join([line for line in lines if line.strip() and (line.startswith('ä½ æ˜¯ä¸€ä½') or line.startswith('è¯·ä¸¥æ ¼éµå®ˆ') or 'ASRé”™è¯æ ¡å¯¹' in line or 'åªä¿®æ­£é”™åˆ«å­—' in line or 'ä¸¥ç¦é‡å†™' in line or 'è¾“å‡ºæ ¼å¼' in line)])
                    user_content = '\n'.join([line for line in lines if line.strip() and line not in system_content.split('\n')])
                else:
                    # å¦åˆ™ä½¿ç”¨é»˜è®¤åˆ†å‰²
                    system_content = app_config.DEEPSEEK_SYSTEM_PROMPT_CORRECTION
                    user_content = full_prompt

                payload = {
                    "model": model_name,
                    "messages": [
                        {"role": "system", "content": system_content},
                        {"role": "user", "content": user_content}
                    ]
                }
                
                # [FIX] Reasoningæ¨¡å‹ï¼ˆGPT-5ç³»åˆ—ã€oç³»åˆ—ï¼‰éœ€è¦ç‰¹æ®Šå¤„ç†
                if self._is_reasoning_model(model_name):
                    # ä½¿ç”¨ max_completion_tokens è€Œä¸æ˜¯ max_tokens
                    payload["max_completion_tokens"] = 4000
                    # ä¸ä¼  temperatureï¼Œä½¿ç”¨æ¨¡å‹é»˜è®¤å€¼
                else:
                    # ä¼ ç»Ÿæ¨¡å‹ä½¿ç”¨ max_tokens å’Œè‡ªå®šä¹‰ temperature
                    payload["max_tokens"] = 4000
                    if temperature is not None:
                        payload["temperature"] = temperature

                headers = {
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {api_key}"
                }
                response = requests.post(base_url, headers=headers, json=payload, timeout=180)

            response.raise_for_status()
            data = response.json()

            # è§£æå“åº”
            content = None
            if "choices" in data and data["choices"]:
                content = data["choices"][0].get("message", {}).get("content")
            elif "candidates" in data and data["candidates"]:
                content = data["candidates"][0].get("content", {}).get("parts", [{}])[0].get("text")

            if content:
                self.log(f"ğŸ“¨ LLMå“åº”æˆåŠŸ ({len(content)}å­—ç¬¦)")
                return content.strip()
            else:
                self.log("âš ï¸ LLMè¿”å›ç©ºå†…å®¹")
                return '{"corrections": []}'

        except requests.exceptions.Timeout:
            self.log("â° LLM APIè¶…æ—¶")
            return '{"corrections": []}'
        except requests.exceptions.RequestException as e:
            self.log(f"ğŸŒ LLMè¯·æ±‚å¤±è´¥: {e}")
            return '{"corrections": []}'
        except json.JSONDecodeError as e:
            self.log(f"ğŸ“„ LLMå“åº”è§£æå¤±è´¥")
            return '{"corrections": []}'
        except Exception as e:
            self.log(f"âŒ LLMè°ƒç”¨å¤±è´¥: {e}")
            return '{"corrections": []}'

    def _parse_llm_correction_response(self, response: str) -> List[Dict[str, Any]]:
        """
        è§£æLLMçº é”™å“åº”

        Args:
            response: LLM APIè¿”å›çš„JSONå“åº”

        Returns:
            çº æ­£ç»“æœåˆ—è¡¨
        """
        try:
            if not response.strip():
                self.log("  âš ï¸ LLMè¿”å›ç©ºå“åº”")
                return []

            # å¦‚æœä¸æ˜¯çº¯JSONæ ¼å¼ï¼Œå°è¯•æå–JSONéƒ¨åˆ†
            if not response.startswith('{') and not response.startswith('['):
                import re
                json_match = re.search(r'\{.*\}', response, re.DOTALL)
                if json_match:
                    response = json_match.group(0)
                else:
                    self.log("  âš ï¸ æ— æ³•ä»å“åº”ä¸­æå–JSON")
                    return []

            # è§£æJSONå“åº”
            response_data = json.loads(response)

            # æ ¹æ®ç³»ç»Ÿæç¤ºè¯æ ¼å¼ï¼Œå“åº”åº”è¯¥æ˜¯ {"line_id": "corrected_text", ...}
            valid_corrections = []

            # ä¼˜å…ˆå¤„ç†ç³»ç»Ÿæç¤ºè¯ä¸­å®šä¹‰çš„æ ¼å¼ï¼š{"0": "ä¿®æ­£åçš„ç¬¬ä¸€å¥", "5": "ä¿®æ­£åçš„ç¬¬å…­å¥"}
            for line_id, corrected_text in response_data.items():
                try:
                    # è·³è¿‡éå­—ç¬¦ä¸²é”®
                    if not isinstance(line_id, str):
                        continue

                    # æ£€æŸ¥æ˜¯å¦æ˜¯æ•°å­—å­—ç¬¦ä¸²ï¼ˆè¡Œå·ï¼‰
                    if line_id.isdigit():
                        segment_index = int(line_id)
                    else:
                        continue  # è·³è¿‡éæ•°å­—é”®

                    if isinstance(corrected_text, str) and corrected_text.strip():
                        valid_corrections.append({
                            "segment_index": segment_index,
                            "original_text": "",  # åœ¨è¿™ç§æ ¼å¼ä¸­æ²¡æœ‰åŸå§‹æ–‡æœ¬
                            "corrected_text": corrected_text.strip(),
                            "changes": []  # åœ¨è¿™ç§æ ¼å¼ä¸­æ²¡æœ‰å˜æ›´åˆ—è¡¨
                        })
                        # ä¸å†æ˜¾ç¤ºæ¯ä¸ªçº é”™çš„è§£æè¯¦æƒ…ï¼Œç®€åŒ–ç”¨æˆ·æ—¥å¿—
                except (ValueError, TypeError) as e:
                    self.log(f"  è§£æçº é”™é¡¹å¤±è´¥: {e}")
                    continue

            # å¦‚æœæ˜¯æ ‡å‡†æ ¼å¼ {"corrections": [...]} (å…¼å®¹æ€§å¤„ç†)
            if "corrections" in response_data and not valid_corrections:
                corrections = response_data.get("corrections", [])
                for correction in corrections:
                    if not isinstance(correction, dict):
                        continue

                    segment_index = correction.get("segment_index", -1)
                    original_text = correction.get("original_text", "")
                    corrected_text = correction.get("corrected_text", "")
                    changes = correction.get("changes", [])

                    if (isinstance(segment_index, int) and segment_index >= 0 and
                        isinstance(corrected_text, str) and corrected_text.strip()):
                        valid_corrections.append({
                            "segment_index": segment_index,
                            "original_text": original_text,
                            "corrected_text": corrected_text,
                            "changes": changes
                        })

            if valid_corrections:
                # çº é”™æ•°é‡åœ¨æœ€ç»ˆç»Ÿè®¡ä¸­æ˜¾ç¤ºï¼Œè¿™é‡Œä¸å†é‡å¤
                pass

            return valid_corrections

        except json.JSONDecodeError as e:
            self.log("ğŸ“„ JSONè§£æé”™è¯¯")
            return []
        except Exception as e:
            self.log(f"âŒ å“åº”è§£æå¤±è´¥: {e}")
            return []

    def process_to_srt(self, parsed_transcription: ParsedTranscription,
                       llm_segments_text: List[str],
                       source_format: str = "elevenlabs",
                       enable_ai_correction: bool = False
                      ) -> tuple[Optional[str], List[str]]:
        self.log("--- å¼€å§‹å¯¹é½ LLM ç‰‡æ®µ (SrtProcessor) ---")

        # åœ¨å‡½æ•°å¼€å§‹å°±åˆå§‹åŒ–correction_hintsï¼Œé¿å…å˜é‡ä½œç”¨åŸŸé”™è¯¯
        correction_hints: List[str] = []

        # ç¡®å®šå¤„ç†æ¨¡å¼
        source_format_lower = source_format.lower() if source_format else ""
        if source_format_lower == "soniox":
            processing_mode = "C"
            self.log("è¯†åˆ«å¤„ç†æ¨¡å¼: Mode C (Sonioxæ™ºèƒ½å¤„ç†)")
        elif source_format_lower in ["elevenlabs", "elevenlabs_api"]:
            processing_mode = "B"
            self.log("è¯†åˆ«å¤„ç†æ¨¡å¼: Mode B (ElevenLabså…¼å®¹å¤„ç†)")
        else:
            processing_mode = "A"
            self.log(f"è¯†åˆ«å¤„ç†æ¨¡å¼: Mode A (åŸºç¡€å¤„ç†) - æºæ ¼å¼: {source_format}")

        # AIçº é”™åŠŸèƒ½å°†åœ¨SRTç”Ÿæˆåè¿›è¡Œï¼Œé¿å…æ•°æ®ä¸ä¸€è‡´é—®é¢˜
        # æ³¨æ„ï¼šenable_ai_correction å‚æ•°å°†åœ¨æœ€åé˜¶æ®µä½¿ç”¨

        # ã€ä¿®å¤ã€‘æ ¹æ®æ˜¯å¦å¯ç”¨AIçº é”™åŠ¨æ€è°ƒæ•´è¿›åº¦æƒé‡åˆ†é…
        if enable_ai_correction and source_format_lower == "soniox":
            # Soniox + AIçº é”™æ¨¡å¼ï¼šä¸ºAIçº é”™é˜¶æ®µåˆ†é…40%æƒé‡ï¼Œå…¶ä»–é˜¶æ®µå„20%
            phase_weight_align = self.WEIGHT_ALIGN  # 20%
            phase_weight_merge = self.WEIGHT_MERGE  # 20%
            phase_weight_format = self.WEIGHT_FORMAT  # 20%
            phase_weight_ai_correction = self.WEIGHT_AI_CORRECTION  # 40%
            self.log("è¿›åº¦åˆ†é…: åŸºç¡€SRTç”Ÿæˆ20% + åˆå¹¶20% + æ ¼å¼åŒ–20% + AIçº é”™40%")
        else:
            # å…¶ä»–æ¨¡å¼ï¼šä½¿ç”¨åŸæœ‰çš„60%æƒé‡åˆ†é…ç»™å‰ä¸‰ä¸ªé˜¶æ®µ
            phase_weight_align = 40  # åŸºç¡€SRTç”Ÿæˆ
            phase_weight_merge = 30  # åˆå¹¶
            phase_weight_format = 30  # æ ¼å¼åŒ–
            phase_weight_ai_correction = 0  # æ— AIçº é”™
            total_assigned = phase_weight_align + phase_weight_merge + phase_weight_format
            # ç¡®ä¿æ€»æƒé‡ä¸º100%
            if total_assigned < 100:
                phase_weight_format = phase_weight_format + (100 - total_assigned)
            self.log(f"è¿›åº¦åˆ†é…: åŸºç¡€SRTç”Ÿæˆ{phase_weight_align}% + åˆå¹¶{phase_weight_merge}% + æ ¼å¼åŒ–{phase_weight_format}%")

        intermediate_entries: List[SubtitleEntry] = []
        word_search_start_index = 0
        unaligned_segments: List[str] = []
        all_parsed_words = parsed_transcription.words
        if not llm_segments_text: self.log("é”™è¯¯ï¼šLLM æœªè¿”å›ä»»ä½•åˆ†å‰²ç‰‡æ®µã€‚"); return None
        if not all_parsed_words: self.log("é”™è¯¯ï¼šè§£æåçš„è¯åˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œå¯¹é½ã€‚"); return None


        total_llm_segments = len(llm_segments_text)
        completed_steps_phase1 = 0
        self.log(f"SRTé˜¶æ®µ1: å¯¹é½LLMç‰‡æ®µ (Mode {processing_mode})...")
        for i, text_seg_from_llm in enumerate(llm_segments_text):
            if not self._is_worker_running():
                self.log("âš ï¸ ä»»åŠ¡è¢«ç”¨æˆ·ä¸­æ–­")
                return None
            matched_words, next_search_idx, match_ratio = self.get_segment_words_fuzzy(text_seg_from_llm, all_parsed_words, word_search_start_index)
            if not matched_words or match_ratio == 0:
                unaligned_segments.append(text_seg_from_llm)
                completed_steps_phase1 += 1
                # ã€ä¿®å¤ã€‘ä½¿ç”¨åŠ¨æ€åˆ†é…çš„æƒé‡
                self._emit_srt_progress(int( (completed_steps_phase1 / total_llm_segments) * phase_weight_align ), 100)
                continue

            
            word_search_start_index = next_search_idx
            first_actual_word_index = -1
            for idx_fw, word_obj_fw in enumerate(matched_words):
                if word_obj_fw.text.strip(): first_actual_word_index = idx_fw; break
            last_actual_word_index = -1
            for idx_bw in range(len(matched_words) - 1, -1, -1):
                if matched_words[idx_bw].text.strip(): last_actual_word_index = idx_bw; break
            entry_text_from_llm = text_seg_from_llm.strip()
            actual_words_for_entry: List[TimestampedWord]
            if first_actual_word_index != -1 and last_actual_word_index != -1 :
                entry_start_time = matched_words[first_actual_word_index].start_time
                entry_end_time = matched_words[last_actual_word_index].end_time
                actual_words_for_entry = matched_words[first_actual_word_index : last_actual_word_index+1]
                if not actual_words_for_entry:
                    self.log(f"è­¦å‘Š: ä¿®æ­£åçš„è¯åˆ—è¡¨ä¸ºç©ºï¼ŒLLMç‰‡æ®µ \"{entry_text_from_llm[:30]}...\"ã€‚å°†ä½¿ç”¨åŸå§‹åŒ¹é…è¾¹ç•Œã€‚")
                    entry_start_time = matched_words[0].start_time; entry_end_time = matched_words[-1].end_time
                    actual_words_for_entry = matched_words
            else:
                self.log(f"è­¦å‘Š: LLMç‰‡æ®µ \"{entry_text_from_llm[:30]}...\" åŒ¹é…åˆ°çš„æ‰€æœ‰ASRè¯å…ƒå‡ä¸ºç©ºæˆ–ç©ºæ ¼ã€‚å°†ä½¿ç”¨åŸå§‹åŒ¹é…è¾¹ç•Œã€‚")
                entry_start_time = matched_words[0].start_time; entry_end_time = matched_words[-1].end_time
                actual_words_for_entry = matched_words
            
            entry_duration = max(0.001, entry_end_time - entry_start_time)
            text_len = len(entry_text_from_llm)
            is_audio_event = self._is_audio_event_words(actual_words_for_entry) if actual_words_for_entry else False

            # åº”ç”¨ç»“æŸæ—¶é—´ä¿®æ­£ (ä¸»æµç¨‹)
            if is_audio_event:
                # éŸ³é¢‘äº‹ä»¶ï¼šä¿æŒåŸå§‹ç»“æŸæ—¶é—´ï¼Œä¸åº”ç”¨æ—¶é—´ä¿®æ­£
                final_audio_event_end_time = entry_end_time
                self.log(f"   æ£€æµ‹åˆ°éŸ³é¢‘äº‹ä»¶ï¼Œä¿æŒåŸå§‹æ—¶é•¿: \"{entry_text_from_llm}\"")

                # éŸ³é¢‘äº‹ä»¶ï¼šä½¿ç”¨ä¿®æ­£åçš„æ—¶é—´ï¼ˆå¯èƒ½åŒ…å«å‘å‰å»¶é•¿ï¼‰ï¼Œä½†ä¸å‘åå»¶é•¿
                audio_event_text_content = "".join([w.text for w in actual_words_for_entry])
                intermediate_entries.append(SubtitleEntry(0, entry_start_time, final_audio_event_end_time, audio_event_text_content, actual_words_for_entry, match_ratio))
            else:
                # æ ¹æ®å¤„ç†æ¨¡å¼å†³å®šæ˜¯å¦åœ¨æ­¤é˜¶æ®µåº”ç”¨æ—¶é—´ä¿®æ­£
                if processing_mode == "B":
                    # Mode B: æš‚ä¸åº”ç”¨æ—¶é—´ä¿®æ­£ï¼Œå°†åœ¨Mode Bé˜¶æ®µè¿›è¡Œä¸“é—¨çš„"ä¸€å¥ä¸€å¥ä¼˜åŒ–"
                    # ä½¿ç”¨åŸå§‹æ—¶é—´åˆ›å»ºå­—å¹•æ¡ç›®ï¼Œè®©Mode Bå¤„ç†é˜¶æ®µè¿›è¡Œæ—¶é—´ä¼˜åŒ–
                    intermediate_entries.append(SubtitleEntry(0, entry_start_time, entry_end_time, entry_text_from_llm, actual_words_for_entry, match_ratio))
                elif processing_mode == "C":
                    # Mode C: åº”ç”¨Sonioxä¸“ç”¨çš„æ—¶é—´ä¿®æ­£é€»è¾‘
                    corrected_end_time = self._apply_end_time_correction(actual_words_for_entry, entry_end_time, entry_start_time)
                    corrected_entry_duration = max(0.001, corrected_end_time - entry_start_time)

                    if corrected_entry_duration > self.max_duration or text_len > self.max_chars_per_line:
                        self.log(f"   âš ï¸ Mode C: ç‰‡æ®µè¶…é™ï¼Œéœ€åˆ†å‰²: \"{entry_text_from_llm[:30]}...\" (ä¿®æ­£åæ—¶é•¿: {corrected_entry_duration:.2f}s, æ–‡æœ¬é•¿åº¦: {text_len})")
                        original_text_for_splitting = "".join([w.text for w in actual_words_for_entry])
                        split_sub_entries = self.split_long_sentence(original_text_for_splitting, actual_words_for_entry, entry_start_time, corrected_end_time, 0, corrected_end_time)

                        final_entries = []
                        for sub_entry in split_sub_entries:
                            sub_entry.alignment_ratio = match_ratio
                            if sub_entry.duration > self.max_duration and len(sub_entry.words_used) > 1:
                                safe_text_for_recursion = "".join([w.text for w in sub_entry.words_used])
                                recursive_splits = self.split_long_sentence(safe_text_for_recursion, sub_entry.words_used, sub_entry.start_time, sub_entry.end_time, 1, corrected_end_time)
                                for recursive_entry in recursive_splits:
                                    recursive_entry.alignment_ratio = match_ratio
                                    final_entries.append(recursive_entry)
                            else:
                                final_entries.append(sub_entry)

                        if len(final_entries) > 1:
                            self.log(f"   âœ… Mode C: ç‰‡æ®µå·²åˆ†å‰²ä¸º {len(final_entries)} ä¸ªå­ç‰‡æ®µ")
                        intermediate_entries.extend(final_entries)
                    elif corrected_entry_duration < self.min_duration_target:
                        # Mode CçŸ­æ—¶é•¿å¤„ç†
                        is_bracketed = self._is_bracketed_content(entry_text_from_llm)
                        if is_bracketed:
                            self.log(f"   Mode C: æ£€æµ‹åˆ°æ‹¬å·å†…å®¹ï¼Œä¿æŒåŸå§‹æ—¶é•¿: \"{entry_text_from_llm}\" ({corrected_entry_duration:.2f}s)")
                            final_short_entry_end_time = corrected_end_time
                        else:
                            final_short_entry_end_time = entry_start_time + self.min_duration_target
                            if corrected_entry_duration < app_config.MIN_DURATION_ABSOLUTE:
                                final_short_entry_end_time = entry_start_time + app_config.MIN_DURATION_ABSOLUTE
                            original_end_of_last_actual_word = actual_words_for_entry[-1].end_time if actual_words_for_entry else entry_start_time
                            max_allowed_extension = original_end_of_last_actual_word + 0.5
                            final_short_entry_end_time = min(final_short_entry_end_time, max_allowed_extension)
                            final_short_entry_end_time = max(final_short_entry_end_time, corrected_end_time)
                            final_short_entry_end_time = max(final_short_entry_end_time, entry_start_time + 0.001)

                        intermediate_entries.append(SubtitleEntry(0, entry_start_time, final_short_entry_end_time, entry_text_from_llm, actual_words_for_entry, match_ratio))
                    else:
                        # Mode Cæ­£å¸¸æƒ…å†µ
                        intermediate_entries.append(SubtitleEntry(0, entry_start_time, corrected_end_time, entry_text_from_llm, actual_words_for_entry, match_ratio))
                else:
                    # Mode A: åº”ç”¨åŸºç¡€çš„æ—¶é—´ä¿®æ­£é€»è¾‘
                    corrected_end_time = self._apply_end_time_correction(actual_words_for_entry, entry_end_time, entry_start_time)
                    corrected_entry_duration = max(0.001, corrected_end_time - entry_start_time)

                    if corrected_entry_duration > self.max_duration or text_len > self.max_chars_per_line:
                        self.log(f"   âš ï¸ Mode A: ç‰‡æ®µè¶…é™ï¼Œéœ€åˆ†å‰²: \"{entry_text_from_llm[:30]}...\" (ä¿®æ­£åæ—¶é•¿: {corrected_entry_duration:.2f}s, æ–‡æœ¬é•¿åº¦: {text_len})")
                        original_text_for_splitting = "".join([w.text for w in actual_words_for_entry])
                        split_sub_entries = self.split_long_sentence(original_text_for_splitting, actual_words_for_entry, entry_start_time, corrected_end_time, 0, corrected_end_time)

                        final_entries = []
                        for sub_entry in split_sub_entries:
                            sub_entry.alignment_ratio = match_ratio
                            if sub_entry.duration > self.max_duration and len(sub_entry.words_used) > 1:
                                safe_text_for_recursion = "".join([w.text for w in sub_entry.words_used])
                                recursive_splits = self.split_long_sentence(safe_text_for_recursion, sub_entry.words_used, sub_entry.start_time, sub_entry.end_time, 1, corrected_end_time)
                                for recursive_entry in recursive_splits:
                                    recursive_entry.alignment_ratio = match_ratio
                                    final_entries.append(recursive_entry)
                            else:
                                final_entries.append(sub_entry)

                        if len(final_entries) > 1:
                            self.log(f"   âœ… Mode A: ç‰‡æ®µå·²åˆ†å‰²ä¸º {len(final_entries)} ä¸ªå­ç‰‡æ®µ")
                        intermediate_entries.extend(final_entries)
                    elif corrected_entry_duration < self.min_duration_target:
                        # Mode AçŸ­æ—¶é•¿å¤„ç†
                        is_bracketed = self._is_bracketed_content(entry_text_from_llm)
                        if is_bracketed:
                            self.log(f"   Mode A: æ£€æµ‹åˆ°æ‹¬å·å†…å®¹ï¼Œä¿æŒåŸå§‹æ—¶é•¿: \"{entry_text_from_llm}\" ({corrected_entry_duration:.2f}s)")
                            final_short_entry_end_time = corrected_end_time
                        else:
                            final_short_entry_end_time = entry_start_time + self.min_duration_target
                            if corrected_entry_duration < app_config.MIN_DURATION_ABSOLUTE:
                                final_short_entry_end_time = entry_start_time + app_config.MIN_DURATION_ABSOLUTE
                            original_end_of_last_actual_word = actual_words_for_entry[-1].end_time if actual_words_for_entry else entry_start_time
                            max_allowed_extension = original_end_of_last_actual_word + 0.5
                            final_short_entry_end_time = min(final_short_entry_end_time, max_allowed_extension)
                            final_short_entry_end_time = max(final_short_entry_end_time, corrected_end_time)
                            final_short_entry_end_time = max(final_short_entry_end_time, entry_start_time + 0.001)

                        intermediate_entries.append(SubtitleEntry(0, entry_start_time, final_short_entry_end_time, entry_text_from_llm, actual_words_for_entry, match_ratio))
                    else:
                        # Mode Aæ­£å¸¸æƒ…å†µ
                        intermediate_entries.append(SubtitleEntry(0, entry_start_time, corrected_end_time, entry_text_from_llm, actual_words_for_entry, match_ratio))
            completed_steps_phase1 += 1
            # ã€ä¿®å¤ã€‘ä½¿ç”¨åŠ¨æ€åˆ†é…çš„æƒé‡
            self._emit_srt_progress(int( (completed_steps_phase1 / total_llm_segments) * phase_weight_align ), 100)
        self.log("--- LLMç‰‡æ®µå¯¹é½ç»“æŸ ---")
        if unaligned_segments:
            self.log(f"\\n--- ä»¥ä¸‹ {len(unaligned_segments)} ä¸ªLLMç‰‡æ®µæœªèƒ½æˆåŠŸå¯¹é½ï¼Œå·²è·³è¿‡ ---")
            for seg_idx, seg_text in enumerate(unaligned_segments): self.log(f"- ç‰‡æ®µ {seg_idx+1}: \"{seg_text}\"")
            self.log("----------------------------------------\\n")
        if not intermediate_entries: self.log("é”™è¯¯ï¼šå¯¹é½åæ²¡æœ‰ç”Ÿæˆä»»ä½•æœ‰æ•ˆçš„å­—å¹•æ¡ç›®ã€‚"); return None
        intermediate_entries.sort(key=lambda e: e.start_time)
        
        # --- Phase 2: æ™ºèƒ½åˆå¹¶ (æ¨¡å¼ç‰¹å®š) ---
        self.log(f"SRTé˜¶æ®µ2: åˆå¹¶è°ƒæ•´å­—å¹•æ¡ç›® (Mode {processing_mode})...")

        # æ ¹æ®æ¨¡å¼è®¾ç½®åˆå¹¶å‚æ•°
        if processing_mode == "C":
            # Mode C: Soniox - æ›´æ¿€è¿›çš„åˆå¹¶ç­–ç•¥ï¼Œå› ä¸ºæ—¶é—´æˆ³æ›´ç²¾ç¡®
            merge_gap_threshold = 0.5  # æ›´å°çš„é—´éš™å…è®¸åˆå¹¶
            self.log("Mode C: ä½¿ç”¨æ¿€è¿›çš„åˆå¹¶ç­–ç•¥ (é—´éš™é˜ˆå€¼: 0.5s)")
        elif processing_mode == "B":
            # Mode B: ElevenLabs - é€‚ä¸­çš„åˆå¹¶ç­–ç•¥
            merge_gap_threshold = 0.8  # åŸæœ‰çš„é˜ˆå€¼
            self.log("Mode B: ä½¿ç”¨é€‚ä¸­çš„åˆå¹¶ç­–ç•¥ (é—´éš™é˜ˆå€¼: 0.8s)")
        else:
            # Mode A: åŸºç¡€ - ä¿å®ˆçš„åˆå¹¶ç­–ç•¥
            merge_gap_threshold = 1.0  # æ›´ä¿å®ˆçš„é˜ˆå€¼
            self.log("Mode A: ä½¿ç”¨ä¿å®ˆçš„åˆå¹¶ç­–ç•¥ (é—´éš™é˜ˆå€¼: 1.0s)")
        merged_entries: List[SubtitleEntry] = []
        idx_merge = 0
        total_intermediate_entries = len(intermediate_entries)
        
        while idx_merge < total_intermediate_entries:
            if not self._is_worker_running(): self.log("ä»»åŠ¡è¢«ç”¨æˆ·ä¸­æ–­(åˆå¹¶é˜¶æ®µ)ã€‚"); return None
            
            current_entry = intermediate_entries[idx_merge]
            merged_this_iteration = False
            
            # å°è¯•ä¸ä¸‹ä¸€æ¡åˆå¹¶
            if idx_merge + 1 < len(intermediate_entries):
                next_entry = intermediate_entries[idx_merge+1]
                
                # æ£€æŸ¥æ˜¯å¦æ»¡è¶³åˆå¹¶çš„åŸºæœ¬ç¡¬æ€§æ¡ä»¶
                can_merge, reason = self._can_merge_entries(current_entry, next_entry)
                
                if can_merge:
                    # è®¡ç®—åˆå¹¶æ”¶ç›Š
                    benefit = self._calculate_merge_benefit(current_entry, next_entry)
                    
                    # åªæœ‰æ”¶ç›Šè¶…è¿‡é˜ˆå€¼æ‰åˆå¹¶ (Scribe2SRT é»˜è®¤é˜ˆå€¼ 5.0)
                    if benefit > 5.0:
                        self.log(f"   åˆå¹¶å­—å¹• (æ”¶ç›Š {benefit:.1f}): \"{current_entry.text[:15]}...\" + \"{next_entry.text[:15]}...\"")
                        merged_entry = self._merge_two_entries(current_entry, next_entry)
                        merged_entries.append(merged_entry)
                        idx_merge += 2
                        merged_this_iteration = True
                    else:
                        pass

            if not merged_this_iteration:
                merged_entries.append(current_entry)
                idx_merge += 1
                
            current_phase2_progress_component = int(((idx_merge) / total_intermediate_entries if total_intermediate_entries > 0 else 1) * phase_weight_merge)
            # ã€ä¿®å¤ã€‘åˆå¹¶é˜¶æ®µè¿›åº¦è®¡ç®—ï¼šé˜¶æ®µ1æƒé‡ + å½“å‰é˜¶æ®µ2è¿›åº¦
            self._emit_srt_progress(phase_weight_align + current_phase2_progress_component, 100)
        # --- End Phase 2 ---

        self.log(f"--- åˆå¹¶è°ƒæ•´åå¾—åˆ° {len(merged_entries)} ä¸ªå­—å¹•æ¡ç›®ï¼Œå¼€å§‹æœ€ç»ˆæ ¼å¼åŒ– ---")
        self.log(f"SRTé˜¶æ®µ3: æœ€ç»ˆæ ¼å¼åŒ–å­—å¹• (Mode {processing_mode})...")

        # åˆå§‹åŒ–æ ¡å¯¹æç¤ºåˆ—è¡¨ï¼ˆå¿…é¡»åœ¨æ¨¡å¼å¤„ç†ä¹‹å‰ï¼‰
        correction_hints: List[str] = []

        # æ ¹æ®æ¨¡å¼è®¾ç½®æ—¶é—´ä¼˜åŒ–å‚æ•°
        if processing_mode == "C":
            # Mode C: Soniox - åº”ç”¨ç½®ä¿¡åº¦åŸºç¡€çš„æ—¶é—´ä¼˜åŒ–
            self.log("Mode C: åº”ç”¨Sonioxä¸“ç”¨æ—¶é—´ä¼˜åŒ–ç­–ç•¥")
            correction_hints.extend(self._apply_mode_c_optimization_to_entries(merged_entries, parsed_transcription))

            # ã€ä¿®å¤ã€‘Mode Cæ—¶é—´ä¼˜åŒ–é˜¶æ®µå®Œæˆåï¼Œæ‰‹åŠ¨æ›´æ–°è¿›åº¦
            # é˜¶æ®µ1(20%) + é˜¶æ®µ2(20%) = 40%
            mode_c_optimization_completion_progress = phase_weight_align + phase_weight_merge
            self._emit_srt_progress(mode_c_optimization_completion_progress, 100)
        elif processing_mode == "B":
            # Mode B: ElevenLabs - å…ˆåº”ç”¨ä¸€å¥ä¸€å¥ä¼˜åŒ–ï¼Œå†è¿›è¡Œåˆå¹¶
            self.log("Mode B: åº”ç”¨ElevenLabsä¸€å¥ä¸€å¥ä¼˜åŒ–ç­–ç•¥")
            self._apply_mode_b_time_optimization(merged_entries)
            # Mode Béœ€è¦åœ¨æ—¶é—´ä¼˜åŒ–åè¿›è¡Œåˆå¹¶ï¼Œå› ä¸ºåˆå¹¶å†³ç­–åº”è¯¥åŸºäºä¼˜åŒ–åçš„æ—¶é—´æˆ³
            self.log("Mode B: å¼€å§‹åŸºäºä¼˜åŒ–åæ—¶é—´æˆ³çš„åˆå¹¶è°ƒæ•´")
            self._apply_mode_b_merge_optimization(merged_entries)

            # ã€ä¿®å¤ã€‘Mode Båˆå¹¶é˜¶æ®µå®Œæˆåï¼Œæ‰‹åŠ¨æ›´æ–°è¿›åº¦ï¼ˆè·³è¿‡ç‹¬ç«‹æ–¹æ³•çš„è¿›åº¦è®¡ç®—ï¼‰
            # é˜¶æ®µ1(20%) + é˜¶æ®µ2(20%) = 40%
            mode_b_merge_completion_progress = phase_weight_align + phase_weight_merge
            self._emit_srt_progress(mode_b_merge_completion_progress, 100)
        else:
            # Mode A: åŸºç¡€ - æœ€å°å¿…è¦å¤„ç†
            self.log("Mode A: åº”ç”¨åŸºç¡€æ—¶é—´ä¼˜åŒ–ç­–ç•¥")
            self._apply_mode_a_optimization_to_entries(merged_entries)

            # ã€ä¿®å¤ã€‘Mode Aæ—¶é—´ä¼˜åŒ–é˜¶æ®µå®Œæˆåï¼Œæ‰‹åŠ¨æ›´æ–°è¿›åº¦
            # é˜¶æ®µ1(20%) + é˜¶æ®µ2(20%) = 40%
            mode_a_optimization_completion_progress = phase_weight_align + phase_weight_merge
            self._emit_srt_progress(mode_a_optimization_completion_progress, 100)
        final_srt_formatted_list: List[str] = []
        final_entry_objects: List[SubtitleEntry] = []  # <--- [æ–°å¢] åˆå§‹åŒ–åˆ—è¡¨
        last_processed_entry_object: Optional[SubtitleEntry] = None
        subtitle_index = 1
        total_merged_final_entries = len(merged_entries)
        for entry_idx, current_entry in enumerate(merged_entries):
            if not self._is_worker_running(): self.log("ä»»åŠ¡è¢«ç”¨æˆ·ä¸­æ–­(æœ€ç»ˆæ ¼å¼åŒ–é˜¶æ®µ)ã€‚"); return None
            self.log(f"   æ ¼å¼åŒ–æ¡ç›® {entry_idx+1}/{total_merged_final_entries}: \"{current_entry.text[:30]}...\"")
            if last_processed_entry_object is not None:

                # åªæœ‰Mode Bæ‰§è¡Œå¤æ‚çš„æ—¶é—´ä¼˜åŒ–é€»è¾‘
                if processing_mode == "B":
                    # Mode B: ElevenLabs - å¤æ‚æ—¶é—´ä¼˜åŒ–é€»è¾‘
                    raw_gap = current_entry.start_time - last_processed_entry_object.end_time

                    # æ£€æµ‹å¹¶ä¿®æ­£æ—¶é—´é‡å 
                    if raw_gap < -0.01:  # æ£€æµ‹åˆ°è´Ÿæ—¶é—´ï¼ˆé‡å ï¼‰
                        self.log(f"å­—å¹•æ—¶é—´é‡å ä¿®æ­£: è°ƒæ•´é‡å æ—¶é—´ {raw_gap:.3f}s")
                        new_start_time = last_processed_entry_object.end_time + 0.01
                        if new_start_time < current_entry.end_time:
                            current_entry.start_time = new_start_time

                    # åº”ç”¨å¼€å§‹æ—¶é—´ä¿®æ­£ (æå‰0.25s)
                    current_is_audio_event = self._is_bracketed_content(current_entry.text)
                    if not current_is_audio_event and raw_gap > 0.5:
                        self.log(f"å­—å¹•æ—¶é—´ä¼˜åŒ–: æ£€æµ‹åˆ°è¾ƒå¤§æ—¶é—´é—´éš™ ({raw_gap:.2f}s)")
                        new_start_time = current_entry.start_time - 0.25
                        if new_start_time > last_processed_entry_object.end_time:
                            self.log(f"å­—å¹•æ—¶é—´ä¼˜åŒ–: æå‰å¼€å§‹æ—¶é—´ä»¥å‡å°‘é—´éš™")
                            current_entry.start_time = new_start_time
                        else:
                            self.log(f"æ—¶é—´ä¼˜åŒ–è·³è¿‡: ä¼šä¸ä¸Šä¸€å¥é‡å ")

                    # 100ms é—´éš™é€»è¾‘
                    last_is_audio_event = self._is_bracketed_content(last_processed_entry_object.text)
                    gap_seconds = self.default_gap_ms / 1000.0

                    if not (current_is_audio_event or last_is_audio_event):
                        if current_entry.start_time < last_processed_entry_object.end_time + gap_seconds:
                            new_current_start_time = last_processed_entry_object.end_time + gap_seconds
                            min_current_duration = app_config.MIN_DURATION_ABSOLUTE
                            if new_current_start_time + min_current_duration <= current_entry.end_time:
                                self.log(f"å­—å¹•æ—¶é—´ä¼˜åŒ–: è°ƒæ•´ä»¥ä¿æŒæœ€å°é—´è·")
                                current_entry.start_time = new_current_start_time
                            if final_srt_formatted_list:
                                final_srt_formatted_list[-1] = last_processed_entry_object.to_srt_format(self)
                else:
                    # Mode A & C: åªè¿›è¡ŒåŸºæœ¬çš„é‡å ä¿®æ­£
                    raw_gap = current_entry.start_time - last_processed_entry_object.end_time
                    if raw_gap < -0.01:
                        self.log(f"åŸºç¡€é‡å ä¿®æ­£: è°ƒæ•´é‡å æ—¶é—´ {raw_gap:.3f}s")
                        current_entry.start_time = last_processed_entry_object.end_time + 0.01
            current_duration = current_entry.duration
            entry_is_audio_event = False
            if current_entry.words_used: entry_is_audio_event = any(not w.text.strip() or getattr(w, 'type', 'word') == 'audio_event' or re.match(r"^\(.*\)$|^ï¼ˆ.*ï¼‰$", w.text.strip()) for w in current_entry.words_used)

            # åˆå§‹åŒ–å˜é‡ï¼Œé¿å…UnboundLocalError
            min_duration_to_apply_val = None

            # æ ¹æ®æ¨¡å¼åº”ç”¨ä¸åŒçš„æ—¶é•¿å¤„ç†
            if not current_entry.is_intentionally_oversized and not entry_is_audio_event:
                if processing_mode == "A":
                    # Mode A: åªåº”ç”¨ç»å¯¹æœ€å°æ—¶é•¿
                    min_duration_to_apply_val = app_config.MIN_DURATION_ABSOLUTE if current_duration < app_config.MIN_DURATION_ABSOLUTE else None
                elif processing_mode == "C":
                    # Mode C: åº”ç”¨Sonioxä¸“ç”¨æ—¶é•¿ç­–ç•¥ï¼ˆå¯èƒ½æ›´ä¸¥æ ¼ï¼‰
                    min_duration_to_apply_val = self.min_duration_target if current_duration < self.min_duration_target else None
                else:
                    # Mode B: ElevenLabs å…¼å®¹æ¨¡å¼
                    min_duration_to_apply_val = None

                    # è®¡ç®—æœ€ç»ˆç”Ÿæ•ˆçš„æœ€å°ç›®æ ‡ï¼šå–ç”¨æˆ·è®¾ç½®å’Œç³»ç»Ÿåº•é™ä¸­çš„è¾ƒå¤§è€…
                    # ä¾‹å¦‚ï¼šç”¨æˆ·è®¾1.2sï¼Œç³»ç»Ÿåº•é™1.0s -> ç›®æ ‡ä¸º 1.2s
                    final_target_min = max(self.min_duration_target, app_config.MIN_DURATION_ABSOLUTE)

                    # åªæœ‰å½“æ—¶é•¿å°äºè¿™ä¸ªç›®æ ‡æ—¶æ‰åº”ç”¨ä¿®æ­£
                    if current_duration < final_target_min:
                        min_duration_to_apply_val = final_target_min

            if min_duration_to_apply_val is not None:
                current_entry.end_time = max(current_entry.end_time, current_entry.start_time + min_duration_to_apply_val)

            # æœ€å¤§æ—¶é•¿é™åˆ¶ï¼ˆæ‰€æœ‰æ¨¡å¼éƒ½åº”ç”¨ï¼‰
            if not current_entry.is_intentionally_oversized and current_entry.duration > self.max_duration:
                # Mode Cç‰¹æ®Šå¤„ç†ï¼šå°è¯•å¯¹èˆ’é€‚åº¦ä¼˜åŒ–åçš„è¶…é™ç‰‡æ®µè¿›è¡Œåˆ†å‰²
                if processing_mode == "C" and len(current_entry.words_used) > 1:
                    self.log(f"å­—å¹• \"{current_entry.text[:30]}...\" æ—¶é•¿ {current_duration:.2f}s è¶…å‡ºæœ€å¤§å€¼ {self.max_duration}sï¼Œå°è¯•ç‰¹æ®Šåˆ†å‰²ã€‚")

                    # å°è¯•åˆ†å‰²è¶…é™ç‰‡æ®µï¼ˆä¸é‡å¤æ·»åŠ èˆ’é€‚åº¦æ—¶é—´ï¼‰
                    split_entries = self._split_comfort_optimized_entry(current_entry)

                    if len(split_entries) > 1:
                        # åˆ†å‰²æˆåŠŸï¼Œç«‹å³æ ¼å¼åŒ–æ‰€æœ‰åˆ†å‰²åçš„ç‰‡æ®µå¹¶æ·»åŠ åˆ°final_srt_formatted_list
                        self.log(f"ç‰¹æ®Šåˆ†å‰²æˆåŠŸï¼šåŸç‰‡æ®µåˆ†ä¸º {len(split_entries)} ä¸ªå­ç‰‡æ®µ")

                        for split_idx, split_entry in enumerate(split_entries):
                            split_entry.index = subtitle_index + split_idx

                            # 1. ç”Ÿæˆ SRT å­—ç¬¦ä¸²
                            final_srt_formatted_list.append(split_entry.to_srt_format(self))

                            # 2. [æ–°å¢] æ”¶é›†æ­£ç¡®çš„å¯¹è±¡
                            final_entry_objects.append(split_entry)

                            self.log(f"   æ ¼å¼åŒ–åˆ†å‰²ç‰‡æ®µ {split_idx+1}/{len(split_entries)}: ...")
                    # if len(split_entries) > 1:
                    #     # åˆ†å‰²æˆåŠŸï¼Œç«‹å³æ ¼å¼åŒ–æ‰€æœ‰åˆ†å‰²åçš„ç‰‡æ®µå¹¶æ·»åŠ åˆ°final_srt_formatted_list
                    #     self.log(f"ç‰¹æ®Šåˆ†å‰²æˆåŠŸï¼šåŸç‰‡æ®µåˆ†ä¸º {len(split_entries)} ä¸ªå­ç‰‡æ®µ")

                    #     # ç«‹å³æ ¼å¼åŒ–æ‰€æœ‰åˆ†å‰²åçš„ç‰‡æ®µå¹¶æ·»åŠ åˆ°final_srt_formatted_list
                    #     for split_idx, split_entry in enumerate(split_entries):
                    #         split_entry.index = subtitle_index + split_idx
                    #         final_srt_formatted_list.append(split_entry.to_srt_format(self))
                    #         self.log(f"   æ ¼å¼åŒ–åˆ†å‰²ç‰‡æ®µ {split_idx+1}/{len(split_entries)}: \"{split_entry.text[:30]}...\"")

                        # æ›´æ–°ç´¢å¼•å’Œlast_processed_entry_object
                        subtitle_index += len(split_entries)
                        last_processed_entry_object = split_entries[-1]
                        # è·³è¿‡å½“å‰æ¡ç›®çš„åç»­å¤„ç†ï¼Œä¸è¦ä¿®æ”¹merged_entries
                        continue
                    else:
                        # æ— æ³•åˆ†å‰²ï¼Œä¿æŒåŸæ ·å¹¶æˆªæ–­
                        self.log(f"ç‰¹æ®Šåˆ†å‰²å¤±è´¥ï¼Œæˆªæ–­è¶…é™ç‰‡æ®µ")
                        current_entry.end_time = current_entry.start_time + self.max_duration
                else:
                    # å…¶ä»–æ¨¡å¼æˆ–æ— æ³•åˆ†å‰²çš„ç‰‡æ®µï¼Œç›´æ¥æˆªæ–­
                    self.log(f"å­—å¹• \"{current_entry.text[:30]}...\" æ—¶é•¿ {current_duration:.2f}s è¶…å‡ºæœ€å¤§å€¼ {self.max_duration}sï¼Œå°†è¢«æˆªæ–­ã€‚")
                    current_entry.end_time = current_entry.start_time + self.max_duration
            if current_entry.end_time <= current_entry.start_time: 
                 current_entry.end_time = current_entry.start_time + 0.001
            
            current_entry.index = subtitle_index

            # 1. ç”Ÿæˆ SRT å­—ç¬¦ä¸²
            final_srt_formatted_list.append(current_entry.to_srt_format(self))

            # 2. [æ–°å¢] æ”¶é›†æ­£ç¡®çš„å¯¹è±¡
            final_entry_objects.append(current_entry)

            last_processed_entry_object = current_entry; subtitle_index += 1
            current_phase3_progress_component = int(((entry_idx + 1) / total_merged_final_entries if total_merged_final_entries > 0 else 1) * phase_weight_format)
            # ã€ä¿®å¤ã€‘æ ¼å¼åŒ–é˜¶æ®µè¿›åº¦è®¡ç®—ï¼šé˜¶æ®µ1æƒé‡ + é˜¶æ®µ2æƒé‡ + å½“å‰é˜¶æ®µ3è¿›åº¦
            self._emit_srt_progress(phase_weight_align + phase_weight_merge + current_phase3_progress_component, 100)
        self.log("--- SRT å†…å®¹ç”Ÿæˆå’Œæ ¼å¼åŒ–å®Œæˆ ---")

        # Sonioxä¸“ç”¨è¯çº§é—´è·éªŒè¯ï¼šåœ¨ç»ˆæä¼˜åŒ–ä¹‹å‰åº”ç”¨è¯çº§è°ƒæ•´é€»è¾‘
        if processing_mode == "C" and len(final_entry_objects) >= 2:  # <--- ä½¿ç”¨æ–°åˆ—è¡¨åˆ¤æ–­
            self.log("--- Sonioxè¯çº§é—´è·éªŒè¯ï¼šåœ¨ç»ˆæä¼˜åŒ–å‰æ£€æŸ¥æœ€å°é—´è·è¦æ±‚ ---")

            # åº”ç”¨è¯çº§é—´è·éªŒè¯ (ä¼ å…¥æ­£ç¡®çš„æ–°åˆ—è¡¨)
            # æ³¨æ„ï¼š_apply_word_level_spacing_validation ä¼šç›´æ¥ä¿®æ”¹ä¼ å…¥å¯¹è±¡çš„æ—¶é—´å±æ€§
            self._apply_word_level_spacing_validation(final_entry_objects)

            # åŸºäºè°ƒæ•´åçš„å¯¹è±¡é‡æ–°ç”Ÿæˆ SRT åˆ—è¡¨
            final_srt_formatted_list = []
            for entry in final_entry_objects:  # <--- ä½¿ç”¨æ­£ç¡®çš„æ–°åˆ—è¡¨é‡æ„
                final_srt_formatted_list.append(entry.to_srt_format(self))

            self.log("--- Sonioxè¯çº§é—´è·éªŒè¯å®Œæˆ ---")

        # Sonioxä¸“ç”¨ç»ˆæä¼˜åŒ–ï¼šåŠ¨æ€å‰ç§»å¼€å§‹æ—¶é—´
        if processing_mode == "C" and len(final_srt_formatted_list) > 0:
            self.log("--- Sonioxç»ˆæä¼˜åŒ–ï¼šåŠ¨æ€è°ƒæ•´å­—å¹•å¼€å§‹æ—¶é—´ ---")
            optimized_srt_list = self._apply_soniox_ultimate_optimization(final_srt_formatted_list)
            final_srt_formatted_list = optimized_srt_list
            self.log("--- Sonioxç»ˆæä¼˜åŒ–å®Œæˆ ---")

            # æ³¨æ„ï¼šè¯çº§é—´è·éªŒè¯å·²åœ¨ç»ˆæä¼˜åŒ–å‰å®Œæˆï¼Œæ— éœ€åœ¨æ­¤é‡å¤è¿›è¡Œ

        # ç”Ÿæˆæœ€ç»ˆSRTå†…å®¹
        final_srt_content = "".join(final_srt_formatted_list).strip()

        # ã€æ–°å¢ã€‘åœ¨SRTç”Ÿæˆå®Œæˆåè¿›è¡ŒAIæ ¡å¯¹ï¼ˆåå¤„ç†æ¨¡å¼ï¼‰
        if enable_ai_correction and processing_mode == "C" and final_srt_content:
            if not self.llm_api_key:
                self.log("âš ï¸ æœªé…ç½®LLM APIå¯†é’¥ï¼Œè·³è¿‡AIæ ¡å¯¹")
                correction_hints.append("âš ï¸ æœªé…ç½®LLM APIå¯†é’¥ï¼Œè·³è¿‡AIæ ¡å¯¹")
                # ã€ä¿®å¤ã€‘AIçº é”™é˜¶æ®µè·³è¿‡æ—¶ï¼Œç›´æ¥æ ‡è®°ä¸ºå®Œæˆ
                completion_progress = phase_weight_align + phase_weight_merge + phase_weight_format + phase_weight_ai_correction
                self._emit_srt_progress(completion_progress, 100)
            else:
                try:
                    self.log("ğŸ¤– å¼€å§‹AIçº é”™é˜¶æ®µï¼ˆå æ€»è¿›åº¦40%ï¼‰")
                    # ã€ä¿®å¤ã€‘AIçº é”™é˜¶æ®µå¼€å§‹è¿›åº¦ï¼ˆå‰ä¸‰ä¸ªé˜¶æ®µå®Œæˆï¼‰
                    ai_correction_start_progress = phase_weight_align + phase_weight_merge + phase_weight_format
                    self._emit_srt_progress(ai_correction_start_progress, 100)

                    final_srt_content, ai_correction_hints = self._apply_post_srt_ai_correction(
                        final_srt_content, parsed_transcription.words
                    )
                    correction_hints.extend(ai_correction_hints)

                    # ã€ä¿®å¤ã€‘AIçº é”™é˜¶æ®µå®Œæˆè¿›åº¦
                    completion_progress = ai_correction_start_progress + phase_weight_ai_correction
                    self._emit_srt_progress(completion_progress, 100)
                    self.log("âœ… AIçº é”™é˜¶æ®µå®Œæˆ")
                except Exception as e:
                    error_msg = f"âŒ SRTåå¤„ç†AIæ ¡å¯¹å¤±è´¥: {str(e)}"
                    self.log(error_msg)
                    correction_hints.append(error_msg)
                    # ã€ä¿®å¤ã€‘AIçº é”™å¤±è´¥æ—¶ä¹Ÿæ ‡è®°ä¸ºå®Œæˆè¿›åº¦
                    completion_progress = phase_weight_align + phase_weight_merge + phase_weight_format + phase_weight_ai_correction
                    self._emit_srt_progress(completion_progress, 100)
        elif enable_ai_correction and processing_mode != "C":
            self.log(f"âš ï¸ AIçº é”™ä»…æ”¯æŒSonioxæ¨¡å¼ï¼Œå½“å‰ä¸º{processing_mode}æ¨¡å¼")
            correction_hints.append(f"âš ï¸ AIçº é”™ä»…æ”¯æŒSonioxæ¨¡å¼ï¼Œå½“å‰ä¸º{processing_mode}æ¨¡å¼")

        # æœ€ç»ˆç¡®ä¿æ¸…ç†æ‰€æœ‰æ®‹ç•™çš„ã€ã€‘ç¬¦å·
        final_srt_content = self._clean_bracket_symbols(final_srt_content)

        # ã€ä¿®å¤ã€‘ç¡®ä¿è¿›åº¦æ€»æ˜¯è¾¾åˆ°100%
        final_progress = phase_weight_align + phase_weight_merge + phase_weight_format + phase_weight_ai_correction
        self._emit_srt_progress(final_progress, 100)

        # è¿”å›å·²ç”Ÿæˆçš„SRTå†…å®¹å’Œæ ¡å¯¹æç¤º
        return final_srt_content, correction_hints